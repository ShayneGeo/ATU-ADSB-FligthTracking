{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b10a48-d774-4086-a7bf-e73d3caa1fa7",
   "metadata": {},
   "source": [
    "# IAOC24 codes for most USFS contracted aircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c50c3-9089-49d7-b6a4-4eb187009420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAOC24 codes for most USFS waterbombers\n",
    "IAOC24_CODES = [\"C00E3E\",\"C07CB2\",\"A7D27A\",\"A69072\",\"A11CBB\",\"A7F642\",\"A9926A\",\"AB79CC\",\"A3F3AC\",\"A3F763\",\"A442AA\",\"A85052\",\n",
    "                \"A4EA6D\",\"A380E6\",\"A4B460\",\"A47CBC\",\"A48A3A\",\"A47197\",\"A46DE0\",\"A4C031\",\"A07F21\",\n",
    "               \"A380E6\",\"A4B50C\",\"A0956B\",\"A2F996\",\"A2FD4D\",\"A30104\",\"A304BB\",\"A30872\",\"A46DE0\",\"A47CBC\",\"A48A3A\",\"A47197\",\n",
    "                \"A5CD6C\",\"A5D123\",\"A5D4DA\",\"AD66E3\",\"ADC0C4\",\"AAFD0B\",\"A5C9B5\",\"A8215E\",\"A19DA0\", \"A2B7FD\"]\n",
    "\n",
    "IAOC24_CODES = [code.lower() for code in IAOC24_CODES]\n",
    "print(IAOC24_CODES)\n",
    "print(len(IAOC24_CODES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654e92d-41d9-4f08-8b88-53b9fa3fa09f",
   "metadata": {},
   "source": [
    "### ATU drop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494c040-6ef5-42a2-a4ce-1c360250164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your shapefiles\n",
    "shapefile_paths = [\n",
    "    r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2021_Full_Year_03112024_VLAT.shp\",\n",
    "    r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2022_Full_Year_03112024_VLAT.shp\",\n",
    "    r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2023_Full_Year_03112024_VLAT.shp\"\n",
    "]\n",
    "\n",
    "# Read all shapefiles and combine them into a single GeoDataFrame\n",
    "gdf_list = [gpd.read_file(shapefile_path) for shapefile_path in shapefile_paths]\n",
    "combined_gdf = pd.concat(gdf_list, ignore_index=True)\n",
    "\n",
    "# Step 2: Calculate the centroid for each line\n",
    "# The centroid here refers to the geometric center of the line's bounding box, not always on the line itself\n",
    "combined_gdf['centroid'] = combined_gdf.geometry.centroid\n",
    "\n",
    "# Extract latitude and longitude from the centroid\n",
    "combined_gdf['Latitude1'] = combined_gdf['centroid'].y\n",
    "combined_gdf['Longitude1'] = combined_gdf['centroid'].x\n",
    "\n",
    "# Drop the 'centroid' column as it's no longer needed (optional)\n",
    "combined_gdf.drop('centroid', axis=1, inplace=True)\n",
    "\n",
    "# Convert 'Date_(UTC)' and 'Time_(UTC)' columns to a single datetime column\n",
    "combined_gdf['UTCdateTim'] = pd.to_datetime(combined_gdf['Date_(UTC)'] + ' ' + combined_gdf['Time_(UTC)'])\n",
    "\n",
    "# Extract the prefix from 'dropID' and map to ADS-B codes\n",
    "combined_gdf['dropID_prefix'] = combined_gdf['dropID'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# Define the mapping from dropID_prefix to ADS-B codes\n",
    "dropID_to_adsb_code = {\n",
    "    'T910': 'a7d27a',\n",
    "    'T912': 'a5d891',\n",
    "    'T911': 'a11cbb',\n",
    "    'T914': 'a7d27a'\n",
    "}\n",
    "\n",
    "# Create a new column 'adsb_code' by mapping the dropID_prefix values to their corresponding ADS-B codes\n",
    "combined_gdf['adsb_code'] = combined_gdf['dropID_prefix'].map(dropID_to_adsb_code)\n",
    "\n",
    "# Print the result\n",
    "#print(combined_gdf[['dropID', 'dropID_prefix', 'adsb_code', 'Latitude1', 'Longitude1', 'UTCdateTim']])\n",
    "\n",
    "# You can also convert the GeoDataFrame to a DataFrame and save it as a CSV if needed\n",
    "combined_df_vlat = combined_gdf[['dropID', 'dropID_prefix', 'adsb_code', 'Latitude1', 'Longitude1', 'UTCdateTim']]\n",
    "#combined_df.to_csv('combined_vlat_data.csv', index=False)\n",
    "combined_df_vlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee156c-c49e-42f9-b588-48a767244192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your shapefiles\n",
    "shapefile_paths = [\n",
    "    r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\LAT_20210101_20211231_FullYear.shp\",\n",
    "    r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\LAT_20220101_20221231_FullYear.shp\",\n",
    "    r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\LAT_20230101_20231231_FullYear.shp\"\n",
    "]\n",
    "\n",
    "# Read the target CRS from one of the shapefiles\n",
    "shapefile_path_2 = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2023_Full_Year_03112024_VLAT.shp\"\n",
    "gdf_2 = gpd.read_file(shapefile_path_2)\n",
    "target_crs = gdf_2.crs\n",
    "print(f\"Target CRS: {target_crs}\")\n",
    "\n",
    "# Process each shapefile\n",
    "gdf_list = []\n",
    "for shapefile_path in shapefile_paths:\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    gdf = gdf.to_crs(target_crs)\n",
    "    print(f\"Original length of {shapefile_path}: {len(gdf)}\")\n",
    "    \n",
    "    # Filter by LineLenMi\n",
    "    gdf = gdf[gdf['LineLenMi'] < 0.2]\n",
    "    print(f\"Filtered length of {shapefile_path}: {len(gdf)}\")\n",
    "    \n",
    "    # Calculate the centroid for each line\n",
    "    gdf['centroid'] = gdf.geometry.centroid\n",
    "    \n",
    "    # Extract latitude and longitude from the centroid\n",
    "    gdf['Latitude1'] = gdf['centroid'].y\n",
    "    gdf['Longitude1'] = gdf['centroid'].x\n",
    "    \n",
    "    # Drop the 'centroid' column as it's no longer needed (optional)\n",
    "    gdf.drop('centroid', axis=1, inplace=True)\n",
    "    \n",
    "    # Convert date-time column\n",
    "    gdf['UTCdateTim'] = pd.to_datetime(gdf['DateTimeUT'])\n",
    "    gdf['dropID_prefix'] = gdf['DropID'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    gdf_list.append(gdf)\n",
    "\n",
    "# Combine all the GeoDataFrames\n",
    "combined_gdf = pd.concat(gdf_list, ignore_index=True)\n",
    "combined_gdf.rename(columns={'n': 'TailNumber'}, inplace=True)\n",
    "\n",
    "# Your mapping of 'n' values to 'adsb_CODE' values\n",
    "n_to_adsb_code = {\n",
    "    'N389AC': 'A47CBC',\n",
    "    'N392AC': 'A48A3A',\n",
    "    'N386AC': 'A47197',\n",
    "    'N385AC': 'A46DE0',\n",
    "    'N473NA': 'A5CD6C',\n",
    "    'N474NA': 'A5D123',\n",
    "    'N475NA': 'A5D4DA',\n",
    "    'N472NA': 'A5C9B5',\n",
    "    'N291EA': 'A2F996',\n",
    "    'N292EA': 'A2FD4D',\n",
    "    'N293EA': 'A30104',\n",
    "    'N294EA': 'A304BB',\n",
    "    'N295EA': 'A30872',\n",
    "    'N297EA': 'A30FE0',\n",
    "    'N476NA': 'A5D891',\n",
    "    'N131CG': 'A07F21',\n",
    "    'N132CG': 'A082D8',\n",
    "    'N137CG': 'A0956B',\n",
    "    'N477NA': 'A5DC48',\n",
    "    'C-FKFM': 'CFKFM',\n",
    "    'N478NA': 'A5DFFF',\n",
    "    'N839AC': 'AB79CC',\n",
    "    'N355AC': 'A3F763',\n",
    "    'N366AC': 'A42299',\n",
    "    'N374AC': 'A442AA',\n",
    "    'N635AC': 'A85052',\n",
    "    'N416AC': 'A4EA6D',\n",
    "    'N325AC': 'A380E6',\n",
    "    'N138CG': 'A09922',\n",
    "    'N470NA': 'A5C247',\n",
    "    'N471NA': 'A5C5FE',\n",
    "    'C-FFQF': 'CFFQF',\n",
    "    'C-FFQG': 'CFFQG',\n",
    "    'N358AS': 'A40296',\n",
    "    'N23WT': 'A2082D',\n",
    "    'N90WW': 'AC6EDC',\n",
    "    'N10TP': 'A00412',\n",
    "    'N619SW': 'A811D2',\n",
    "    'N415BT': 'A4E6DE',\n",
    "    'N417BT': 'A4EE4C',\n",
    "    'N418BT': 'A4F203',\n",
    "    'N419BT': 'A4F5BA',\n",
    "    'N406BT': 'A4C316',\n",
    "    'C-FFQL': 'CFFQL',\n",
    "    'C-FFZJ': 'CFFZJ',\n",
    "}\n",
    "\n",
    "#['N619SW', 'N415BT', 'N417BT', 'N418BT', 'N419BT', 'N406BT', 'C-FFQL', 'C-FFZJ']\n",
    "\n",
    "\n",
    "combined_gdf['adsb_CODE_UPPER'] = combined_gdf['TailNumber'].map(n_to_adsb_code)\n",
    "\n",
    "# Convert 'adsb_CODE' values to lowercase, safely handling NaN values\n",
    "#data['adsb_CODE_lower'] = [code.lower() if pd.notnull(code) else None for code in data['adsb_CODE']]\n",
    "combined_gdf['adsb_code'] = combined_gdf['adsb_CODE_UPPER'].str.lower()\n",
    "combined_gdf['UTCdateTim']= pd.to_datetime(combined_gdf['UTCdateTim'])\n",
    "\n",
    "# If you want to create a list of these lowercase 'adsb_CODE' values (excluding None)\n",
    "#dataADSBLIST = [code for code in data['adsb_CODE_lower'] if code is not None]\n",
    "\n",
    "\n",
    "\n",
    "# Output the combined GeoDataFrame\n",
    "#print(combined_gdf[['DropID', 'dropID_prefix', 'Latitude1', 'Longitude1', 'UTCdateTim']])\n",
    "\n",
    "# Optionally, save the combined GeoDataFrame to a new shapefile or CSV\n",
    "#combined_gdf.to_file(\"combined_vlat_data.shp\")\n",
    "#combined_gdf[['DropID', 'dropID_prefix', 'Latitude1', 'Longitude1', 'UTCdateTim']].to_csv(\"combined_vlat_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a952a48-0b5c-4478-b57c-a39ca2b5ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f56862-2039-4835-ab7c-c7ee5adb3134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904796a1-f8cc-4017-beaf-c28fa13be70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBflighttracking\\ATU_2017_2021_1mile_ALL.txt\", sep=\",\", header=0)\n",
    "\n",
    "# Only considering the first 5000 rows as per your previous request\n",
    "#data = data.iloc[0:5000]\n",
    "\n",
    "# Replace ':' with '_' in 'Drop_ID1' column\n",
    "data['Drop_ID1'] = data['Drop_ID1'].str.replace(':', '_')\n",
    "data['Drop_ID1'] = data['Drop_ID1'].str.replace('-', '_')\n",
    "data['Drop_ID1'] = data['Drop_ID1'].str.replace(' ', '')\n",
    "data.rename(columns={'Drop_ID1': 'DropID'}, inplace=True)\n",
    "data.rename(columns={'Tail_Numbe': 'TailNumber'}, inplace=True)\n",
    "\n",
    "# List all filenames in the specified directory (without file extensions)\n",
    "# directory_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\DropSamplesADSB\"\n",
    "# filenames = [os.path.splitext(file)[0] for file in os.listdir(directory_path)]\n",
    "\n",
    "# # Check if modified 'Drop_ID1' values are in the list of filenames\n",
    "# data['FilenameMatch'] = data['Drop_ID1'].isin(filenames)\n",
    "\n",
    "# # Filter the DataFrame to only include rows where 'Drop_ID1' matches a filename\n",
    "# data = data[data['FilenameMatch']]\n",
    "\n",
    "# # Drop the 'FilenameMatch' column if you don't need it anymore\n",
    "# data.drop(columns=['FilenameMatch'], inplace=True)\n",
    "\n",
    "# print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def map_tail_number_to_adsb_code(tail_number):\n",
    "    if tail_number == \"T-01\":\n",
    "        return \"a5cd6c\"\n",
    "    elif tail_number == \"T-02\":\n",
    "        return \"a5d123\"\n",
    "    elif tail_number == \"T-03\":\n",
    "        return \"a5d4da\"\n",
    "    elif tail_number == \"T-05\":\n",
    "        return \"ad66e3\"\n",
    "    elif tail_number == \"T-06\":\n",
    "        return \"adc0c4\"\n",
    "    elif tail_number == \"T-07\":\n",
    "        return \"aafd0b\"\n",
    "    elif tail_number == \"T-10\":\n",
    "        return \"a5c9b5\"\n",
    "    elif tail_number == \"T-101\":\n",
    "        return \"a2f996\"\n",
    "    elif tail_number == \"T-102\":\n",
    "        return \"a2fd4d\"\n",
    "    elif tail_number == \"T-103\":\n",
    "        return \"a30104\"\n",
    "    elif tail_number == \"T-104\":\n",
    "        return \"a304bb\"\n",
    "    elif tail_number == \"T-105\":\n",
    "        return \"a30872\"\n",
    "    elif tail_number == \"T-107\":\n",
    "        return \"a30fe0\"\n",
    "    elif tail_number == \"T-12\":\n",
    "        return \"a5d891\"\n",
    "    elif tail_number == \"T-131\":\n",
    "        return \"a07f21\"\n",
    "    elif tail_number == \"T-132\":\n",
    "        return \"a4c031\"\n",
    "    elif tail_number == \"T-133\":\n",
    "        return \"a4b50c\"\n",
    "    elif tail_number == \"T-137\":\n",
    "        return \"a0956b\"\n",
    "    elif tail_number == \"T-14\":\n",
    "        return \"a8215e\"\n",
    "    elif tail_number == \"T-142\":\n",
    "        return \"a19da0\"\n",
    "    elif tail_number == \"T-15\":\n",
    "        return \"a2b7fd\"\n",
    "    elif tail_number == \"T-152\":\n",
    "        return \"ac75f9\"\n",
    "    elif tail_number == \"T-16\":\n",
    "        return \"a5dfff\"\n",
    "    elif tail_number == \"T-160\":\n",
    "        return \"ab79cc\"\n",
    "    elif tail_number == \"T-161\":\n",
    "        return \"a3f3ac\"\n",
    "    elif tail_number == \"T-162\":\n",
    "        return \"a3f763\"\n",
    "    elif tail_number == \"T-163\":\n",
    "        return \"a42299\"\n",
    "    elif tail_number == \"T-164\":\n",
    "        return \"a442aa\"\n",
    "    elif tail_number == \"T-166\":\n",
    "        return \"c07cb2\"\n",
    "    elif tail_number == \"T-167\":\n",
    "        return \"a85052\"\n",
    "    elif tail_number == \"T-168\":\n",
    "        return \"a4ea6d\"\n",
    "    elif tail_number == \"T-169\":\n",
    "        return \"a380e6\"\n",
    "    elif tail_number == \"T-210\":\n",
    "        return \"a4b460\"\n",
    "    elif tail_number == \"T-260\":\n",
    "        return \"a47cbc\"\n",
    "    elif tail_number == \"T-261\":\n",
    "        return \"a48a3a\"\n",
    "    elif tail_number == \"T-262\":\n",
    "        return \"a47197\"\n",
    "    elif tail_number == \"T-263\":\n",
    "        return \"a46de0\"\n",
    "    elif tail_number == \"T-40\":\n",
    "        return \"A9A086\"\n",
    "    elif tail_number == \"T-41\":\n",
    "        return \"NA\"\n",
    "    elif tail_number == \"T-44\":\n",
    "        return \"c00e3e\"\n",
    "    elif tail_number == \"T-910\":\n",
    "        return \"a7f642\"\n",
    "    elif tail_number == \"T-911\":\n",
    "        return \"a11cbb\"\n",
    "    elif tail_number == \"T-912\":\n",
    "        return \"a69072\"\n",
    "    elif tail_number == \"T-914\":\n",
    "        return \"a7d27a\"\n",
    "    elif tail_number == \"T-944\":\n",
    "        return \"NA\"\n",
    "    elif tail_number == \"N130CG\":\n",
    "        return \"a07b6a\"\n",
    "    else:    \n",
    "        return \"UNKNOWN_ADSB_CODE\"\n",
    "\n",
    "    \n",
    "# Create a new column \"adsbCODE\" based on the values in the \"Tail_Numbe\" column\n",
    "data[\"adsb_code\"] = data[\"TailNumber\"].apply(lambda x: map_tail_number_to_adsb_code(x))\n",
    "data.UTCdateTim = data.UTCdateTim.str[:-4]\n",
    "data['UTCdateTim']= pd.to_datetime(data['UTCdateTim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4eac1-00a6-4318-8d88-def11a8aa495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the DataFrames are already loaded as data, combined_gdf, and combined_df_vlat\n",
    "\n",
    "# Select the relevant columns from each DataFrame\n",
    "df1_selected = data[['DropID', 'Latitude1', 'Longitude1', 'UTCdateTim', 'TailNumber', 'adsb_code']]\n",
    "df2_selected = combined_gdf[['DropID', 'Latitude1', 'Longitude1', 'UTCdateTim','TailNumber', 'adsb_code']]\n",
    "df3_selected = combined_df_vlat[['dropID', 'Latitude1', 'Longitude1', 'UTCdateTim', 'adsb_code']]\n",
    "\n",
    "# Rename the 'dropID' column in df3_selected to 'DropID' for consistency\n",
    "#df3_selected.rename(columns={'dropID': 'DropID'}, inplace=True)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([df1_selected, df2_selected])\n",
    "\n",
    "# Reset the index if necessary\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "# Display the combined DataFrame\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0196fb-1cc5-403d-abce-721f218f27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565070e-89b5-4498-bd36-86e858c2228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df.to_csv('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\Output\\\\ATUdropData_2017_2023.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020420be-0e43-4be3-98db-adb8affb347c",
   "metadata": {},
   "source": [
    "### Function to map tail numbers to ADSB codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01717e24-4932-49e0-af70-e9530daab3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_tail_number_to_adsb_code(tail_number):\n",
    "#     if tail_number == \"T-01\":\n",
    "#         return \"a5cd6c\"\n",
    "#     elif tail_number == \"T-02\":\n",
    "#         return \"a5d123\"\n",
    "#     elif tail_number == \"T-03\":\n",
    "#         return \"a5d4da\"\n",
    "#     elif tail_number == \"T-05\":\n",
    "#         return \"ad66e3\"\n",
    "#     elif tail_number == \"T-06\":\n",
    "#         return \"adc0c4\"\n",
    "#     elif tail_number == \"T-07\":\n",
    "#         return \"aafd0b\"\n",
    "#     elif tail_number == \"T-10\":\n",
    "#         return \"a5c9b5\"\n",
    "#     elif tail_number == \"T-101\":\n",
    "#         return \"a2f996\"\n",
    "#     elif tail_number == \"T-102\":\n",
    "#         return \"a2fd4d\"\n",
    "#     elif tail_number == \"T-103\":\n",
    "#         return \"a30104\"\n",
    "#     elif tail_number == \"T-104\":\n",
    "#         return \"a304bb\"\n",
    "#     elif tail_number == \"T-105\":\n",
    "#         return \"a30872\"\n",
    "#     elif tail_number == \"T-107\":\n",
    "#         return \"a30fe0\"\n",
    "#     elif tail_number == \"T-12\":\n",
    "#         return \"a5d891\"\n",
    "#     elif tail_number == \"T-131\":\n",
    "#         return \"a07f21\"\n",
    "#     elif tail_number == \"T-132\":\n",
    "#         return \"a4c031\"\n",
    "#     elif tail_number == \"T-133\":\n",
    "#         return \"a4b50c\"\n",
    "#     elif tail_number == \"T-137\":\n",
    "#         return \"a0956b\"\n",
    "#     elif tail_number == \"T-14\":\n",
    "#         return \"a8215e\"\n",
    "#     elif tail_number == \"T-142\":\n",
    "#         return \"a19da0\"\n",
    "#     elif tail_number == \"T-15\":\n",
    "#         return \"a2b7fd\"\n",
    "#     elif tail_number == \"T-152\":\n",
    "#         return \"ac75f9\"\n",
    "#     elif tail_number == \"T-16\":\n",
    "#         return \"a5dfff\"\n",
    "#     elif tail_number == \"T-160\":\n",
    "#         return \"ab79cc\"\n",
    "#     elif tail_number == \"T-161\":\n",
    "#         return \"a3f3ac\"\n",
    "#     elif tail_number == \"T-162\":\n",
    "#         return \"a3f763\"\n",
    "#     elif tail_number == \"T-163\":\n",
    "#         return \"a42299\"\n",
    "#     elif tail_number == \"T-164\":\n",
    "#         return \"a442aa\"\n",
    "#     elif tail_number == \"T-166\":\n",
    "#         return \"c07cb2\"\n",
    "#     elif tail_number == \"T-167\":\n",
    "#         return \"a85052\"\n",
    "#     elif tail_number == \"T-168\":\n",
    "#         return \"a4ea6d\"\n",
    "#     elif tail_number == \"T-169\":\n",
    "#         return \"a380e6\"\n",
    "#     elif tail_number == \"T-210\":\n",
    "#         return \"a4b460\"\n",
    "#     elif tail_number == \"T-260\":\n",
    "#         return \"a47cbc\"\n",
    "#     elif tail_number == \"T-261\":\n",
    "#         return \"a48a3a\"\n",
    "#     elif tail_number == \"T-262\":\n",
    "#         return \"a47197\"\n",
    "#     elif tail_number == \"T-263\":\n",
    "#         return \"a46de0\"\n",
    "#     elif tail_number == \"T-40\":\n",
    "#         return \"A9A086\"\n",
    "#     elif tail_number == \"T-41\":\n",
    "#         return \"NA\"\n",
    "#     elif tail_number == \"T-44\":\n",
    "#         return \"c00e3e\"\n",
    "#     elif tail_number == \"T-910\":\n",
    "#         return \"a7f642\"\n",
    "#     elif tail_number == \"T-911\":\n",
    "#         return \"a11cbb\"\n",
    "#     elif tail_number == \"T-912\":\n",
    "#         return \"a69072\"\n",
    "#     elif tail_number == \"T-914\":\n",
    "#         return \"a7d27a\"\n",
    "#     elif tail_number == \"T-944\":\n",
    "#         return \"NA\"\n",
    "#     elif tail_number == \"N130CG\":\n",
    "#         return \"a07b6a\"\n",
    "#     else:    \n",
    "#         return \"UNKNOWN_ADSB_CODE\"\n",
    "\n",
    "    \n",
    "# # Create a new column \"adsbCODE\" based on the values in the \"Tail_Numbe\" column\n",
    "# combined_df[\"adsbCODE\"] = combined_df[\"TailNumber\"].apply(lambda x: map_tail_number_to_adsb_code(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef4bef-d29a-4e88-bbab-8cd3d24d0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "file_path = 'C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\Output\\\\ATUdropData_2017_2023.csv'\n",
    "df = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dbed9-3add-40c8-9a15-5b17e93e4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb04e02-9d01-44d9-923c-7361c4397db1",
   "metadata": {},
   "source": [
    "### Create columns to define the start and end of the query (identify the space-time bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed010c1f-0780-4914-86e2-2c5f146bc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#data.UTCdateTim = data.UTCdateTim.str[:-4]\n",
    "\n",
    "#print(data.UTCdateTim)\n",
    "#data['UTCdateTim']= pd.to_datetime(data['UTCdateTim'])\n",
    "\n",
    "# isolates the drop time - 30 second window\n",
    "time_change = timedelta(minutes=0.25)\n",
    "data[\"Time2\"] = data.UTCdateTim + time_change\n",
    "data[\"Time1\"] = data.UTCdateTim - time_change\n",
    "\n",
    "time_change2 = timedelta(minutes=720)\n",
    "data[\"Time2_2\"] = data.UTCdateTim + time_change2\n",
    "data[\"Time1_2\"] = data.UTCdateTim - time_change2\n",
    "\n",
    "# define bounding box\n",
    "data[\"Latitude1PLUS\"] = data.Latitude1+0.02\n",
    "data[\"Latitude1MINUS\"] = data.Latitude1-0.02\n",
    "\n",
    "data[\"Longitude1PLUS\"] = data.Longitude1+0.02\n",
    "data[\"Longitude1MINUS\"] = data.Longitude1-0.02\n",
    "\n",
    "dataLatitude1PLUS =data[\"Latitude1PLUS\"].tolist()\n",
    "dataLatitude1MINUS =data[\"Latitude1MINUS\"].tolist()\n",
    "\n",
    "dataLongitude1PLUS =data[\"Longitude1PLUS\"].tolist()\n",
    "dataLongitude1MINUS =data[\"Longitude1MINUS\"].tolist()\n",
    "\n",
    "dataLongitude1 =  data.Longitude1.tolist()\n",
    "dataLatitude1 = data.Latitude1.tolist()\n",
    "\n",
    "# 15 seconds prior to and post start of drop\n",
    "datetime1 = data.Time1.tolist()\n",
    "datetime2 = data.Time2.tolist()\n",
    "\n",
    "# 3 minutes prior to and after the start of the drop (for non-drop samples)\n",
    "datetime1_2 = data.Time1_2.tolist()\n",
    "datetime2_2 = data.Time2_2.tolist()\n",
    "\n",
    "\n",
    "data=data.replace(regex=[':'], value='_')\n",
    "data=data.replace(regex=['-'], value='_')\n",
    "data=data.replace(regex=[' '], value='')\n",
    "data_DropID = data.DropID.tolist()\n",
    "\n",
    "print(len(data_DropID))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121210ed-cdc7-47a0-bfae-b503d541c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyopensky import OpenskyImpalaWrapper\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "import csv\n",
    "\n",
    "OutputDirectoryDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputDropSamples2\\\\\"\n",
    "\n",
    "OutputDirectoryNonDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamples2\\\\\"\n",
    "\n",
    "opensky = OpenskyImpalaWrapper()\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    print(i)\n",
    "    \n",
    "    # Perform a query with ICAO filter\n",
    "    df = opensky.query(\n",
    "        type=\"adsb\",\n",
    "        #start = datetime1[i],\n",
    "        #end = datetime2[i],\n",
    "        start = datetime1_utc[i],\n",
    "        end = datetime2_utc[i],\n",
    "        bound=[dataLatitude1MINUS[i],dataLongitude1MINUS[i],dataLatitude1PLUS[i],dataLongitude1PLUS[i]],\n",
    "        #icao24 = adsbCODES\n",
    "        icao24 = dataADSBLIST[i]\n",
    "        #icao24 = adsbCODES[i]\n",
    "        #icao24 = IAOC24_CODES\n",
    "    )\n",
    "    \n",
    "    if df is not None:        \n",
    "        # this filters the data and ensure there is continuous quality data. The main issue I was facing was limited data availability, \n",
    "        # particularly in low elevation mountainous regions with limited ADSB coverage by OpenSky\n",
    "        # this will drastically limit the avalible data\n",
    "        if (df['lastposupdate'].nunique() > (len(df)-15)) is True and (df['icao24'].nunique() == 1) is True and (len(df) > 25) is True:\n",
    "        #if (len(df) > 25) is True:\n",
    "\n",
    "            icaocode24 = df['icao24']\n",
    "            print(\"itsUniqueEnough\")\n",
    "            df2 = df.dropna(subset = [\"lat\"])          # Apply dropna() function to remove missing lat lon \n",
    "            if len(df2) > 0:\n",
    "                filepathcsv = OutputDirectoryDropSamples+data_DropID[i]+\".csv\"\n",
    "                df2.to_csv(filepathcsv)\n",
    "                filepathshp = OutputDirectoryDropSamples+data_DropID[i]+\".shp\"\n",
    "                driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "                data_src = driver.CreateDataSource(filepathshp)\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromEPSG(4326)# 4326 = wgs84\n",
    "                layer = data_src.CreateLayer(filepathshp, \n",
    "                                             srs, \n",
    "                                             geom_type = ogr.wkbPoint)\n",
    "\n",
    "                #Create attribute fields from OpenSky\n",
    "                field_name = ogr.FieldDefn(\"time\", ogr.OFTString)\n",
    "                field_name.SetWidth(50)\n",
    "                layer.CreateField(field_name)\n",
    "                layer.CreateField(ogr.FieldDefn(\"lon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lat\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"velocity\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"heading\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"vertrate\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"callsign\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"onground\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"alert\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"spi\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"squawk\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"baro\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"geo\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastpos\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastcon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"hour\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"icao24\", ogr.OFTString))\n",
    "  \n",
    "                with open(filepathcsv, \"r\") as csv_file:\n",
    "                    csv_reader = csv.reader(csv_file)\n",
    "                    next(csv_reader)\n",
    "                    for row in csv_reader:        \n",
    "                        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "                        feature.SetField(\"time\", row[1])\n",
    "                        feature.SetField(\"lon\", row[4])\n",
    "                        feature.SetField(\"lat\", row[3])\n",
    "                        feature.SetField(\"velocity\", row[5])\n",
    "                        feature.SetField(\"heading\", row[6])\n",
    "                        feature.SetField(\"vertrate\", row[7])\n",
    "                        feature.SetField(\"callsign\", row[8])\n",
    "                        feature.SetField(\"onground\", row[9])\n",
    "                        feature.SetField(\"alert\", row[10])\n",
    "                        feature.SetField(\"spi\", row[11])\n",
    "                        feature.SetField(\"squawk\", row[12])\n",
    "                        feature.SetField(\"baro\", row[13])\n",
    "                        feature.SetField(\"geo\", row[14])\n",
    "                        feature.SetField(\"lastpos\", row[15])\n",
    "                        feature.SetField(\"lastcon\", row[16])\n",
    "                        feature.SetField(\"hour\", row[17])\n",
    "                        feature.SetField(\"icao24\", row[2])\n",
    "\n",
    "                        #Create point geometry\n",
    "                        point = ogr.Geometry(ogr.wkbPoint)\n",
    "                        point.AddPoint(float(row[4]), float(row[3]))\n",
    "\n",
    "                        #Create the feature and set the values \n",
    "                        feature.SetGeometry(point)\n",
    "                        layer.CreateFeature(feature)\n",
    "                        # reset features for next row\n",
    "                        feature = None\n",
    "                data_src = None\n",
    "            \n",
    "            ##\n",
    "            dfLong = opensky.query(\n",
    "                type=\"adsb\",\n",
    "                start = datetime1_2_utc[i],\n",
    "                end = datetime2_2_utc[i],\n",
    "                icao24 = icaocode24\n",
    "                )\n",
    "            \n",
    "            df2_long = dfLong.dropna(subset = [\"lat\"])          # Apply dropna() function to remove missing lat lon \n",
    "            if len(df2_long) > 0:\n",
    "                filepathcsv = OutputDirectoryNonDropSamples+data_DropID[i]+\".csv\"\n",
    "                df2_long.to_csv(filepathcsv)\n",
    "                filepathshp = OutputDirectoryNonDropSamples+data_DropID[i]+\".shp\"\n",
    "                driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "                data_src = driver.CreateDataSource(filepathshp)\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromEPSG(4326)# 4326 = wgs84\n",
    "                layer = data_src.CreateLayer(filepathshp, \n",
    "                                             srs, \n",
    "                                             geom_type = ogr.wkbPoint)\n",
    "\n",
    "                #Create attribute fields from OpenSky\n",
    "                field_name = ogr.FieldDefn(\"time\", ogr.OFTString)\n",
    "                field_name.SetWidth(50)\n",
    "                layer.CreateField(field_name)\n",
    "                layer.CreateField(ogr.FieldDefn(\"lon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lat\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"velocity\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"heading\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"vertrate\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"callsign\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"onground\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"alert\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"spi\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"squawk\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"baro\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"geo\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastpos\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastcon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"hour\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"icao24\", ogr.OFTString))\n",
    "\n",
    "                with open(filepathcsv, \"r\") as csv_file:\n",
    "                    csv_reader = csv.reader(csv_file)\n",
    "                    next(csv_reader)\n",
    "                    for row in csv_reader:        \n",
    "                        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "                        feature.SetField(\"time\", row[1])\n",
    "                        feature.SetField(\"lon\", row[4])\n",
    "                        feature.SetField(\"lat\", row[3])\n",
    "                        feature.SetField(\"velocity\", row[5])\n",
    "                        feature.SetField(\"heading\", row[6])\n",
    "                        feature.SetField(\"vertrate\", row[7])\n",
    "                        feature.SetField(\"callsign\", row[8])\n",
    "                        feature.SetField(\"onground\", row[9])\n",
    "                        feature.SetField(\"alert\", row[10])\n",
    "                        feature.SetField(\"spi\", row[11])\n",
    "                        feature.SetField(\"squawk\", row[12])\n",
    "                        feature.SetField(\"baro\", row[13])\n",
    "                        feature.SetField(\"geo\", row[14])\n",
    "                        feature.SetField(\"lastpos\", row[15])\n",
    "                        feature.SetField(\"lastcon\", row[16])\n",
    "                        feature.SetField(\"hour\", row[17])\n",
    "                        feature.SetField(\"icao24\", row[2])\n",
    "\n",
    "\n",
    "                        #Create point geometry\n",
    "                        point = ogr.Geometry(ogr.wkbPoint)\n",
    "                        point.AddPoint(float(row[4]), float(row[3]))\n",
    "\n",
    "                        #Create the feature and set the values \n",
    "                        feature.SetGeometry(point)\n",
    "                        layer.CreateFeature(feature)\n",
    "                        # reset features for next row\n",
    "                        feature = None\n",
    "                data_src = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a813404-fa6f-4710-acd7-1c787ffb863b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256570c5-02f2-40eb-bfee-4aa34c3909bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb055e-6132-4bcc-89a3-6e0857905aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ded71-b666-4388-8f1f-1cdf156bdb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e7b51-9acb-43e7-85a5-18872f9fd12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee778b-6bf0-4916-9140-eee3107c86ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79da48dd-f43c-4a90-b12e-4f0455d277ec",
   "metadata": {},
   "source": [
    "# Calculate HeightAGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c0900f-2dfc-441f-bcf7-55986e2e9acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin-magstadt\\anaconda3\\envs\\ee\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.9.1-CAPI-1.14.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: 'geo'\n",
      "Error occurred: 'geo'\n",
      "Error occurred: 'geo'\n",
      "Error occurred: 'geo'\n",
      "Error occurred: 'geo'\n",
      "Error occurred: 'geo'\n",
      "Error occurred: Invalid JSON payload received. Unexpected token.\n",
      ": {\"constantValue\": NaN}, \"vertical_r\": \n",
      "                    ^\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "\n",
    "#ee.Authenticate()\n",
    "\n",
    "#ee.Initialize(project = 'ee-magstadt')\n",
    "ee.Initialize()\n",
    "\n",
    "\n",
    "#input_folder = pathshp\n",
    "#input_folder = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputDropSamples\\\\\"\n",
    "input_folder = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\DropSamples\\\\\"\n",
    "#output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamplesWAGL\\\\\"\n",
    "#output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPSAMPLES\\\\\"\n",
    "output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\DropSamplesAGL\\\\\"\n",
    "\n",
    "desired_columns_order = ['heading', 'icao24', 'lon', 'onground', 'velocity', 'lastpos', 'spi', 'geo', 'vertrate', 'baro', 'squawk', 'hour', 'alert', 'callsign', 'time', 'lastcon', 'lat', 'elevation', 'slope', 'hillshade', 'aspect', 'heightAGL']\n",
    "\n",
    "\n",
    "# Loop over all shapefiles in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".shp\"):\n",
    "        #print(\"Processing:\", filename)\n",
    "        # Construct input and output file paths\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder_short, filename[:-4] + \".csv\")  # Remove \".shp\" from input filename and add \".csv\"\n",
    "        \n",
    "        # Run the existing script with the input and output file paths\n",
    "        import ee\n",
    "        import geemap\n",
    "        import pandas as pd\n",
    "        import time\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            ee_fc = geemap.shp_to_ee(input_path)\n",
    "            buffered_geometry = ee_fc.geometry().bounds().buffer(1000)  # Adjust buffer distance as needed\n",
    "\n",
    "#             # NEW\n",
    "#             df = pd.read_csv(input_path)\n",
    "\n",
    "#             # Function to create a feature from a row\n",
    "#             def row_to_feature(row):\n",
    "#                 geom = ee.Geometry.Point([row['lon'], row['lat']])\n",
    "#                 feature = ee.Feature(geom, row.to_dict())\n",
    "#                 return feature\n",
    "\n",
    "#             # Create a list of ee.Feature objects\n",
    "#             features = df.apply(row_to_feature, axis=1).tolist()\n",
    "\n",
    "#             # Convert the list of features into a FeatureCollection\n",
    "#             ee_feature_collection = ee.FeatureCollection(features)\n",
    "#             # end new\n",
    "           \n",
    "            #Print the number of features in the FeatureCollection\n",
    "            num_features = ee_fc.size().getInfo()\n",
    "            #print(f\"Number of features in {filename}: {num_features}\")\n",
    "#             shapefile = gpd.read_file(input_path)\n",
    "#             ee_fc = ee.Geometry.MultiPolygon(shapefile)#.geometry.to_crs(epsg='4326').map(lambda x: x.__geo_interface__['coordinates']))\n",
    "\n",
    "            \n",
    "            \n",
    "             # Load the 3DEP dataset  and others and filter by the extent of the points\n",
    "            dem = ee.Image(\"USGS/3DEP/10m\").clip(buffered_geometry)#ee_fc.geometry().bounds())\n",
    "            \n",
    "            #dem = dem.where(dem.mask().Not(), 0)\n",
    "        \n",
    "            # Extract data for each image at appropriate scale\n",
    "            terrain_fc = ee.Terrain.products(dem).sampleRegions(collection=ee_fc, scale=10)#, geometries=False)\n",
    "            #terrain_fc_size = terrain_fc.size()\n",
    "\n",
    "            # Fetch the size (number of features) as a client-side value\n",
    "            #print(terrain_fc_size.getInfo(), \"ddd\")\n",
    "\n",
    "            # Convert the elevation and terrain product data to Pandas DataFrames\n",
    "            terrain_df = geemap.ee_to_pandas(terrain_fc)\n",
    "            #print(len(terrain_df), \"ss\")  # Print the columns to check the actual column names\n",
    "            \n",
    "            #print(terrain_df)\n",
    "\n",
    "            # Drop rows with missing values\n",
    "            df_sample = terrain_df#.dropna()\n",
    "            #df_sample = terrain_df.fillna(method='bfill').dropna()\n",
    "            \n",
    "            #print(len(df_sample))  # Print the columns to check the actual column names\n",
    "\n",
    "            # Calculate height AGL\n",
    "            baro_altitude = df_sample['geo']\n",
    "            elevation = df_sample['elevation']\n",
    "            height_AGL = baro_altitude - elevation\n",
    "            df_sample['heightAGL'] = height_AGL\n",
    "            \n",
    "            df_sample = df_sample.reindex(columns=desired_columns_order)\n",
    "\n",
    "            # Export as CSV\n",
    "            #df_sample.head(30).to_csv(output_path, index=False)\n",
    "            df_sample.to_csv(output_path, index=False)\n",
    "\n",
    "            #print(\"Completed:\", filename)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "            continue\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5e410-5b17-4a0a-924f-cf3dae0ed9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d1cfd-de53-4b11-ab7f-74ed53d36c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51271529-dded-4387-85ed-104529b53734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#COMBINE ALLDROPS\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing CSV files\n",
    "folder_path = r\"C:\\Users\\admin-magstadt\\Desktop\\CLEANATUDROPDATA\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames from each CSV file\n",
    "dfs = []\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read each CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "# Concatenate all DataFrames in the list to create one big DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(combined_df)\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the folder containing CSV files\n",
    "folder_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\Output\\OutputNonDropSamples\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames from each CSV file\n",
    "dfs = []\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read each CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "# Concatenate all DataFrames in the list to create one big DataFrame\n",
    "combined_df_nondrop = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(len(combined_df_nondrop))\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df_nondrop\n",
    "\n",
    "combined_df_nondrop_noDup = combined_df_nondrop.drop_duplicates()\n",
    "\n",
    "print(len(combined_df_nondrop_noDup))\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_non_drop_samples = combined_df_nondrop_noDup\n",
    "\n",
    "# Extract values from the \"time\" column in the second DataFrame\n",
    "drop_samples_times = combined_df[\"time\"].tolist()\n",
    "drop_samples_icao24 = combined_df[\"icao24\"].tolist()\n",
    "\n",
    "# Filter the first DataFrame to exclude rows where the \"time\" column matches values from the second DataFrame\n",
    "filtered_df_non_drop_samples = df_non_drop_samples[~df_non_drop_samples[['time', 'icao24']].apply(tuple, axis=1).isin(zip(drop_samples_times, drop_samples_icao24))]\n",
    "\n",
    "# Calculate the number of chunks\n",
    "num_chunks = len(filtered_df_non_drop_samples) // 30\n",
    "print(num_chunks)\n",
    "# Iterate through chunks and export as CSV\n",
    "for i in range(num_chunks):\n",
    "    chunk = filtered_df_non_drop_samples.iloc[i*30:(i+1)*30]\n",
    "    # Check if the number of unique times in the chunk is 30\n",
    "    time_diff = (chunk['time'].max()+1) - chunk['time'].min()\n",
    "    unique_times_count = chunk['lastcontact'].nunique()\n",
    "    unique_ADSB_count = chunk['icao24'].nunique()\n",
    "\n",
    "    if time_diff == 30 and unique_times_count > 26 and unique_ADSB_count==1:\n",
    "        file_name = \"combined_df_nondrop_noDup_chunk\"  # Change the file name as needed\n",
    "        chunk.to_csv(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TESTAGL\\\\{file_name}_{i+1}.csv\", index=False)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(chunk, geometry=gpd.points_from_xy(chunk['lon'], chunk['lat']))\n",
    "        gdf.crs = 'epsg:4326'  # Set the CRS as needed\n",
    "        gdf.to_file(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TESTAGL\\\\{file_name}_{i+1}.shp\")      \n",
    "        \n",
    "    else:\n",
    "        print(f\"Skipping chunk {i+1} as it does not have 30 unique times.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58468a4c-3204-46a4-8bb0-daf862f3159b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d95ec5e8-4413-4457-9aa2-d1d75481cbd8",
   "metadata": {},
   "source": [
    "# Prepare drop and non drop samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de79ba-a952-4870-8c8b-20d9c8136173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData2\\\\DropSamplesADSB\\\\\"\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData\\\\SmoothedDropSamples\\\\\"\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPDATA\\\\\"\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TEST\\\\DROP\\\\\"\n",
    "dir_path_dropsamples = r\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPSAMPLES\\\\\"\n",
    "\n",
    "\n",
    "filenames = glob.glob(dir_path_dropsamples + \"*.csv\")\n",
    "data = []\n",
    "usecols = [8,21]  # Specified columns to use\n",
    "\n",
    "#usecols = [4,22,21]  # Specified columns to use\n",
    "#usecols = [4,8]  # Specified columns to use\n",
    "\n",
    "for filename in filenames:\n",
    "    num_rows = sum(1 for line in open(filename))\n",
    "    if num_rows < 26:\n",
    "        continue\n",
    "    skip_rows = max(0, num_rows - 30)  # Adjust to ensure reading up to the last 30 rows\n",
    "    df = pd.read_csv(filename, usecols=usecols, skiprows=skip_rows)\n",
    "    #df = df.dropna()\n",
    "    arr = df.values.astype(float)\n",
    "    # Check if the array has fewer rows than expected and pad if necessary\n",
    "    if arr.shape[0] < 30:\n",
    "        pad_size = 30 - arr.shape[0]\n",
    "        # Use the last row of arr for padding\n",
    "        last_row = arr[-1:]\n",
    "        padding = np.repeat(last_row, pad_size, axis=0)\n",
    "        arr = np.vstack([padding, arr])  # Prepend the padding\n",
    "    data.append(arr)\n",
    "\n",
    "if len(data) == 0:\n",
    "    print(\"No data found\")\n",
    "else:\n",
    "    array1 = np.stack(data, axis=0)\n",
    "    #print(array1.shape)\n",
    "    array1 = np.stack(data, axis=0)\n",
    "    print(array1.shape)\n",
    "\n",
    "    # Check for NaN values\n",
    "    nan_indices = np.argwhere(np.isnan(array1))\n",
    "    if len(nan_indices) > 0:\n",
    "        print(\"NaN values found at indices:\", nan_indices)\n",
    "    else:\n",
    "        print(\"No NaN values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77655243-26f2-4cd9-b1b1-37ed04e75433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "dir_path_dropsamples = r\"C:\\Users\\admin-magstadt\\Desktop\\CLEANATUNONDROPSAMPELS\\\\\"\n",
    "filenames = glob.glob(dir_path_dropsamples + \"*.csv\")\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    num_rows = sum(1 for line in open(filename))\n",
    "    if num_rows < 26:\n",
    "        continue\n",
    "    skip_rows = max(0, num_rows - 30)\n",
    "    df = pd.read_csv(filename, usecols=usecols, skiprows=skip_rows)\n",
    "    \n",
    "    # Drop rows with any NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    arr = df.values.astype(float)\n",
    "    \n",
    "    if arr.shape[0] < 30:\n",
    "        pad_size = 30 - arr.shape[0]\n",
    "        last_row = arr[-1:]\n",
    "        padding = np.repeat(last_row, pad_size, axis=0)\n",
    "        arr = np.vstack([padding, arr])\n",
    "    \n",
    "    data.append(arr)\n",
    "\n",
    "if len(data) == 0:\n",
    "    print(\"No data found\")\n",
    "else:\n",
    "    array2 = np.stack(data, axis=0)\n",
    "    print(array2.shape)\n",
    "\n",
    "    nan_indices = np.argwhere(np.isnan(array2))\n",
    "    if len(nan_indices) > 0:\n",
    "        print(\"NaN values found at indices:\", nan_indices)\n",
    "    else:\n",
    "        print(\"No NaN values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d8377-5bf7-4e93-91c8-4d6be7ad5a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33090a27-a8ec-4272-81f2-45dd5e796e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c4015-6c51-4395-975c-1633bca8bdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb531f-21f9-4361-9cb9-590a7f7a2593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b91f6-479f-40ec-aabf-90889be9eaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa41d31-6efb-4041-9d8f-978df75ed912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abdb9b4-929f-4f14-9d8c-e8d3cd70d8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f213bf8-3447-4f02-957c-538e351797e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
