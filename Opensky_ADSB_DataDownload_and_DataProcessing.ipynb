{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08e47ec-cce8-48fe-8ea2-14b94b9dacdf",
   "metadata": {},
   "source": [
    "# FIGURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6546e88-cb05-43cf-8003-0137399a81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "#shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\LAT_20230101_20231231_FullYear.shp\"\n",
    "shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2023_Full_Year_03112024_VLAT.shp\"\n",
    "\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf = gdf[gdf['LineLenMi'] <= .20]\n",
    "\n",
    "# print(len(gdf))\n",
    "# gdf = gdf[gdf['AC_Type'] == 'LAT']\n",
    "# print(len(gdf))\n",
    "\n",
    "# Check the columns in the shapefile\n",
    "print(gdf.columns)\n",
    "\n",
    "# Assuming the dropTimeAI column exists, calculate the average drop time\n",
    "#average_drop_time = gdf['TotalDropT'].mean()\n",
    "average_drop_time = gdf['dropTimeAl'].mean()\n",
    "\n",
    "\n",
    "print(\"Average drop time:\", average_drop_time)\n",
    "\n",
    "gdf['dropTimeAl'] = pd.to_numeric(gdf['dropTimeAl'], errors='coerce')\n",
    "\n",
    "# Filter out values greater than 20 seconds in the 'dropTimeAl' column\n",
    "filtered_gdf = gdf[gdf['dropTimeAl'] <= 20]\n",
    "\n",
    "# Check the columns in the filtered shapefile\n",
    "print(filtered_gdf.columns)\n",
    "\n",
    "# Calculate the average drop time from the filtered data\n",
    "average_drop_time = filtered_gdf['dropTimeAl'].mean()\n",
    "\n",
    "print(\"Average drop time (filtered):\", average_drop_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75df2a0-11ae-4b8a-a5cc-dc3d3df9d2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34362ae-fe1e-4201-b63b-05f5360007fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# Determine the center of the time axis for the vertical line\n",
    "time_center = data['time'].iloc[len(data['time']) // 2] + 1\n",
    "\n",
    "# Configure each subplot\n",
    "for ax, y_label, y_data, label, ylim in zip(\n",
    "    axs,\n",
    "    ['Velocity (m/s)', 'Vertical Rate (m/s)', 'Height Above Ground Level (m)'],\n",
    "    [data['velocity'], data['vertrate'], data['heightAGL']],\n",
    "    [' ', ' ', ' '],\n",
    "    [(65, 85), (-20, 10), (0, 800)]\n",
    "):\n",
    "    ax.plot(data['time'], y_data, color='black')\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    #ax.axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.text(0.95, 0.86, label, transform=ax.transAxes, fontsize=16, color='black', va='bottom')\n",
    "    ax.set_ylim(*ylim)  # Set the y-limits\n",
    "    # Remove top and right borders\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.01, 'Unix Time (s)', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\figureone_DropSampleEx_FORPPT.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ba4d5-4989-4928-9d5d-0f26487e1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# Determine the center of the time axis for the vertical line\n",
    "time_center = data['time'].iloc[len(data['time']) // 2] + 1\n",
    "\n",
    "# Configure each subplot\n",
    "for ax, y_label, y_data, label, ylim in zip(\n",
    "    axs,\n",
    "    ['Velocity (m/s)', 'Vertical Rate (m/s)', 'Height Above Ground Level (m)'],\n",
    "    [data['velocity'], data['vertrate'], data['heightAGL']],\n",
    "    ['(a)', ' ', ' '],\n",
    "    [(65, 85), (-20, 10), (0, 800)]\n",
    "):\n",
    "    ax.plot(data['time'], y_data, color='black')\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    ax.axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.text(0.95, 0.86, label, transform=ax.transAxes, fontsize=16, color='black', va='bottom')\n",
    "    ax.set_ylim(*ylim)  # Set the y-limits\n",
    "    # Remove top and right borders\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.01, 'Unix Time (s)', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\figureone_DropSampleEx.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c13aa-f133-4eeb-ad08-b4798cc986e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# Determine the center of the time axis for the vertical line\n",
    "time_center = data['time'].iloc[len(data['time']) // 2] + 1\n",
    "\n",
    "# Configure each subplot\n",
    "for ax, y_label, y_data, label, ylim in zip(\n",
    "    axs,\n",
    "    ['Velocity (m/s)', 'Vertical Rate (m/s)', 'Height Above Ground Level (m)'],\n",
    "    [data['velocity'], data['vertrate'], data['heightAGL']],\n",
    "    ['(a)', ' ', ' '],\n",
    "    [(65, 85), (-20, 10), (0, 800)]\n",
    "):\n",
    "    ax.plot(data['time'], y_data, color='black')\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    ax.axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.text(0.95, 0.86, label, transform=ax.transAxes, fontsize=16, color='black', va='bottom')\n",
    "    ax.set_ylim(*ylim)  # Set the y-limits\n",
    "    # Remove top and right borders\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.01, 'Unix Time (s)', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\figureone_DropSampleEx.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80499750-c477-430b-832a-6caca440482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\NonDropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# Determine the center of the time axis for the vertical line\n",
    "time_center = data['time'].iloc[len(data['time']) // 2]\n",
    "\n",
    "# Plot for the first graph\n",
    "axs[0].plot(data['time'], data['velocity'], color='black')\n",
    "axs[0].set_ylabel('Velocity (m/s)', fontsize=12)\n",
    "axs[0].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "axs[0].text(0.95, 0.86, '(b)', transform=axs[0].transAxes, fontsize=16, color='black', va='bottom')\n",
    "axs[0].set_ylim(65, 85)  # Set the y-limits here\n",
    "\n",
    "# Plot for the second graph\n",
    "axs[1].plot(data['time'], data['vertrate'], color='black')\n",
    "axs[1].set_ylabel('Vertical Rate (m/s)', fontsize=12)\n",
    "axs[1].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "axs[1].text(0.95, 0.86, ' ', transform=axs[1].transAxes, fontsize=16, color='black', va='bottom')\n",
    "axs[1].set_ylim(-20, 10)  # Set the y-limits here\n",
    "\n",
    "# Plot for the third graph\n",
    "axs[2].plot(data['time'], data['heightAGL'], color='black')\n",
    "axs[2].set_ylabel('Height Above Ground Level (m)', fontsize=12)\n",
    "axs[2].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "axs[2].text(0.95, 0.86, '', transform=axs[2].transAxes, fontsize=16, color='black', va='bottom')\n",
    "axs[2].set_ylim(0, 800)  # Set the y-limits here\n",
    "\n",
    "# Set the x-axis limit from 15 to 45\n",
    "# Assuming you want to set the limits relative to your data's time range\n",
    "time_min = data['time'].min()  # Use actual time data to set limits correctly\n",
    "time_max = data['time'].max()\n",
    "for ax in axs:\n",
    "    ax.set_xlim(time_min, time_max)\n",
    "\n",
    "# Remove top and right borders for each subplot\n",
    "for ax in axs:\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim(1557185795, 1557185795+30)\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, 0.01, 'Unix Time (s)', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\figureone_NonDropSampleEx.png\")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e4c09-3ffd-46ec-8102-4deef0e4eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\NonDropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcece787-eab5-461d-9f4e-087cc64cff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Read the CSV files\n",
    "# file_path_drop = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "# file_path_nondrop = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\NonDropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "# data_drop = pd.read_csv(file_path_drop)\n",
    "# data_nondrop = pd.read_csv(file_path_nondrop)\n",
    "\n",
    "# # Create a figure and subplots\n",
    "# fig, axs = plt.subplots(3, 2, figsize=(16, 12), sharex='col')\n",
    "\n",
    "# # Plot settings\n",
    "# colors = ['black', 'red', 'green']\n",
    "# labels = ['Velocity', 'vertrate', 'heightAGL']\n",
    "# ylims = [(0, 100), (-25, 25), (0, 800)]\n",
    "\n",
    "# # Plot each variable in a separate row\n",
    "# for i, (label, color, ylim) in enumerate(zip(labels, colors, ylims)):\n",
    "#     # Drop Samples Plot\n",
    "#     axs[i, 0].plot(data_drop['time'], data_drop[label.lower()], color=color, label=label)\n",
    "#     axs[i, 0].set_ylabel(label)\n",
    "#     axs[i, 0].set_ylim(ylim)\n",
    "#     axs[i, 0].legend(loc='upper left')\n",
    "    \n",
    "#     # Non-Drop Samples Plot\n",
    "#     axs[i, 1].plot(data_nondrop['time'], data_nondrop[label.lower()], color=color, label=label)\n",
    "#     axs[i, 1].set_ylabel(label)\n",
    "#     axs[i, 1].set_ylim(ylim)\n",
    "#     axs[i, 1].legend(loc='upper left')\n",
    "\n",
    "# # Title for columns\n",
    "# axs[0, 0].set_title('Drop Samples ADSB')\n",
    "# axs[0, 1].set_title('Non-Drop Samples ADSB')\n",
    "\n",
    "# # X-axis label for the bottom plots\n",
    "# for ax in axs[2]:\n",
    "#     ax.set_xlabel('Unix Time (s)')\n",
    "\n",
    "# # Remove the spines on the top and right for all plots\n",
    "# for ax in axs.flat:\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199fedd8-7c82-4a3e-8b56-51811f92a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Read the CSV file\n",
    "# file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_131_18_25_42.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_137_20_02_44.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_137_20_07_24.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_142_22_22_42.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_142_22_33_59.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_155_23_41_03.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_157_20_39_30.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_17_22_50.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_18_18_58.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_18_19_04.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_19_16_51.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_20_07_11.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "# # Create a figure and a set of subplots\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# # Determine the center of the time axis for the vertical line\n",
    "# time_center = data['time'].iloc[len(data['time']) // 2]+1\n",
    "\n",
    "# # Plot for the first graph\n",
    "# axs[0].plot(data['time'], data['velocity'], color='black')\n",
    "# axs[0].set_ylabel('Velocity (m/s)', fontsize=12)\n",
    "# axs[0].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "# axs[0].text(0.95, 0.86, '(a)', transform=axs[0].transAxes, fontsize=16, color='black', va='bottom')\n",
    "# axs[0].set_ylim(65, 85)  # Set the y-limits here\n",
    "\n",
    "# # Plot for the second graph\n",
    "# axs[1].plot(data['time'], data['vertrate'], color='black')\n",
    "# axs[1].set_ylabel('Vertical Rate (m/s)', fontsize=12)\n",
    "# axs[1].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "# axs[1].text(0.95, 0.86, '(b)', transform=axs[1].transAxes, fontsize=16, color='black', va='bottom')\n",
    "# axs[1].set_ylim(-20, 10)  # Set the y-limits here\n",
    "\n",
    "# # Plot for the third graph\n",
    "# axs[2].plot(data['time'], data['heightAGL'], color='black')\n",
    "# axs[2].set_ylabel('Height Above Ground Level (m)', fontsize=12)\n",
    "# axs[2].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "# axs[2].text(0.95, 0.86, '(c)', transform=axs[2].transAxes, fontsize=16, color='black', va='bottom')\n",
    "# axs[2].set_ylim(0, 800)  # Set the y-limits here\n",
    "\n",
    "# # Set common labels\n",
    "# fig.text(0.5, 0.01, 'Unix Time (s)', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# # Improve layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4898a-5ecc-40d9-a5b2-2cebbf82b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Read the CSV file\n",
    "# file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\NonDropSamplesADSB\\N130CG_2019_126_23_38_59.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_131_18_25_42.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_137_20_02_44.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_137_20_07_24.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_142_22_22_42.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_142_22_33_59.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_155_23_41_03.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_157_20_39_30.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_17_22_50.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_18_18_58.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_18_19_04.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_19_16_51.csv\"\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\\N130CG_2019_160_20_07_11.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "# # Create a figure and a set of subplots\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# # Plot for the first graph\n",
    "# axs[0].plot(data['time'], data['velocity'], color='black')\n",
    "# axs[0].set_ylabel('Velocity (m/s)', fontsize=12)\n",
    "# axs[0].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "# axs[0].text(0.95, 0.86, '(a)', transform=axs[0].transAxes, fontsize=16, color='black', va='bottom')\n",
    "# axs[0].set_ylim(65, 85)  # Set the y-limits here\n",
    "\n",
    "# # Plot for the second graph\n",
    "# axs[1].plot(data['time'], data['vertrate'], color='black')\n",
    "# axs[1].set_ylabel('Vertical Rate (m/s)', fontsize=12)\n",
    "# axs[1].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "# axs[1].text(0.95, 0.86, '(b)', transform=axs[1].transAxes, fontsize=16, color='black', va='bottom')\n",
    "# axs[1].set_ylim(-20, 10)  # Set the y-limits here\n",
    "\n",
    "# # Plot for the third graph\n",
    "# axs[2].plot(data['time'], data['heightAGL'], color='black')\n",
    "# axs[2].set_ylabel('Height Above Ground Level (m)', fontsize=12)\n",
    "# axs[2].axvline(time_center, color='gray', linestyle='--', linewidth=1)\n",
    "# axs[2].text(0.95, 0.86, '(c)', transform=axs[2].transAxes, fontsize=16, color='black', va='bottom')\n",
    "# axs[2].set_ylim(0, 800)  # Set the y-limits here\n",
    "\n",
    "# # Set the x-axis limit from 15 to 45\n",
    "# for ax in axs:\n",
    "#     ax.set_xlim(1557185795, 1557185795+30)\n",
    "\n",
    "# # Set common labels\n",
    "# fig.text(0.5, 0.01, 'Unix Time (s)', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# # Improve layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5776a9c-cefa-4024-992d-ac4f887de012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6237369-b07c-4514-ab1b-d6553ffb74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Path to the directory with CSV files\n",
    "directory_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData2\\DropSamplesADSB\"\n",
    "\n",
    "# List to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Read all CSV files in the directory\n",
    "for file in os.listdir(directory_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Select only the columns of interest\n",
    "        df = df[['velocity', 'vertrate', 'heightAGL']]\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes along the index, aligning by index\n",
    "concat_df = pd.concat(dataframes, axis=0, keys=range(len(dataframes)))\n",
    "\n",
    "# Compute the mean and standard deviation for each row across all dataframes\n",
    "mean_df = concat_df.mean(level=1)  # The level=1 computes the mean across each row\n",
    "std_df = concat_df.std(level=1)\n",
    "\n",
    "# Assuming the rows correspond to regular time intervals\n",
    "time_intervals = mean_df.index\n",
    "\n",
    "# Plot mean and standard deviation for each variable\n",
    "for column in mean_df.columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(time_intervals, mean_df[column], label=f'Mean {column}')\n",
    "    plt.fill_between(time_intervals, \n",
    "                     mean_df[column] - std_df[column], \n",
    "                     mean_df[column] + std_df[column], \n",
    "                     alpha=0.2, label=f'Standard Deviation {column}')\n",
    "    plt.xlabel('Row Number')\n",
    "    plt.ylabel(column)\n",
    "    plt.title(f'Mean and Standard Deviation of {column} Across Files')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf0b28-4a44-453f-95df-20ca187f4286",
   "metadata": {},
   "source": [
    "### IAOC24 codes for most USFS contracted aircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e1375-3fa5-4bfd-9d31-b13dc1a35753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IAOC24 codes for most USFS waterbombers\n",
    "IAOC24_CODES = [\"C00E3E\",\"C07CB2\",\"A7D27A\",\"A69072\",\"A11CBB\",\"A7F642\",\"A9926A\",\"AB79CC\",\"A3F3AC\",\"A3F763\",\"A442AA\",\"A85052\",\n",
    "                \"A4EA6D\",\"A380E6\",\"A4B460\",\"A47CBC\",\"A48A3A\",\"A47197\",\"A46DE0\",\"A4C031\",\"A07F21\",\n",
    "               \"A380E6\",\"A4B50C\",\"A0956B\",\"A2F996\",\"A2FD4D\",\"A30104\",\"A304BB\",\"A30872\",\"A46DE0\",\"A47CBC\",\"A48A3A\",\"A47197\",\n",
    "                \"A5CD6C\",\"A5D123\",\"A5D4DA\",\"AD66E3\",\"ADC0C4\",\"AAFD0B\",\"A5C9B5\",\"A8215E\",\"A19DA0\", \"A2B7FD\"]\n",
    "\n",
    "IAOC24_CODES = [code.lower() for code in IAOC24_CODES]\n",
    "print(IAOC24_CODES)\n",
    "print(len(IAOC24_CODES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0b85d-41ec-4c4e-be3d-22bb65918c27",
   "metadata": {},
   "source": [
    "### ATU drop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daec838-8287-418e-a79a-5fac64b0c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your shapefile\n",
    "shapefile_path_2 = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2023_Full_Year_03112024_VLAT.shp\"\n",
    "gdf_2 = gpd.read_file(shapefile_path_2)\n",
    "target_crs = gdf_2.crs\n",
    "print(target_crs)\n",
    "\n",
    "\n",
    "\n",
    "#shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\LAT_20210101_20211231_FullYear.shp\"\n",
    "#shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\LAT_20220101_20221231_FullYear.shp\"\n",
    "shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\LAT_20230101_20231231_FullYear.shp\"\n",
    "\n",
    "# Step 1: Read the shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf = gdf.to_crs(target_crs)\n",
    "\n",
    "print(len(gdf))\n",
    "gdf= gdf[gdf['LineLenMi'] < 0.2]\n",
    "\n",
    "print(len(gdf))\n",
    "\n",
    "# Step 2: Calculate the centroid for each line\n",
    "# The centroid here refers to the geometric center of the line's bounding box, not always on the line itself\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# Extract latitude and longitude from the centroid\n",
    "gdf['Latitude1'] = gdf['centroid'].y\n",
    "gdf['Longitude1'] = gdf['centroid'].x\n",
    "\n",
    "# Drop the 'centroid' column as it's no longer needed (optional)\n",
    "gdf.drop('centroid', axis=1, inplace=True)\n",
    "\n",
    "gdf['UTCdateTim'] = pd.to_datetime(gdf['DateTimeUT'])\n",
    "gdf['dropID_prefix'] = gdf['DropID'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "\n",
    "# # Define the mapping from dropID_prefix to ADS-B codes\n",
    "# dropID_to_adsb_code = {\n",
    "#     'T910': 'a7d27a',\n",
    "#     'T912': 'a5d891',\n",
    "#     'T911': 'a11cbb',\n",
    "#     'T914': 'a7d27a'\n",
    "# }\n",
    "\n",
    "# # Create a new column 'adsb_code' by mapping the dropID_prefix values to their corresponding ADS-B codes\n",
    "# gdf['adsb_code'] = gdf['dropID_prefix'].map(dropID_to_adsb_code)\n",
    "\n",
    "# # Now your GeoDataFrame has an additional column 'adsb_code' with the mapped ADS-B codes\n",
    "# print(gdf[['dropID', 'dropID_prefix', 'adsb_code']])\n",
    "\n",
    "# #adsbCODES = gdf['adsb_code'].tolist()\n",
    "# adsbCODES = ['a7d27a','a5d891','a11cbb','a7d27a']\n",
    "\n",
    "\n",
    "# # Now, gdf has two new columns: 'Latitude1' and 'Longitude1' representing the center of each line.\n",
    "data = gdf\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec60ff-4f8a-487f-8812-7676163688b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_n_values = gdf['n'].unique()\n",
    "print(unique_n_values)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc997a9-bc55-4914-9237-6cc0ae34df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your shapefile\n",
    "#shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2021_Full_Year_03112024_VLAT.shp\"\n",
    "#shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2022_Full_Year_03112024_VLAT.shp\"\n",
    "#shapefile_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\2023_Full_Year_03112024_VLAT.shp\"\n",
    "\n",
    "# Step 1: Read the shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Step 2: Calculate the centroid for each line\n",
    "# The centroid here refers to the geometric center of the line's bounding box, not always on the line itself\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# Extract latitude and longitude from the centroid\n",
    "gdf['Latitude1'] = gdf['centroid'].y\n",
    "gdf['Longitude1'] = gdf['centroid'].x\n",
    "\n",
    "# Drop the 'centroid' column as it's no longer needed (optional)\n",
    "gdf.drop('centroid', axis=1, inplace=True)\n",
    "\n",
    "gdf['UTCdateTim'] = pd.to_datetime(gdf['Date_(UTC)'] + ' ' + gdf['Time_(UTC)'])\n",
    "gdf['dropID_prefix'] = gdf['dropID'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "\n",
    "\n",
    "# Define the mapping from dropID_prefix to ADS-B codes\n",
    "dropID_to_adsb_code = {\n",
    "    'T910': 'a7d27a',\n",
    "    'T912': 'a5d891',\n",
    "    'T911': 'a11cbb',\n",
    "    'T914': 'a7d27a'\n",
    "}\n",
    "\n",
    "# Create a new column 'adsb_code' by mapping the dropID_prefix values to their corresponding ADS-B codes\n",
    "gdf['adsb_code'] = gdf['dropID_prefix'].map(dropID_to_adsb_code)\n",
    "\n",
    "# Now your GeoDataFrame has an additional column 'adsb_code' with the mapped ADS-B codes\n",
    "print(gdf[['dropID', 'dropID_prefix', 'adsb_code']])\n",
    "\n",
    "#adsbCODES = gdf['adsb_code'].tolist()\n",
    "adsbCODES = ['a7d27a','a5d891','a11cbb','a7d27a']\n",
    "\n",
    "\n",
    "# Now, gdf has two new columns: 'Latitude1' and 'Longitude1' representing the center of each line.\n",
    "data = gdf\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4da1de-0ed2-438f-8c56-0734aad39410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique values from 'dropID_prefix' column\n",
    "unique_dropID_prefixes = gdf['dropID_prefix'].unique()\n",
    "\n",
    "# Convert to a list (if you specifically need a list object)\n",
    "unique_dropID_prefixes_list = list(unique_dropID_prefixes)\n",
    "\n",
    "# Print the list of unique 'dropID_prefix' values\n",
    "print(unique_dropID_prefixes_list)\n",
    "# ['T910', 'T912', 'T911', 'T914']\n",
    "# a5c9b5, a5d891,a11cbb , a8215e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4e8b1-2c68-4a34-8c82-b5115e416d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBflighttracking\\ATU_2017_2021_1mile_ALL.txt\", sep=\",\", header=0)\n",
    "data = pd.read_csv(r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBflighttracking\\ATU_2017_2021_1mile_ALL.txt\", sep=\",\", header=0)\n",
    "\n",
    "#data = data.iloc[0:5000]\n",
    "\n",
    "print(list(data.columns))\n",
    "print(len(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45c65a7-4cad-4216-88d1-fe638844a5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBflighttracking\\ATU_2017_2021_1mile_ALL.txt\", sep=\",\", header=0)\n",
    "\n",
    "# Only considering the first 5000 rows as per your previous request\n",
    "#data = data.iloc[0:5000]\n",
    "\n",
    "# Replace ':' with '_' in 'Drop_ID1' column\n",
    "data['Drop_ID1'] = data['Drop_ID1'].str.replace(':', '_')\n",
    "data['Drop_ID1'] = data['Drop_ID1'].str.replace('-', '_')\n",
    "data['Drop_ID1'] = data['Drop_ID1'].str.replace(' ', '')\n",
    "\n",
    "# List all filenames in the specified directory (without file extensions)\n",
    "directory_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\DropSamplesADSB\"\n",
    "filenames = [os.path.splitext(file)[0] for file in os.listdir(directory_path)]\n",
    "\n",
    "# Check if modified 'Drop_ID1' values are in the list of filenames\n",
    "data['FilenameMatch'] = data['Drop_ID1'].isin(filenames)\n",
    "\n",
    "# Filter the DataFrame to only include rows where 'Drop_ID1' matches a filename\n",
    "data = data[data['FilenameMatch']]\n",
    "\n",
    "# Drop the 'FilenameMatch' column if you don't need it anymore\n",
    "data.drop(columns=['FilenameMatch'], inplace=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016ad53-2a3c-49fc-8520-213d02a21ae2",
   "metadata": {},
   "source": [
    "### Function to map tail numbers to ADSB codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602f6a91-f919-4533-880e-4dc4c433c6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_tail_number_to_adsb_code(tail_number):\n",
    "    if tail_number == \"T-01\":\n",
    "        return \"a5cd6c\"\n",
    "    elif tail_number == \"T-02\":\n",
    "        return \"a5d123\"\n",
    "    elif tail_number == \"T-03\":\n",
    "        return \"a5d4da\"\n",
    "    elif tail_number == \"T-05\":\n",
    "        return \"ad66e3\"\n",
    "    elif tail_number == \"T-06\":\n",
    "        return \"adc0c4\"\n",
    "    elif tail_number == \"T-07\":\n",
    "        return \"aafd0b\"\n",
    "    elif tail_number == \"T-10\":\n",
    "        return \"a5c9b5\"\n",
    "    elif tail_number == \"T-101\":\n",
    "        return \"a2f996\"\n",
    "    elif tail_number == \"T-102\":\n",
    "        return \"a2fd4d\"\n",
    "    elif tail_number == \"T-103\":\n",
    "        return \"a30104\"\n",
    "    elif tail_number == \"T-104\":\n",
    "        return \"a304bb\"\n",
    "    elif tail_number == \"T-105\":\n",
    "        return \"a30872\"\n",
    "    elif tail_number == \"T-107\":\n",
    "        return \"a30fe0\"\n",
    "    elif tail_number == \"T-12\":\n",
    "        return \"a5d891\"\n",
    "    elif tail_number == \"T-131\":\n",
    "        return \"a07f21\"\n",
    "    elif tail_number == \"T-132\":\n",
    "        return \"a4c031\"\n",
    "    elif tail_number == \"T-133\":\n",
    "        return \"a4b50c\"\n",
    "    elif tail_number == \"T-137\":\n",
    "        return \"a0956b\"\n",
    "    elif tail_number == \"T-14\":\n",
    "        return \"a8215e\"\n",
    "    elif tail_number == \"T-142\":\n",
    "        return \"a19da0\"\n",
    "    elif tail_number == \"T-15\":\n",
    "        return \"a2b7fd\"\n",
    "    elif tail_number == \"T-152\":\n",
    "        return \"ac75f9\"\n",
    "    elif tail_number == \"T-16\":\n",
    "        return \"a5dfff\"\n",
    "    elif tail_number == \"T-160\":\n",
    "        return \"ab79cc\"\n",
    "    elif tail_number == \"T-161\":\n",
    "        return \"a3f3ac\"\n",
    "    elif tail_number == \"T-162\":\n",
    "        return \"a3f763\"\n",
    "    elif tail_number == \"T-163\":\n",
    "        return \"a42299\"\n",
    "    elif tail_number == \"T-164\":\n",
    "        return \"a442aa\"\n",
    "    elif tail_number == \"T-166\":\n",
    "        return \"c07cb2\"\n",
    "    elif tail_number == \"T-167\":\n",
    "        return \"a85052\"\n",
    "    elif tail_number == \"T-168\":\n",
    "        return \"a4ea6d\"\n",
    "    elif tail_number == \"T-169\":\n",
    "        return \"a380e6\"\n",
    "    elif tail_number == \"T-210\":\n",
    "        return \"a4b460\"\n",
    "    elif tail_number == \"T-260\":\n",
    "        return \"a47cbc\"\n",
    "    elif tail_number == \"T-261\":\n",
    "        return \"a48a3a\"\n",
    "    elif tail_number == \"T-262\":\n",
    "        return \"a47197\"\n",
    "    elif tail_number == \"T-263\":\n",
    "        return \"a46de0\"\n",
    "    elif tail_number == \"T-40\":\n",
    "        return \"A9A086\"\n",
    "    elif tail_number == \"T-41\":\n",
    "        return \"NA\"\n",
    "    elif tail_number == \"T-44\":\n",
    "        return \"c00e3e\"\n",
    "    elif tail_number == \"T-910\":\n",
    "        return \"a7f642\"\n",
    "    elif tail_number == \"T-911\":\n",
    "        return \"a11cbb\"\n",
    "    elif tail_number == \"T-912\":\n",
    "        return \"a69072\"\n",
    "    elif tail_number == \"T-914\":\n",
    "        return \"a7d27a\"\n",
    "    elif tail_number == \"T-944\":\n",
    "        return \"NA\"\n",
    "    elif tail_number == \"N130CG\":\n",
    "        return \"a07b6a\"\n",
    "    else:    \n",
    "        return \"UNKNOWN_ADSB_CODE\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a new column \"adsbCODE\" based on the values in the \"Tail_Numbe\" column\n",
    "data[\"adsbCODE\"] = data[\"Tail_Numbe\"].apply(lambda x: map_tail_number_to_adsb_code(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e66da1-9154-495f-8d0c-ae60a4d298a1",
   "metadata": {},
   "source": [
    "### Create columns to define the start and end of the query (identify the space-time bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1835fc1-61da-46bc-b985-39868e25f7a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'DropID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mreplace(regex\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m], value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mreplace(regex\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m], value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m data_DropID \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropID\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#data_DropID = data.Drop_ID1.tolist()\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#adsbCODES = data.adsbCODE.tolist()\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#data_ALT = data[\"AltMeter1\"]+200\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#data_ALT = data_ALT.tolist()\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_DropID))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'DropID'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "#data.UTCdateTim = data.UTCdateTim.str[:-4]\n",
    "\n",
    "#print(data.UTCdateTim)\n",
    "data['UTCdateTim']= pd.to_datetime(data['UTCdateTim'])\n",
    "\n",
    "# isolates the drop time - 30 second window\n",
    "time_change = timedelta(minutes=0.25)\n",
    "data[\"Time2\"] = data.UTCdateTim + time_change\n",
    "data[\"Time1\"] = data.UTCdateTim - time_change\n",
    "\n",
    "# isolates the drop time - 6 minute 30 second segment\n",
    "time_change2 = timedelta(minutes=720)\n",
    "data[\"Time2_2\"] = data.UTCdateTim + time_change2\n",
    "data[\"Time1_2\"] = data.UTCdateTim - time_change2\n",
    "\n",
    "# define bounding box\n",
    "data[\"Latitude1PLUS\"] = data.Latitude1+0.02\n",
    "data[\"Latitude1MINUS\"] = data.Latitude1-0.02\n",
    "\n",
    "data[\"Longitude1PLUS\"] = data.Longitude1+0.02\n",
    "data[\"Longitude1MINUS\"] = data.Longitude1-0.02\n",
    "\n",
    "dataLatitude1PLUS =data[\"Latitude1PLUS\"].tolist()\n",
    "dataLatitude1MINUS =data[\"Latitude1MINUS\"].tolist()\n",
    "\n",
    "dataLongitude1PLUS =data[\"Longitude1PLUS\"].tolist()\n",
    "dataLongitude1MINUS =data[\"Longitude1MINUS\"].tolist()\n",
    "\n",
    "dataLongitude1 =  data.Longitude1.tolist()\n",
    "dataLatitude1 = data.Latitude1.tolist()\n",
    "\n",
    "# 15 seconds prior to and post start of drop\n",
    "datetime1 = data.Time1.tolist()\n",
    "datetime2 = data.Time2.tolist()\n",
    "\n",
    "# 3 minutes prior to and after the start of the drop (for non-drop samples)\n",
    "datetime1_2 = data.Time1_2.tolist()\n",
    "datetime2_2 = data.Time2_2.tolist()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=data.replace(regex=[':'], value='_')\n",
    "data=data.replace(regex=['-'], value='_')\n",
    "data=data.replace(regex=[' '], value='')\n",
    "\n",
    "data_DropID = data.DropID.tolist()\n",
    "\n",
    "#data_DropID = data.Drop_ID1.tolist()\n",
    "#adsbCODES = data.adsbCODE.tolist()\n",
    "\n",
    "# Altitude from feet to meters\n",
    "#data[\"AltMeter1\"] = data[\"AltFeet1\"]*0.3048\n",
    "#data_ALT = data[\"AltMeter1\"]+200\n",
    "#data_ALT = data_ALT.tolist()\n",
    "print(len(data_DropID))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d4b59-c469-4e8c-801d-a86c9307850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5b4b17-ba0e-4cb7-8834-32ab03f67fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'n'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m are_there_nans_in_n \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m      2\u001b[0m are_there_nans_in_n\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'n'"
     ]
    }
   ],
   "source": [
    "are_there_nans_in_n = data['n'].isnull().any()\n",
    "are_there_nans_in_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee62857-71c3-4636-8840-a21c4046f8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adsb code for lat\n",
    "\n",
    "adsbCODES = ['A47CBC', 'A48A3A' ,'A47197', 'A46DE0' ,'A5CD6C' ,'A5D123' ,'A5D4DA' ,'A5C9B5',\n",
    " 'A2F996' ,'A2FD4D', 'A30104', 'A304BB', 'A30872', 'A30FE0', 'A5D891', 'A07F21',\n",
    " 'A082D8', 'A0956B', 'A5DC48' ,'C-FKFM', 'A5DFFF' ,'AB79CC', 'A3F763', 'A42299',\n",
    " 'A442AA', 'A85052', 'A4EA6D' ,'A380E6' ,'A09922' ,'A5C247', 'A5C5FE' ,'C-FFQF',\n",
    " 'C-FFQG', 'A40296', 'A2082D' ,'AC6EDC', 'A00412']\n",
    "\n",
    "# adsbCODES = [code.lower() for code in adsbCODES]\n",
    "\n",
    "# adsbCODES\n",
    "\n",
    "\n",
    "['N389AC' 'N392AC' 'N386AC' 'N385AC' 'N473NA' 'N474NA' 'N475NA' 'N472NA'\n",
    " 'N291EA' 'N292EA' 'N293EA' 'N294EA' 'N295EA' 'N297EA' 'N476NA' 'N131CG'\n",
    " 'N132CG' 'N137CG' 'N477NA' 'C-FKFM' 'N478NA' 'N839AC' 'N355AC' 'N366AC'\n",
    " 'N374AC' 'N635AC' 'N416AC' 'N325AC' 'N138CG' 'N470NA' 'N471NA' 'C-FFQF'\n",
    " 'C-FFQG' 'N358AS' 'N23WT' 'N90WW' 'N10TP']\n",
    "\n",
    "# 2022\n",
    "['N619SW' 'N389AC' 'N392AC' 'N386AC' 'N385AC' 'N415BT' 'N417BT' 'N418BT'\n",
    " 'N419BT' 'N406BT' 'N473NA' 'N474NA' 'N475NA' 'N472NA' 'N291EA' 'N292EA'\n",
    " 'N293EA' 'N294EA' 'N295EA' 'N297EA' 'N476NA' 'N131CG' 'N132CG' 'N137CG'\n",
    " 'N477NA' 'N478NA' 'N839AC' 'N355AC' 'N366AC' 'N374AC' 'N635AC' 'N416AC'\n",
    " 'N325AC' 'N138CG' 'N470NA' 'N471NA' 'C-FFQF' 'C-FFQG' 'C-FFQL' 'C-FFZJ'\n",
    " 'N358AS' 'N23WT' 'N90WW' 'N10TP']\n",
    "\n",
    "\n",
    "# Your mapping of 'n' values to 'adsb_CODE' values\n",
    "n_to_adsb_code = {\n",
    "    'N389AC': 'A47CBC',\n",
    "    'N392AC': 'A48A3A',\n",
    "    'N386AC': 'A47197',\n",
    "    'N385AC': 'A46DE0',\n",
    "    'N473NA': 'A5CD6C',\n",
    "    'N474NA': 'A5D123',\n",
    "    'N475NA': 'A5D4DA',\n",
    "    'N472NA': 'A5C9B5',\n",
    "    'N291EA': 'A2F996',\n",
    "    'N292EA': 'A2FD4D',\n",
    "    'N293EA': 'A30104',\n",
    "    'N294EA': 'A304BB',\n",
    "    'N295EA': 'A30872',\n",
    "    'N297EA': 'A30FE0',\n",
    "    'N476NA': 'A5D891',\n",
    "    'N131CG': 'A07F21',\n",
    "    'N132CG': 'A082D8',\n",
    "    'N137CG': 'A0956B',\n",
    "    'N477NA': 'A5DC48',\n",
    "    'C-FKFM': 'CFKFM',\n",
    "    'N478NA': 'A5DFFF',\n",
    "    'N839AC': 'AB79CC',\n",
    "    'N355AC': 'A3F763',\n",
    "    'N366AC': 'A42299',\n",
    "    'N374AC': 'A442AA',\n",
    "    'N635AC': 'A85052',\n",
    "    'N416AC': 'A4EA6D',\n",
    "    'N325AC': 'A380E6',\n",
    "    'N138CG': 'A09922',\n",
    "    'N470NA': 'A5C247',\n",
    "    'N471NA': 'A5C5FE',\n",
    "    'C-FFQF': 'CFFQF',\n",
    "    'C-FFQG': 'CFFQG',\n",
    "    'N358AS': 'A40296',\n",
    "    'N23WT': 'A2082D',\n",
    "    'N90WW': 'AC6EDC',\n",
    "    'N10TP': 'A00412',\n",
    "    'N619SW': 'A811D2',\n",
    "    'N415BT': 'A4E6DE',\n",
    "    'N417BT': 'A4EE4C',\n",
    "    'N418BT': 'A4F203',\n",
    "    'N419BT': 'A4F5BA',\n",
    "    'N406BT': 'A4C316',\n",
    "    'C-FFQL': 'CFFQL',\n",
    "    'C-FFZJ': 'CFFZJ',\n",
    "}\n",
    "\n",
    "#['N619SW', 'N415BT', 'N417BT', 'N418BT', 'N419BT', 'N406BT', 'C-FFQL', 'C-FFZJ']\n",
    "\n",
    "\n",
    "data['adsb_CODE'] = data['n'].map(n_to_adsb_code)\n",
    "\n",
    "# Convert 'adsb_CODE' values to lowercase, safely handling NaN values\n",
    "#data['adsb_CODE_lower'] = [code.lower() if pd.notnull(code) else None for code in data['adsb_CODE']]\n",
    "data['adsb_CODE_lower'] = data['adsb_CODE'].str.lower()\n",
    "\n",
    "# If you want to create a list of these lowercase 'adsb_CODE' values (excluding None)\n",
    "dataADSBLIST = [code for code in data['adsb_CODE_lower'] if code is not None]\n",
    "\n",
    "print(len(dataADSBLIST))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cec9a-21ca-4465-9137-7a35566d23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "are_there_nans_in_n = data['adsb_CODE'].isnull().any()\n",
    "print(are_there_nans_in_n)\n",
    "len(data['adsb_CODE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58060d38-38a3-42e1-8f12-76f964dffeea",
   "metadata": {},
   "source": [
    "### Opensky - collect and download - https://opensky-network.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cb069bc-322d-4d05-a084-df4ac2769ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\paramiko\\client.py:365\u001b[0m, in \u001b[0;36mSSHClient.connect\u001b[1;34m(self, hostname, port, username, password, pkey, key_filename, timeout, allow_agent, look_for_keys, compress, sock, gss_auth, gss_kex, gss_deleg_creds, gss_host, banner_timeout, auth_timeout, channel_timeout, gss_trust_dns, passphrase, disabled_algorithms, transport_factory)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Break out of the loop on success\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m OutputDirectoryDropSamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124madmin-magstadt\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mVLATfromJIM\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOutputDropSamples2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m OutputDirectoryNonDropSamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124madmin-magstadt\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mVLATfromJIM\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOutputNonDropSamples2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m opensky \u001b[38;5;241m=\u001b[39m \u001b[43mOpenskyImpalaWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#for i in range(1000):#len(data_DropID)):\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pyopensky\\impala_wrapper.py:51\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pyopensky\\impala_wrapper.py:54\u001b[0m, in \u001b[0;36mconnect_opensky\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\pyopensky\\ssh_client.py:59\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(self, host_name, retries, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ee\\lib\\site-packages\\paramiko\\client.py:365\u001b[0m, in \u001b[0;36mSSHClient.connect\u001b[1;34m(self, hostname, port, username, password, pkey, key_filename, timeout, allow_agent, look_for_keys, compress, sock, gss_auth, gss_kex, gss_deleg_creds, gss_host, banner_timeout, auth_timeout, channel_timeout, gss_trust_dns, passphrase, disabled_algorithms, transport_factory)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Break out of the loop on success\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyopensky import OpenskyImpalaWrapper\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "import csv\n",
    "\n",
    "\n",
    "#OutputDirectoryDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TestGIT\\\\DropSamples\"\n",
    "#OutputDirectoryNonDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TestGIT\\\\NondropSamples\"\n",
    "\n",
    "# OutputDirectoryDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSB_2_28_2024\\\\OutputDropSamples\\\\\"\n",
    "# OutputDirectoryNonDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSB_2_28_2024\\\\OutputNonDropSamples\\\\\"\n",
    "\n",
    "datetime1_utc = [pd.to_datetime(dt).tz_convert('UTC').tz_localize(None) for dt in datetime1]\n",
    "datetime2_utc = [pd.to_datetime(dt).tz_convert('UTC').tz_localize(None) for dt in datetime2]\n",
    "datetime1_2_utc = [pd.to_datetime(dt).tz_convert('UTC').tz_localize(None) for dt in datetime1_2]\n",
    "datetime2_2_utc = [pd.to_datetime(dt).tz_convert('UTC').tz_localize(None) for dt in datetime2_2]\n",
    "\n",
    "\n",
    "\n",
    "OutputDirectoryDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputDropSamples2\\\\\"\n",
    "\n",
    "OutputDirectoryNonDropSamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamples2\\\\\"\n",
    "\n",
    "opensky = OpenskyImpalaWrapper()\n",
    "\n",
    "#for i in range(1000):#len(data_DropID)):\n",
    "for i in range(len(data)):\n",
    "\n",
    "    print(i)\n",
    "    \n",
    "    # Perform a query with ICAO filter\n",
    "    df = opensky.query(\n",
    "        type=\"adsb\",\n",
    "        #start = datetime1[i],\n",
    "        #end = datetime2[i],\n",
    "        start = datetime1_utc[i],\n",
    "        end = datetime2_utc[i],\n",
    "        bound=[dataLatitude1MINUS[i],dataLongitude1MINUS[i],dataLatitude1PLUS[i],dataLongitude1PLUS[i]],\n",
    "        #icao24 = adsbCODES\n",
    "        icao24 = dataADSBLIST[i]\n",
    "        #icao24 = adsbCODES[i]\n",
    "        #icao24 = IAOC24_CODES\n",
    "    )\n",
    "    \n",
    "    if df is not None:        \n",
    "        # this filters the data and ensure there is continuous quality data. The main issue I was facing was limited data availability, \n",
    "        # particularly in low elevation mountainous regions with limited ADSB coverage by OpenSky\n",
    "        # this will drastically limit the avalible data\n",
    "        if (df['lastposupdate'].nunique() > (len(df)-15)) is True and (df['icao24'].nunique() == 1) is True and (len(df) > 25) is True:\n",
    "        #if (len(df) > 25) is True:\n",
    "\n",
    "            icaocode24 = df['icao24']\n",
    "            print(\"itsUniqueEnough\")\n",
    "            df2 = df.dropna(subset = [\"lat\"])          # Apply dropna() function to remove missing lat lon \n",
    "            if len(df2) > 0:\n",
    "                filepathcsv = OutputDirectoryDropSamples+data_DropID[i]+\".csv\"\n",
    "                df2.to_csv(filepathcsv)\n",
    "                filepathshp = OutputDirectoryDropSamples+data_DropID[i]+\".shp\"\n",
    "                driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "                data_src = driver.CreateDataSource(filepathshp)\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromEPSG(4326)# 4326 = wgs84\n",
    "                layer = data_src.CreateLayer(filepathshp, \n",
    "                                             srs, \n",
    "                                             geom_type = ogr.wkbPoint)\n",
    "\n",
    "                #Create attribute fields from OpenSky\n",
    "                field_name = ogr.FieldDefn(\"time\", ogr.OFTString)\n",
    "                field_name.SetWidth(50)\n",
    "                layer.CreateField(field_name)\n",
    "                layer.CreateField(ogr.FieldDefn(\"lon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lat\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"velocity\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"heading\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"vertrate\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"callsign\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"onground\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"alert\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"spi\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"squawk\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"baro\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"geo\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastpos\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastcon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"hour\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"icao24\", ogr.OFTString))\n",
    "  \n",
    "                with open(filepathcsv, \"r\") as csv_file:\n",
    "                    csv_reader = csv.reader(csv_file)\n",
    "                    next(csv_reader)\n",
    "                    for row in csv_reader:        \n",
    "                        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "                        feature.SetField(\"time\", row[1])\n",
    "                        feature.SetField(\"lon\", row[4])\n",
    "                        feature.SetField(\"lat\", row[3])\n",
    "                        feature.SetField(\"velocity\", row[5])\n",
    "                        feature.SetField(\"heading\", row[6])\n",
    "                        feature.SetField(\"vertrate\", row[7])\n",
    "                        feature.SetField(\"callsign\", row[8])\n",
    "                        feature.SetField(\"onground\", row[9])\n",
    "                        feature.SetField(\"alert\", row[10])\n",
    "                        feature.SetField(\"spi\", row[11])\n",
    "                        feature.SetField(\"squawk\", row[12])\n",
    "                        feature.SetField(\"baro\", row[13])\n",
    "                        feature.SetField(\"geo\", row[14])\n",
    "                        feature.SetField(\"lastpos\", row[15])\n",
    "                        feature.SetField(\"lastcon\", row[16])\n",
    "                        feature.SetField(\"hour\", row[17])\n",
    "                        feature.SetField(\"icao24\", row[2])\n",
    "\n",
    "                        #Create point geometry\n",
    "                        point = ogr.Geometry(ogr.wkbPoint)\n",
    "                        point.AddPoint(float(row[4]), float(row[3]))\n",
    "\n",
    "                        #Create the feature and set the values \n",
    "                        feature.SetGeometry(point)\n",
    "                        layer.CreateFeature(feature)\n",
    "                        # reset features for next row\n",
    "                        feature = None\n",
    "                data_src = None\n",
    "            \n",
    "            ##\n",
    "            dfLong = opensky.query(\n",
    "                type=\"adsb\",\n",
    "                start = datetime1_2_utc[i],\n",
    "                end = datetime2_2_utc[i],\n",
    "                icao24 = icaocode24\n",
    "                )\n",
    "            \n",
    "            df2_long = dfLong.dropna(subset = [\"lat\"])          # Apply dropna() function to remove missing lat lon \n",
    "            if len(df2_long) > 0:\n",
    "                filepathcsv = OutputDirectoryNonDropSamples+data_DropID[i]+\".csv\"\n",
    "                df2_long.to_csv(filepathcsv)\n",
    "                filepathshp = OutputDirectoryNonDropSamples+data_DropID[i]+\".shp\"\n",
    "                driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "                data_src = driver.CreateDataSource(filepathshp)\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromEPSG(4326)# 4326 = wgs84\n",
    "                layer = data_src.CreateLayer(filepathshp, \n",
    "                                             srs, \n",
    "                                             geom_type = ogr.wkbPoint)\n",
    "\n",
    "                #Create attribute fields from OpenSky\n",
    "                field_name = ogr.FieldDefn(\"time\", ogr.OFTString)\n",
    "                field_name.SetWidth(50)\n",
    "                layer.CreateField(field_name)\n",
    "                layer.CreateField(ogr.FieldDefn(\"lon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lat\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"velocity\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"heading\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"vertrate\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"callsign\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"onground\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"alert\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"spi\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"squawk\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"baro\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"geo\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastpos\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"lastcon\", ogr.OFTReal))\n",
    "                layer.CreateField(ogr.FieldDefn(\"hour\", ogr.OFTString))\n",
    "                layer.CreateField(ogr.FieldDefn(\"icao24\", ogr.OFTString))\n",
    "\n",
    "                with open(filepathcsv, \"r\") as csv_file:\n",
    "                    csv_reader = csv.reader(csv_file)\n",
    "                    next(csv_reader)\n",
    "                    for row in csv_reader:        \n",
    "                        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "                        feature.SetField(\"time\", row[1])\n",
    "                        feature.SetField(\"lon\", row[4])\n",
    "                        feature.SetField(\"lat\", row[3])\n",
    "                        feature.SetField(\"velocity\", row[5])\n",
    "                        feature.SetField(\"heading\", row[6])\n",
    "                        feature.SetField(\"vertrate\", row[7])\n",
    "                        feature.SetField(\"callsign\", row[8])\n",
    "                        feature.SetField(\"onground\", row[9])\n",
    "                        feature.SetField(\"alert\", row[10])\n",
    "                        feature.SetField(\"spi\", row[11])\n",
    "                        feature.SetField(\"squawk\", row[12])\n",
    "                        feature.SetField(\"baro\", row[13])\n",
    "                        feature.SetField(\"geo\", row[14])\n",
    "                        feature.SetField(\"lastpos\", row[15])\n",
    "                        feature.SetField(\"lastcon\", row[16])\n",
    "                        feature.SetField(\"hour\", row[17])\n",
    "                        feature.SetField(\"icao24\", row[2])\n",
    "\n",
    "\n",
    "                        #Create point geometry\n",
    "                        point = ogr.Geometry(ogr.wkbPoint)\n",
    "                        point.AddPoint(float(row[4]), float(row[3]))\n",
    "\n",
    "                        #Create the feature and set the values \n",
    "                        feature.SetGeometry(point)\n",
    "                        layer.CreateFeature(feature)\n",
    "                        # reset features for next row\n",
    "                        feature = None\n",
    "                data_src = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54a889-74a9-4fcb-81b0-d27156750778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#COMBINE ALLDROPS\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing CSV files\n",
    "folder_path = r\"C:\\Users\\admin-magstadt\\Desktop\\CLEANATUDROPDATA\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames from each CSV file\n",
    "dfs = []\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read each CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "# Concatenate all DataFrames in the list to create one big DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(combined_df)\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the folder containing CSV files\n",
    "folder_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\Output\\OutputNonDropSamples\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames from each CSV file\n",
    "dfs = []\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read each CSV file into a DataFrame and append it to the list\n",
    "        dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "# Concatenate all DataFrames in the list to create one big DataFrame\n",
    "combined_df_nondrop = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(len(combined_df_nondrop))\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df_nondrop\n",
    "\n",
    "combined_df_nondrop_noDup = combined_df_nondrop.drop_duplicates()\n",
    "\n",
    "print(len(combined_df_nondrop_noDup))\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "# ##############################################################################\n",
    "# #######################################\n",
    "# #######################################\n",
    "# # Define the paths to the CSV files\n",
    "# #######################################\n",
    "# #######################################\n",
    "# ##############################################################################\n",
    "# #non_drop_samples_file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\Output\\OutputNonDropSamples\\T910_2021_240_221715.csv\"\n",
    "# non_drop_samples_file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\Output\\OutputNonDropSamples\"\n",
    "\n",
    "# csv_files = [os.path.join(non_drop_samples_file_path, file) for file in os.listdir(non_drop_samples_file_path) if file.endswith('.csv')]\n",
    "\n",
    "# for csv_file in csv_files:\n",
    "\n",
    "#     # Read both CSV files into separate pandas DataFrames\n",
    "#     df_non_drop_samples = pd.read_csv(csv_file)\n",
    "#     print(len(df_non_drop_samples))\n",
    "#     #df_drop_samples = pd.read_csv(drop_samples_file_path)\n",
    "\n",
    "#     # Extract values from the \"time\" column in the second DataFrame\n",
    "#     drop_samples_times = combined_df[\"time\"].tolist()\n",
    "#     drop_samples_icao24 = combined_df[\"icao24\"].tolist()\n",
    "\n",
    "#     # Filter the first DataFrame to exclude rows where the \"time\" column matches values from the second DataFrame\n",
    "#     #filtered_df_non_drop_samples = df_non_drop_samples[~df_non_drop_samples[\"time\"].isin(drop_samples_times)]\n",
    "#     filtered_df_non_drop_samples = df_non_drop_samples[~df_non_drop_samples[['time', 'icao24']].apply(tuple, axis=1).isin(zip(drop_samples_times, drop_samples_icao24))]\n",
    "\n",
    "\n",
    "#     # Display the filtered DataFrame\n",
    "#     filtered_df_non_drop_samples\n",
    "\n",
    "#     print(len(filtered_df_non_drop_samples))\n",
    "\n",
    "#     # Calculate the number of chunks\n",
    "#     num_chunks = len(filtered_df_non_drop_samples) // 30\n",
    "#     print(num_chunks)\n",
    "#     # Iterate through chunks and export as CSV\n",
    "#     for i in range(num_chunks):\n",
    "#         chunk = filtered_df_non_drop_samples.iloc[i*30:(i+1)*30]\n",
    "#         #print(len(chunk))\n",
    "#         # Check if the number of unique times in the chunk is 30\n",
    "#         time_diff = (chunk['time'].max()+1) - chunk['time'].min()\n",
    "#         unique_times_count = chunk['lastcontact'].nunique()\n",
    "\n",
    "#         #print(time_diff)\n",
    "#         if time_diff == 30 and unique_times_count > 26:\n",
    "#             file_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "#             chunk.to_csv(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUNONDROPSAMPELS\\\\{file_name}_{i+1}.csv\", index=False)\n",
    "#         else:\n",
    "#             print(f\"Skipping chunk {i+1} as it does not have 30 unique times.\")\n",
    "\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the paths to the Shapefile and CSV file\n",
    "# shp_file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\Output\\OutputNonDropSamples\\T910_2021_240_221715.shp\"\n",
    "# csv_file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\Output\\OutputDropSamples\\T910_2021_240_221715.csv\"\n",
    "\n",
    "# # Read in the Shapefile and CSV file\n",
    "# gdf = gpd.read_file(shp_file_path)\n",
    "# df_drop_samples = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Extract values from the \"time\" column in the CSV file\n",
    "# drop_samples_times = df_drop_samples[\"time\"].tolist()\n",
    "\n",
    "# # Filter the Shapefile to exclude geometries where the \"time\" column matches values from the CSV file\n",
    "# filtered_gdf = gdf[~gdf[\"time\"].isin(drop_samples_times)]\n",
    "\n",
    "# # Save the filtered Shapefile\n",
    "# output_file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\CLEANATUNONDROPSAMPELS\\T910_2021_240_221715_filtered.shp\"\n",
    "# filtered_gdf.to_file(output_file_path)\n",
    "# print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe565f-398c-4a1a-a785-2d03afd2d684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# # Define the path to the folder containing CSV files\n",
    "# folder_path = r\"C:\\Users\\admin-magstadt\\Desktop\\CLEANATUDROPDATA\"\n",
    "\n",
    "# # Initialize an empty list to store DataFrames from each CSV file\n",
    "# dfs = []\n",
    "\n",
    "# # Loop through all CSV files in the folder\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         # Read each CSV file into a DataFrame and append it to the list\n",
    "#         dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "# # Concatenate all DataFrames in the list to create one big DataFrame\n",
    "# combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # Display the combined DataFrame\n",
    "# print(combined_df)\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the folder containing CSV files\n",
    "# folder_path = r\"C:\\Users\\admin-magstadt\\Desktop\\VLATfromJIM\\Output\\OutputNonDropSamples\"\n",
    "\n",
    "# # Initialize an empty list to store DataFrames from each CSV file\n",
    "# dfs = []\n",
    "\n",
    "# # Loop through all CSV files in the folder\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         # Read each CSV file into a DataFrame and append it to the list\n",
    "#         dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "# # Concatenate all DataFrames in the list to create one big DataFrame\n",
    "# combined_df_nondrop = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # Display the combined DataFrame\n",
    "# combined_df_nondrop\n",
    "\n",
    "# # Remove duplicate rows from the combined DataFrame\n",
    "# combined_df_nondrop_noDup = combined_df_nondrop.drop_duplicates()\n",
    "\n",
    "# Use the existing combined_df_nondrop_noDup DataFrame instead of reading CSV files\n",
    "df_non_drop_samples = combined_df_nondrop_noDup\n",
    "\n",
    "# Extract values from the \"time\" column in the second DataFrame\n",
    "drop_samples_times = combined_df[\"time\"].tolist()\n",
    "drop_samples_icao24 = combined_df[\"icao24\"].tolist()\n",
    "\n",
    "# Filter the first DataFrame to exclude rows where the \"time\" column matches values from the second DataFrame\n",
    "filtered_df_non_drop_samples = df_non_drop_samples[~df_non_drop_samples[['time', 'icao24']].apply(tuple, axis=1).isin(zip(drop_samples_times, drop_samples_icao24))]\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the number of chunks\n",
    "num_chunks = len(filtered_df_non_drop_samples) // 30\n",
    "print(num_chunks)\n",
    "# Iterate through chunks and export as CSV\n",
    "for i in range(num_chunks):\n",
    "    chunk = filtered_df_non_drop_samples.iloc[i*30:(i+1)*30]\n",
    "    # Check if the number of unique times in the chunk is 30\n",
    "    time_diff = (chunk['time'].max()+1) - chunk['time'].min()\n",
    "    unique_times_count = chunk['lastcontact'].nunique()\n",
    "    unique_ADSB_count = chunk['icao24'].nunique()\n",
    "\n",
    "    if time_diff == 30 and unique_times_count > 26 and unique_ADSB_count==1:\n",
    "        file_name = \"combined_df_nondrop_noDup_chunk\"  # Change the file name as needed\n",
    "        chunk.to_csv(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TESTAGL\\\\{file_name}_{i+1}.csv\", index=False)\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(chunk, geometry=gpd.points_from_xy(chunk['lon'], chunk['lat']))\n",
    "        gdf.crs = 'epsg:4326'  # Set the CRS as needed\n",
    "        gdf.to_file(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TESTAGL\\\\{file_name}_{i+1}.shp\")      \n",
    "        \n",
    "    else:\n",
    "        print(f\"Skipping chunk {i+1} as it does not have 30 unique times.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb37324-5152-40f6-9b20-97c4f92e0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of chunks\n",
    "# num_chunks = len(filtered_df_non_drop_samples) // 30\n",
    "# print(num_chunks)\n",
    "# # Iterate through chunks and export as CSV\n",
    "# for i in range(10):#num_chunks):\n",
    "#     chunk = filtered_df_non_drop_samples.iloc[i*30:(i+1)*30]\n",
    "    \n",
    "#     chunk.to_csv(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUNONDROPSAMPELS\\\\T910_2021_240_221715_{i+1}.csv\", index=False)\n",
    "    \n",
    "    \n",
    "# import geopandas as gpd\n",
    "\n",
    "# # Read in the Shapefile\n",
    "# shp_file = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamples\\\\T910_2021_240_221715.shp\"\n",
    "# gdf = gpd.read_file(shp_file)\n",
    "\n",
    "# # Calculate the number of chunks\n",
    "# num_chunks = len(gdf) // 30\n",
    "\n",
    "# # Iterate through chunks and export as Shapefiles\n",
    "# for i in range(10):# num_chunks\n",
    "#     chunk = gdf.iloc[i*30:(i+1)*30]\n",
    "#     chunk.to_file(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUNONDROPSAMPELS\\\\T910_2021_240_221715_{i+1}.shp\")\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f8418-e0ff-4f54-86e8-bdb7adf846b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ee\n",
    "# #ee.Authenticate()\n",
    "\n",
    "# #ee.Initialize(project = 'ee-magstadt')\n",
    "# ee.Initialize(project = 'ee-magstadt605')\n",
    "\n",
    "\n",
    "# #input_folder = pathshp\n",
    "# #input_folder = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputDropSamples\\\\\"\n",
    "# input_folder = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUNONDROPSAMPELS\\\\\"\n",
    "# #output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamplesWAGL\\\\\"\n",
    "# output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TESTAGL\\\\\"\n",
    "\n",
    "\n",
    "# # Loop over all shapefiles in the input folder\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith(\".shp\"):\n",
    "#         #print(\"Processing:\", filename)\n",
    "#         # Construct input and output file paths\n",
    "#         input_path = os.path.join(input_folder, filename)\n",
    "#         output_path = os.path.join(output_folder_short, filename[:-4] + \".csv\")  # Remove \".shp\" from input filename and add \".csv\"\n",
    "        \n",
    "#         # Run the existing script with the input and output file paths\n",
    "#         import ee\n",
    "#         import geemap\n",
    "#         import pandas as pd\n",
    "#         import time\n",
    "\n",
    "#         try:\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             ee_fc = geemap.shp_to_ee(input_path)\n",
    "\n",
    "#             # Load the 3DEP dataset  and others and filter by the extent of the points\n",
    "#             dem = ee.Image(\"USGS/3DEP/10m\").clip(ee_fc.geometry().bounds())\n",
    "\n",
    "#             # Extract data for each image at appropriate scale\n",
    "#             terrain_fc = ee.Terrain.products(dem).sampleRegions(collection=ee_fc, scale=10.2)\n",
    "\n",
    "#             # Convert the elevation and terrain product data to Pandas DataFrames\n",
    "#             terrain_df = geemap.ee_to_pandas(terrain_fc)\n",
    "\n",
    "#             # Drop rows with missing values\n",
    "#             df_sample = terrain_df.dropna()\n",
    "\n",
    "#             # Calculate height AGL\n",
    "#             baro_altitude = df_sample['geo']\n",
    "#             elevation = df_sample['elevation']\n",
    "#             height_AGL = baro_altitude - elevation\n",
    "#             df_sample['heightAGL'] = height_AGL\n",
    "\n",
    "#             # Export as CSV\n",
    "#             df_sample.to_csv(output_path, index=False)\n",
    "\n",
    "#             #print(\"Completed:\", filename)\n",
    "#         except Exception as e:\n",
    "#             print(\"Error occurred:\", e)\n",
    "#             continue\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce91598-8efe-44bd-8475-c5c9ffd430c3",
   "metadata": {},
   "source": [
    "# Height AGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dea968-a41f-4ae6-b4f8-db39bf70279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "import geemap\n",
    "#ee.Authenticate()\n",
    "\n",
    "#ee.Initialize(project = 'ee-magstadt')\n",
    "ee.Initialize(project = 'ee-magstadt605')\n",
    "\n",
    "input_path = r\"C:\\Users\\admin-magstadt\\Desktop\\TESTAGL\\combined_df_nondrop_noDup_chunk_10015.shp\"\n",
    "ee_fc = geemap.shp_to_ee(input_path)\n",
    "ee_fc\n",
    "buffered_geometry = ee_fc.geometry().bounds().buffer(1000)  # Adjust buffer distance as needed\n",
    "\n",
    "dem = ee.Image(\"USGS/3DEP/10m\").clip(buffered_geometry)\n",
    "\n",
    "\n",
    "# Initialize a geemap map\n",
    "Map = geemap.Map()\n",
    "\n",
    "# Add the DEM layer to the map\n",
    "Map.addLayer(dem, {'min': 0, 'max': 1600, 'palette': ['blue', 'green', 'yellow', 'orange', 'red']}, 'DEM')\n",
    "\n",
    "# Set the map center to the feature collection bounds\n",
    "Map.centerObject(ee_fc, zoom=12)\n",
    "Map.addLayer(ee_fc, {'color': 'red'}, 'Feature Collection')\n",
    "\n",
    "# Show the map\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd629a6-bff6-4355-bbc9-2e6fce9638a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "\n",
    "#ee.Authenticate()\n",
    "\n",
    "#ee.Initialize(project = 'ee-magstadt')\n",
    "ee.Initialize(project = 'ee-magstadt605')\n",
    "\n",
    "\n",
    "#input_folder = pathshp\n",
    "#input_folder = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputDropSamples\\\\\"\n",
    "input_folder = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TESTAGL_DROP\\\\\"\n",
    "#output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamplesWAGL\\\\\"\n",
    "#output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPSAMPLES\\\\\"\n",
    "output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPSAMPLES\\\\\"\n",
    "\n",
    "desired_columns_order = ['heading', 'icao24', 'lon', 'onground', 'velocity', 'lastpos', 'spi', 'geo', 'vertrate', 'baro', 'squawk', 'hour', 'alert', 'callsign', 'time', 'lastcon', 'lat', 'elevation', 'slope', 'hillshade', 'aspect', 'heightAGL']\n",
    "\n",
    "\n",
    "# Loop over all shapefiles in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".shp\"):\n",
    "        #print(\"Processing:\", filename)\n",
    "        # Construct input and output file paths\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder_short, filename[:-4] + \".csv\")  # Remove \".shp\" from input filename and add \".csv\"\n",
    "        \n",
    "        # Run the existing script with the input and output file paths\n",
    "        import ee\n",
    "        import geemap\n",
    "        import pandas as pd\n",
    "        import time\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            ee_fc = geemap.shp_to_ee(input_path)\n",
    "            buffered_geometry = ee_fc.geometry().bounds().buffer(1000)  # Adjust buffer distance as needed\n",
    "\n",
    "#             # NEW\n",
    "#             df = pd.read_csv(input_path)\n",
    "\n",
    "#             # Function to create a feature from a row\n",
    "#             def row_to_feature(row):\n",
    "#                 geom = ee.Geometry.Point([row['lon'], row['lat']])\n",
    "#                 feature = ee.Feature(geom, row.to_dict())\n",
    "#                 return feature\n",
    "\n",
    "#             # Create a list of ee.Feature objects\n",
    "#             features = df.apply(row_to_feature, axis=1).tolist()\n",
    "\n",
    "#             # Convert the list of features into a FeatureCollection\n",
    "#             ee_feature_collection = ee.FeatureCollection(features)\n",
    "#             # end new\n",
    "           \n",
    "            #Print the number of features in the FeatureCollection\n",
    "            num_features = ee_fc.size().getInfo()\n",
    "            #print(f\"Number of features in {filename}: {num_features}\")\n",
    "#             shapefile = gpd.read_file(input_path)\n",
    "#             ee_fc = ee.Geometry.MultiPolygon(shapefile)#.geometry.to_crs(epsg='4326').map(lambda x: x.__geo_interface__['coordinates']))\n",
    "\n",
    "            \n",
    "            \n",
    "             # Load the 3DEP dataset  and others and filter by the extent of the points\n",
    "            dem = ee.Image(\"USGS/3DEP/10m\").clip(buffered_geometry)#ee_fc.geometry().bounds())\n",
    "            \n",
    "            #dem = dem.where(dem.mask().Not(), 0)\n",
    "        \n",
    "            # Extract data for each image at appropriate scale\n",
    "            terrain_fc = ee.Terrain.products(dem).sampleRegions(collection=ee_fc, scale=10)#, geometries=False)\n",
    "            #terrain_fc_size = terrain_fc.size()\n",
    "\n",
    "            # Fetch the size (number of features) as a client-side value\n",
    "            #print(terrain_fc_size.getInfo(), \"ddd\")\n",
    "\n",
    "            # Convert the elevation and terrain product data to Pandas DataFrames\n",
    "            terrain_df = geemap.ee_to_pandas(terrain_fc)\n",
    "            #print(len(terrain_df), \"ss\")  # Print the columns to check the actual column names\n",
    "            \n",
    "            #print(terrain_df)\n",
    "\n",
    "            # Drop rows with missing values\n",
    "            df_sample = terrain_df#.dropna()\n",
    "            #df_sample = terrain_df.fillna(method='bfill').dropna()\n",
    "            \n",
    "            #print(len(df_sample))  # Print the columns to check the actual column names\n",
    "\n",
    "            # Calculate height AGL\n",
    "            baro_altitude = df_sample['geo']\n",
    "            elevation = df_sample['elevation']\n",
    "            height_AGL = baro_altitude - elevation\n",
    "            df_sample['heightAGL'] = height_AGL\n",
    "            \n",
    "            df_sample = df_sample.reindex(columns=desired_columns_order)\n",
    "\n",
    "            # Export as CSV\n",
    "            #df_sample.head(30).to_csv(output_path, index=False)\n",
    "            df_sample.to_csv(output_path, index=False)\n",
    "\n",
    "            #print(\"Completed:\", filename)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "            continue\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c1162a-5651-468d-86fe-6098090627d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ee\n",
    "# #ee.Authenticate()\n",
    "\n",
    "# #ee.Initialize(project = 'ee-magstadt')\n",
    "# ee.Initialize(project = 'ee-magstadt605')\n",
    "\n",
    "\n",
    "# #input_folder = pathshp\n",
    "# input_folder = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\Output\\\\OutputNonDropSamples\\\\\"\n",
    "\n",
    "# output_folder_short = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamplesWAGL\\\\\"\n",
    "\n",
    "# # Loop over all shapefiles in the input folder\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith(\".shp\"):\n",
    "#         #print(\"Processing:\", filename)\n",
    "#         # Construct input and output file paths\n",
    "#         input_path = os.path.join(input_folder, filename)\n",
    "#         output_path = os.path.join(output_folder_short, filename[:-4] + \".csv\")  # Remove \".shp\" from input filename and add \".csv\"\n",
    "        \n",
    "#         # Run the existing script with the input and output file paths\n",
    "#         import ee\n",
    "#         import geemap\n",
    "#         import pandas as pd\n",
    "#         import time\n",
    "\n",
    "#         try:\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             ee_fc = geemap.shp_to_ee(input_path)\n",
    "\n",
    "#             # Load the 3DEP dataset  and others and filter by the extent of the points\n",
    "#             dem = ee.Image(\"USGS/3DEP/10m\").clip(ee_fc.geometry().bounds())\n",
    "\n",
    "#             # Extract data for each image at appropriate scale\n",
    "#             terrain_fc = ee.Terrain.products(dem).sampleRegions(collection=ee_fc, scale=10.2)\n",
    "\n",
    "#             # Convert the elevation and terrain product data to Pandas DataFrames\n",
    "#             terrain_df = geemap.ee_to_pandas(terrain_fc)\n",
    "\n",
    "#             # Drop rows with missing values\n",
    "#             df_sample = terrain_df.dropna()\n",
    "\n",
    "#             # Calculate height AGL\n",
    "#             baro_altitude = df_sample['geo']\n",
    "#             elevation = df_sample['elevation']\n",
    "#             height_AGL = baro_altitude - elevation\n",
    "#             df_sample['heightAGL'] = height_AGL\n",
    "\n",
    "#             # Export as CSV\n",
    "#             df_sample.to_csv(output_path, index=False)\n",
    "\n",
    "#             #print(\"Completed:\", filename)\n",
    "#         except Exception as e:\n",
    "#             print(\"Error occurred:\", e)\n",
    "#             continue\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cda104-d180-48e0-934f-435106fdf366",
   "metadata": {},
   "source": [
    "# Vert Rate Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abede26-211f-4679-a538-e0c3d4a80b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSB_2_28_2024\\OutputNonDropSamples\\N291EA_2020_191_19_15_35.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the rolling average for 'vertrate' with a window of N data points\n",
    "window_size = 5\n",
    "df['vertrate_smooth'] = df['vertrate'].rolling(window=window_size).mean()\n",
    "\n",
    "# Define the range to plot (0 to 100)\n",
    "start_index = 450\n",
    "end_index = 650\n",
    "\n",
    "# Slice the DataFrame for the defined range\n",
    "df_sliced = df[start_index:end_index+1]\n",
    "\n",
    "# Plot the original 'vertrate' data within the specified range\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(df_sliced['vertrate'], label='Original', alpha=0.5)\n",
    "\n",
    "# Plot the smoothed 'vertrate' data within the specified range\n",
    "plt.plot(df_sliced['vertrate_smooth'], label='Smoothed (Rolling Average)', color='red')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Vertical Rate Over Time with Rolling Average')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Vertical Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "#plt.savefig(r\"C:\\Users\\admin-magstadt\\Desktop\\figures\\vertrate_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a021cf1-0800-4080-86a5-38fcba1cd40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Directory paths\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData2\\\\DropSamplesADSB\\\\\"\n",
    "\n",
    "dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData2\\\\NondropSamplesADSB\\\\\"\n",
    "\n",
    "#output_dir = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData\\\\SmoothedDropSamples\\\\\"  # Specify your output directory here\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData\\\\SmoothedDropSamplesNonDrop\\\\\"  # Specify your output directory here\n",
    "\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# File pattern to match\n",
    "filenames = glob.glob(dir_path_dropsamples + \"*.csv\")\n",
    "\n",
    "# Rolling window size\n",
    "window_size = 2\n",
    "\n",
    "# Loop over all files\n",
    "for filename in filenames:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Check if 'vertrate' column exists\n",
    "    if 'vertrate' in df.columns:\n",
    "        # Calculate the rolling average and create a new column\n",
    "        df['smoothedvertrate'] = df['vertrate'].rolling(window=5, min_periods=1).mean()\n",
    "    \n",
    "        # Save the DataFrame back to a new CSV file\n",
    "        base_name = os.path.basename(filename)\n",
    "        new_file_path = os.path.join(output_dir, base_name)\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "    else:\n",
    "        print(f\"'vertrate' column not found in {filename}\")\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6a169-3889-4c5f-86d0-27337773e517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\SmoothedDropSamplesNonDrop\\N291EA_2020_191_19_15_35.csv\")\n",
    "\n",
    "# Plot vertical rate and smoothed vertical rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['time'], df['vertrate'], label='Vertical Rate')\n",
    "plt.plot(df['time'], df['smoothedvertrate'], label='Smoothed Vertical Rate')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Vertical Rate vs Smoothed Vertical Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27274bd4-c041-4b47-9699-cf05a2530f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TEST\\\\DROP\\\\\"\n",
    "filenames = glob.glob(dir_path_dropsamples + \"*.csv\")\n",
    "data = []\n",
    "usecols = [5, 7]  # Specified columns to use\n",
    "\n",
    "for filename in filenames:\n",
    "    num_rows = sum(1 for line in open(filename))\n",
    "    if num_rows < 26:\n",
    "        continue\n",
    "    skip_rows = max(0, num_rows - 30)  # Adjust to ensure reading up to the last 30 rows\n",
    "    df = pd.read_csv(filename, usecols=usecols, skiprows=skip_rows)\n",
    "    df = df.dropna()  # Drop rows with NaN values\n",
    "    arr = df.values.astype(float)\n",
    "    # Check if the array has fewer rows than expected and pad if necessary\n",
    "    if arr.shape[0] < 30:\n",
    "        pad_size = 30 - arr.shape[0]\n",
    "        # Create a row of NaN values\n",
    "        nan_row = np.full((1, arr.shape[1]), np.nan)\n",
    "        padding = np.repeat(nan_row, pad_size, axis=0)\n",
    "        arr = np.vstack([padding, arr])  # Prepend the padding\n",
    "    data.append(arr)\n",
    "\n",
    "if len(data) == 0:\n",
    "    print(\"No data found\")\n",
    "else:\n",
    "    # Pad arrays with NaN values to ensure they have the same shape\n",
    "    max_shape = max(arr.shape for arr in data)\n",
    "    padded_data = [np.pad(arr, ((0, max_shape[0] - arr.shape[0]), (0, max_shape[1] - arr.shape[1])), mode='constant', constant_values=np.nan) for arr in data]\n",
    "    array1 = np.stack(padded_data, axis=0)\n",
    "    print(array1.shape)\n",
    "### Drop Samples Data Preperation (937 drop samples were obtained from Opensky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b80092-44e9-4b25-9824-779502378464",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices = np.argwhere(np.isnan(array1))\n",
    "nan_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518e2b6-521f-4f42-ace5-03d239cc2a68",
   "metadata": {},
   "source": [
    "# PREPARE THE DROP AND NONDROP SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b612a-f81c-4147-b907-789940335f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData2\\\\DropSamplesADSB\\\\\"\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData\\\\SmoothedDropSamples\\\\\"\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPDATA\\\\\"\n",
    "#dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TEST\\\\DROP\\\\\"\n",
    "dir_path_dropsamples = r\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPSAMPLES\\\\\"\n",
    "\n",
    "\n",
    "filenames = glob.glob(dir_path_dropsamples + \"*.csv\")\n",
    "data = []\n",
    "usecols = [8,21]  # Specified columns to use\n",
    "\n",
    "#usecols = [4,22,21]  # Specified columns to use\n",
    "#usecols = [4,8]  # Specified columns to use\n",
    "\n",
    "for filename in filenames:\n",
    "    num_rows = sum(1 for line in open(filename))\n",
    "    if num_rows < 26:\n",
    "        continue\n",
    "    skip_rows = max(0, num_rows - 30)  # Adjust to ensure reading up to the last 30 rows\n",
    "    df = pd.read_csv(filename, usecols=usecols, skiprows=skip_rows)\n",
    "    #df = df.dropna()\n",
    "    arr = df.values.astype(float)\n",
    "    # Check if the array has fewer rows than expected and pad if necessary\n",
    "    if arr.shape[0] < 30:\n",
    "        pad_size = 30 - arr.shape[0]\n",
    "        # Use the last row of arr for padding\n",
    "        last_row = arr[-1:]\n",
    "        padding = np.repeat(last_row, pad_size, axis=0)\n",
    "        arr = np.vstack([padding, arr])  # Prepend the padding\n",
    "    data.append(arr)\n",
    "\n",
    "if len(data) == 0:\n",
    "    print(\"No data found\")\n",
    "else:\n",
    "    array1 = np.stack(data, axis=0)\n",
    "    #print(array1.shape)\n",
    "    array1 = np.stack(data, axis=0)\n",
    "    print(array1.shape)\n",
    "\n",
    "    # Check for NaN values\n",
    "    nan_indices = np.argwhere(np.isnan(array1))\n",
    "    if len(nan_indices) > 0:\n",
    "        print(\"NaN values found at indices:\", nan_indices)\n",
    "    else:\n",
    "        print(\"No NaN values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d91a0-2a12-4b78-ad44-1e06d3bb2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "dir_path_dropsamples = r\"C:\\Users\\admin-magstadt\\Desktop\\CLEANATUNONDROPSAMPELS\\\\\"\n",
    "filenames = glob.glob(dir_path_dropsamples + \"*.csv\")\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    num_rows = sum(1 for line in open(filename))\n",
    "    if num_rows < 26:\n",
    "        continue\n",
    "    skip_rows = max(0, num_rows - 30)\n",
    "    df = pd.read_csv(filename, usecols=usecols, skiprows=skip_rows)\n",
    "    \n",
    "    # Drop rows with any NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    arr = df.values.astype(float)\n",
    "    \n",
    "    if arr.shape[0] < 30:\n",
    "        pad_size = 30 - arr.shape[0]\n",
    "        last_row = arr[-1:]\n",
    "        padding = np.repeat(last_row, pad_size, axis=0)\n",
    "        arr = np.vstack([padding, arr])\n",
    "    \n",
    "    data.append(arr)\n",
    "\n",
    "if len(data) == 0:\n",
    "    print(\"No data found\")\n",
    "else:\n",
    "    array2 = np.stack(data, axis=0)\n",
    "    print(array2.shape)\n",
    "\n",
    "    nan_indices = np.argwhere(np.isnan(array2))\n",
    "    if len(nan_indices) > 0:\n",
    "        print(\"NaN values found at indices:\", nan_indices)\n",
    "    else:\n",
    "        print(\"No NaN values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8b531-7bd6-4a0f-a423-4a0ea1ce2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT QUIT RIGHT\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import glob\n",
    "\n",
    "# #dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData2\\\\DropSamplesADSB\\\\\"\n",
    "# #dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData\\\\SmoothedDropSamples\\\\\"\n",
    "# #dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUDROPDATA\\\\\"\n",
    "# #dir_path_dropsamples = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TEST\\\\DROP\\\\\"\n",
    "# dir_path_dropsamples = r\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUNONDROPSAMPELS\\\\\"\n",
    "\n",
    "\n",
    "# filenames = glob.glob(dir_path_dropsamples + \"*.csv\")\n",
    "# data = []\n",
    "# #usecols = [4,8,21]  # Specified columns to use\n",
    "\n",
    "# #usecols = [4,22,21]  # Specified columns to use\n",
    "# #usecols = [4,8]  # Specified columns to use\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 26:\n",
    "#         continue\n",
    "#     skip_rows = max(0, num_rows - 30)  # Adjust to ensure reading up to the last 30 rows\n",
    "#     df = pd.read_csv(filename, usecols=usecols, skiprows=skip_rows)\n",
    "#     #df = df.dropna()\n",
    "#     arr = df.values.astype(float)\n",
    "#     # Check if the array has fewer rows than expected and pad if necessary\n",
    "#     if arr.shape[0] < 30:\n",
    "#         pad_size = 30 - arr.shape[0]\n",
    "#         # Use the last row of arr for padding\n",
    "#         last_row = arr[-1:]\n",
    "#         padding = np.repeat(last_row, pad_size, axis=0)\n",
    "#         arr = np.vstack([padding, arr])  # Prepend the padding\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array2 = np.stack(data, axis=0)\n",
    "#     #print(array1.shape)\n",
    "#     array2 = np.stack(data, axis=0)\n",
    "#     print(array2.shape)\n",
    "\n",
    "#     # Check for NaN values\n",
    "#     nan_indices = np.argwhere(np.isnan(array2))\n",
    "#     if len(nan_indices) > 0:\n",
    "#         print(\"NaN values found at indices:\", nan_indices)\n",
    "#     else:\n",
    "#         print(\"No NaN values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc2e0d-b1c4-4f38-ab9d-4d8ab533c920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming 'array1' is a 3D array of shape (n_samples, 30, 3)\n",
    "# # where n_samples is the number of samples, 30 is the number of points in each sample,\n",
    "# # and 3 represents the columns from the original CSV files.\n",
    "\n",
    "# # Select three samples to plot\n",
    "# sample_indices = [0, 1, 2]  # Replace with the indices of the samples you want to plot\n",
    "\n",
    "# # Create a figure and an array of subplots with 3 rows and 1 column\n",
    "# fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# # Plot each sample on its own subplot\n",
    "# for i, ax in enumerate(axes):\n",
    "#     # Check if the sample index is within the range of available samples\n",
    "#     if i < array1.shape[0]:\n",
    "#         # Plotting the selected columns from the sample\n",
    "#         ax.plot(array1[sample_indices[i], :, 0], label='Column 4')\n",
    "#         ax.plot(array1[sample_indices[i], :, 1], label='Column 8')\n",
    "#         ax.plot(array1[sample_indices[i], :, 2], label='Column 21')\n",
    "#         ax.legend()\n",
    "#         ax.set_title(f'Sample {sample_indices[i]}')\n",
    "#         ax.set_xlabel('Point Index')\n",
    "#         ax.set_ylabel('Value')\n",
    "\n",
    "# # Adjust the layout so there is no overlap\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Number of samples to plot\n",
    "# n_samples_to_plot = 1\n",
    "\n",
    "# # Create figures for the first 10 samples\n",
    "# for i in range(n_samples_to_plot):\n",
    "#     fig, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "#     for col in range(array1.shape[2]):\n",
    "#         # Plot each column in a subplot\n",
    "#         axes[col].plot(array1[i, :, col], label=f'Column {usecols[col]}')\n",
    "#         axes[col].legend()\n",
    "#         axes[col].set_title(f'Sample {i+1} - Column {usecols[col]}')\n",
    "#         axes[col].set_xlabel('Point Index')\n",
    "#         axes[col].set_ylabel('Value')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc612dcf-7210-4414-b6cf-e175801a24f2",
   "metadata": {},
   "source": [
    "### Non-drop Data Samples Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd911898-cf83-412a-988e-119d1c6990a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dir_path =\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\TEST\\\\NONDROP\\\\\"\n",
    "# #dir_path =\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData2\\\\NondropSamplesADSB\\\\\"\n",
    "# #dir_path =\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\ADSBData\\\\SmoothedDropSamplesNonDrop\\\\\"\n",
    "\n",
    "# filenames = glob.glob(dir_path + \"*.csv\")\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols, nrows=30)\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array2 = np.stack(data, axis=0)\n",
    "#     print(array2.shape)\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols, nrows=30, skiprows=range(0, 30))\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array3 = np.stack(data, axis=0)\n",
    "#     print(array3.shape)\n",
    "    \n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols, nrows=30, skiprows=range(0, 60))\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array4 = np.stack(data, axis=0)\n",
    "#     print(array4.shape)\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols, nrows=30, skiprows=range(0, 90))\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array5 = np.stack(data, axis=0)\n",
    "#     print(array5.shape)\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols, nrows=30, skiprows=range(0, 120))\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array6 = np.stack(data, axis=0)\n",
    "#     print(array6.shape)\n",
    "    \n",
    "    \n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols, nrows=30, skiprows=range(0, 150))\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array7 = np.stack(data, axis=0)\n",
    "#     print(array7.shape)\n",
    "    \n",
    "    \n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols)\n",
    "#     df = df.iloc[-31:-1]\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array8 = np.stack(data, axis=0)\n",
    "#     print(array8.shape)\n",
    "    \n",
    "    \n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols)\n",
    "#     df = df.iloc[-61:-31]\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array9 = np.stack(data, axis=0)\n",
    "#     print(array9.shape)\n",
    "      \n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols)\n",
    "#     df = df.iloc[-91:-61]\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array10 = np.stack(data, axis=0)\n",
    "#     print(array10.shape)\n",
    "    \n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols)\n",
    "#     df = df.iloc[-121:-91]\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array11 = np.stack(data, axis=0)\n",
    "#     print(array11.shape)\n",
    "    \n",
    "\n",
    "# data = []\n",
    "\n",
    "# for filename in filenames:\n",
    "#     num_rows = sum(1 for line in open(filename))\n",
    "#     if num_rows < 51:\n",
    "#         #print(f\"Skipping {filename}: less than 51 rows\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(filename, usecols=usecols)\n",
    "#     df = df.iloc[-151:-121]\n",
    "#     arr = df.values.astype(float)\n",
    "#     data.append(arr)\n",
    "\n",
    "# if len(data) == 0:\n",
    "#     print(\"No data found\")\n",
    "# else:\n",
    "#     array12 = np.stack(data, axis=0)\n",
    "#     print(array12.shape)\n",
    "    \n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c02c032-5297-4350-9eb7-94782071b82c",
   "metadata": {},
   "source": [
    "### Stack arrays and split into testing and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54269476-08e8-46ea-ba63-637106fd794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stacked_array = np.concatenate((array2,array3,array4,array5,array6,array7,array8,array9,array10,array11,array12), axis=0)\n",
    "#stacked_array = np.concatenate((array2), axis=0)\n",
    "\n",
    "#print(stacked_array.shape)\n",
    "\n",
    "combined_array = np.concatenate((array1, stacked_array), axis=0)\n",
    "#combined_array = np.concatenate((array1, array12), axis=0)\n",
    "\n",
    "print(combined_array.shape)\n",
    "\n",
    "# Split the data into training, validation,\n",
    "#DROP_label = np.concatenate((np.ones(array1.shape[0]), np.zeros(stacked_array.shape[0])))\n",
    "DROP_label = np.concatenate((np.ones(array1.shape[0]), np.zeros(stacked_array.shape[0])))\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(combined_array, DROP_label, test_size=0.3, random_state=5499)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.3, random_state=5499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96734e-0e9f-453d-8279-0b87f65c775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#stacked_array = np.concatenate((array2,array3,array4,array5,array6,array7,array8,array9,array10,array11,array12), axis=0)\n",
    "#stacked_array = np.concatenate((array2), axis=0)\n",
    "\n",
    "#print(stacked_array.shape)\n",
    "\n",
    "combined_array = np.concatenate((array1, array2), axis=0)\n",
    "#combined_array = np.concatenate((array1, array12), axis=0)\n",
    "\n",
    "print(combined_array.shape)\n",
    "\n",
    "# Split the data into training, validation,\n",
    "#DROP_label = np.concatenate((np.ones(array1.shape[0]), np.zeros(stacked_array.shape[0])))\n",
    "DROP_label = np.concatenate((np.ones(array1.shape[0]), np.zeros(array2.shape[0])))\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(combined_array, DROP_label, test_size=0.3, random_state=54999)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.3, random_state=54999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc36896-7e29-4ba3-ac02-626c83766a2f",
   "metadata": {},
   "source": [
    "### Build and fit lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc528d7-ec0d-43be-bd8b-3252ea42c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(30, input_shape=input_shape))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = (train_data.shape[1], train_data.shape[2])\n",
    "modelLSTM = create_lstm_model(input_shape)\n",
    "\n",
    "history = modelLSTM.fit(train_data, train_labels, epochs=10, batch_size=64, validation_data=(val_data, val_labels))\n",
    "\n",
    "test_loss, test_acc = modelLSTM.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = modelLSTM.predict(test_data)\n",
    "test_predictions = (test_predictions > 0.5).astype(np.int32)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5e156-37c2-4000-95ea-6704fb43cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming test_labels and test_predictions are already defined and that test_predictions are binary (0s and 1s)\n",
    "\n",
    "# Calculate Precision score\n",
    "precision = precision_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate Recall score\n",
    "recall = recall_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate F1 score (this was already in your code, but included for completeness)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237e98f-18f9-400c-a325-b9d0f2253f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.utils import resample\n",
    "# from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "# from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# # Define the number of bootstrap iterations\n",
    "# n_iterations = 100\n",
    "\n",
    "# # Initialize empty lists to store metric values for each iteration\n",
    "# accuracy_list = []\n",
    "# sensitivity_list = []\n",
    "# specificity_list = []\n",
    "# f1_list = []\n",
    "\n",
    "# # Perform bootstrapping\n",
    "# for _ in range(n_iterations):\n",
    "#     # Resample the test labels with replacement\n",
    "#     resampled_labels = resample(test_labels, replace=True)\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(test_labels, test_predictions)\n",
    "#     sensitivity = recall_score(test_labels, test_predictions)\n",
    "#     conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "#     specificity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "#     f1 = f1_score(test_labels, test_predictions)\n",
    "    \n",
    "#     # Append metric values to lists\n",
    "#     accuracy_list.append(accuracy)\n",
    "#     sensitivity_list.append(sensitivity)\n",
    "#     specificity_list.append(specificity)\n",
    "#     f1_list.append(f1)\n",
    "\n",
    "# # Calculate mean and standard deviation for each metric\n",
    "# mean_accuracy = np.mean(accuracy_list)\n",
    "# std_accuracy = np.std(accuracy_list)\n",
    "\n",
    "# mean_sensitivity = np.mean(sensitivity_list)\n",
    "# std_sensitivity = np.std(sensitivity_list)\n",
    "\n",
    "# mean_specificity = np.mean(specificity_list)\n",
    "# std_specificity = np.std(specificity_list)\n",
    "\n",
    "# mean_f1 = np.mean(f1_list)\n",
    "# std_f1 = np.std(f1_list)\n",
    "\n",
    "# # Print the results\n",
    "# print('Mean Accuracy:', mean_accuracy)\n",
    "# print('Standard Deviation of Accuracy:', std_accuracy)\n",
    "\n",
    "# print('Mean Sensitivity:', mean_sensitivity)\n",
    "# print('Standard Deviation of Sensitivity:', std_sensitivity)\n",
    "\n",
    "# print('Mean Specificity:', mean_specificity)\n",
    "# print('Standard Deviation of Specificity:', std_specificity)\n",
    "\n",
    "# print('Mean F1 Score:', mean_f1)\n",
    "# print('Standard Deviation of F1 Scores:', std_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95b332-8c22-498e-ac13-7895c1dc4626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Predict probabilities for the test data\n",
    "# test_probs = modelLSTM.predict(test_data)\n",
    "\n",
    "# # Compute ROC curve and AUC\n",
    "# fpr, tpr, thresholds = roc_curve(test_labels, test_probs)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307911a4-6e29-4f12-b02c-d5f97aa5caad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate predictions\n",
    "predictions = modelLSTM.predict(test_data)\n",
    "# Convert predictions to label indices (binary classification)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403150c1-2f4c-4429-b8f4-1f73d26b789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901b59d-a1cb-4ff6-8178-1b7ea23afb5b",
   "metadata": {},
   "source": [
    "### Build and fit 1dCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c0eba-ad61-42f8-81b1-bfe4e9ed375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def create_1dcnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=8, kernel_size=3, activation='relu', input_shape=(30, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = (train_data.shape[1], train_data.shape[2])\n",
    "modelCNN = create_1dcnn_model(input_shape)\n",
    "\n",
    "history = modelCNN.fit(train_data, train_labels, epochs=10, batch_size=64, validation_data=(val_data, val_labels))\n",
    "\n",
    "test_loss, test_acc = modelCNN.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = modelCNN.predict(test_data)\n",
    "test_predictions = (test_predictions > 0.3).astype(np.int32)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b981fd-a0b9-4488-8ef5-9f4967dd7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming test_labels and test_predictions are already defined and that test_predictions are binary (0s and 1s)\n",
    "\n",
    "# Calculate Precision score\n",
    "precision = precision_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate Recall score\n",
    "recall = recall_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate F1 score (this was already in your code, but included for completeness)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7918ad-0886-40ee-86d4-e02733843c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "# Define the number of bootstrap iterations\n",
    "n_iterations = 10\n",
    "\n",
    "# Initialize empty lists to store metric values for each iteration\n",
    "accuracy_list = []\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Perform bootstrapping\n",
    "for _ in range(n_iterations):\n",
    "    # Resample the test labels with replacement\n",
    "    resampled_labels = resample(test_labels, replace=True)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    sensitivity = recall_score(test_labels, test_predictions)\n",
    "    conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "    specificity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    f1 = f1_score(test_labels, test_predictions)\n",
    "    \n",
    "    # Append metric values to lists\n",
    "    accuracy_list.append(accuracy)\n",
    "    sensitivity_list.append(sensitivity)\n",
    "    specificity_list.append(specificity)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Calculate mean and standard deviation for each metric\n",
    "mean_accuracy = np.mean(accuracy_list)\n",
    "std_accuracy = np.std(accuracy_list)\n",
    "\n",
    "mean_sensitivity = np.mean(sensitivity_list)\n",
    "std_sensitivity = np.std(sensitivity_list)\n",
    "\n",
    "mean_specificity = np.mean(specificity_list)\n",
    "std_specificity = np.std(specificity_list)\n",
    "\n",
    "mean_f1 = np.mean(f1_list)\n",
    "std_f1 = np.std(f1_list)\n",
    "\n",
    "# Print the results\n",
    "print('Mean Accuracy:', mean_accuracy)\n",
    "print('Standard Deviation of Accuracy:', std_accuracy)\n",
    "\n",
    "print('Mean Sensitivity:', mean_sensitivity)\n",
    "print('Standard Deviation of Sensitivity:', std_sensitivity)\n",
    "\n",
    "print('Mean Specificity:', mean_specificity)\n",
    "print('Standard Deviation of Specificity:', std_specificity)\n",
    "\n",
    "print('Mean F1 Score:', mean_f1)\n",
    "print('Standard Deviation of F1 Scores:', std_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ea83f-f7f5-4597-bbcd-41ab6f503cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# # Create the model with the specified input shape\n",
    "# input_shape = (30, 3)\n",
    "# model = create_1dcnn_model(input_shape)\n",
    "\n",
    "# # Generate a plot of the model\n",
    "# plot_model(model, to_file='C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\model.png', show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741a6fa-03fc-4ddc-a8da-f968e7e0d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict probabilities for the test data\n",
    "test_probs = modelCNN.predict(test_data)\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a92c39-0ee0-4819-8ff4-039bf596529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming test_probs and test_labels are already defined\n",
    "# test_probs = modelCNN.predict(test_data)\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(test_labels, test_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560d460-6e2f-4591-849d-6810b105b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict the labels\n",
    "predictions = modelCNN.predict(test_data)\n",
    "predicted_labels = (predictions > 0.3).astype(int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Plotting the confusion matrix with custom labels without the color bar\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non-drop', 'drop'])\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "\n",
    "#plt.savefig('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\confusion_matrix.png')  # Saving the plot\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e47b38-d4e4-452d-8102-6334b7a228f0",
   "metadata": {},
   "source": [
    "# Real time visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a3ad6-e90e-43a9-a483-13fedaad3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N137CG_2020_227_21_09_46.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N130CG_2019_282_17_55_24.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T_03_2020_236_20_27_49.csv\" #\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T_03_2020_291_23_38_08.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T_163_2019_160_21_05_57.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T910_2020_173_02_14_26.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T912_2020_290_22_05_27.csv\" \n",
    "\n",
    "file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T914_2020_213_00_05_11.csv\" #\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T914_2021_131_01_41_27.csv\"\n",
    "\n",
    "\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T914_2021_242_01_19_49.csv\" #\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N291EA_2020_157_21_35_58.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N291EA_2020_157_23_04_12.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N291EA_2020_157_23_12_54.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N291EA_2020_181_20_24_44.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N291EA_2020_181_21_07_02.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N291EA_2020_191_18_10_38.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N137CG_2020_215_20_27_48.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N137CG_2020_215_21_47_41.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T914_2021_157_01_51_22.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T914_2021_238_22_49_47.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N130CG_2019_131_18_25_42.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N130CG_2019_137_20_02_44.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N130CG_2019_137_20_07_24.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\CLEANATUDROPSAMPLES\\S_260_2022_135_173544.csv\"\n",
    "\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\T914_2021_156_23_47_42.csv\"\n",
    "\n",
    "# long version\n",
    "#\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSB_2_28_2024\\OutputNonDropSamples\\N130CG_2019_126_23_38_59.csv\"\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSB_2_28_2024\\OutputNonDropSamples\\N130CG_2019_245_21_27_11.csv\"\n",
    "#file_path = r\"T914_2020_213_00_05_11.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 12))  # 2 Rows, 1 Column\n",
    "\n",
    "# # First plot for velocity\n",
    "# axs[0].plot(df['time'], df['velocity'], label='Velocity', color = 'black')\n",
    "# axs[0].set_xlabel('Time')\n",
    "# axs[0].set_ylabel('Velocity (m/s)')\n",
    "# axs[0].set_title('Velocity across Time')\n",
    "# #axs[0].legend()\n",
    "\n",
    "# Second plot for vertical rate\n",
    "axs[0].plot(df['time'], df['vertrate'], label='Vertical Rate (m/s)', color = 'black')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Vertical Rate')\n",
    "axs[0].set_title('Vertical Rate across Time')\n",
    "#axs[1].legend()\n",
    "\n",
    "# Second plot for vertical rate\n",
    "axs[1].plot(df['time'], df['heightAGL'], label='Vertical Rate (m/s)', color = 'black')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Height AGL')\n",
    "axs[1].set_title('Height AGL across Time')\n",
    "#axs[1].legend()\n",
    "\n",
    "output_dir = r\"C:\\Users\\admin-magstadt\\Desktop\\figures\\ModelOutputVisual\"\n",
    "\n",
    "plt.tight_layout() \n",
    "\n",
    "file_name_without_extension = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# # Define the output path for the figure\n",
    "# output_path = os.path.join(output_dir, f'velocity_vertical_rate_plot_{file_name_without_extension}.png')\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig(output_path)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b021f-0e0c-470c-b6aa-b310d2ed2833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# # Read the CSV file\n",
    "# #file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N137CG_2020_227_21_09_46.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "# print(len(df))\n",
    "# # Assuming column indexes 4 and 8 are the ones you're interested in\n",
    "# # Adjust these indices as per your CSV's structure\n",
    "# data = df.iloc[0:30, [4, 8]].values  # Python uses 0-based indexing\n",
    "# data.shape\n",
    "# # # Reshape the data to fit the model's input shape\n",
    "# data_reshaped = data.reshape(1, 30, 2)  # 1 sample, 30 time steps, 2 features\n",
    "# data_reshaped\n",
    "\n",
    "# prediction = modelCNN.predict(data_reshaped)\n",
    "# print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1be989-6850-4654-b16e-ebe22ecc6aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Assuming modelCNN is already defined and compiled as per your previous code\n",
    "\n",
    "# Read the CSV file\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N137CG_2020_227_21_09_46.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#df = df.astype('float32')  # Assuming all data in df should be float32\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Loop through the DataFrame in windows of 30 rows\n",
    "for start_row in range(len(df) - 30):\n",
    "    # Select a window of 30 rows, columns 4 and 8 OR 5 ,7\n",
    "    data = df.iloc[start_row:start_row+30, [8, 21]].values\n",
    "    \n",
    "    # Reshape the data to fit the model's input shape\n",
    "    data_reshaped = data.reshape(1, 30, 2)  # 1 sample, 30 time steps, 2 features\n",
    "    data_reshaped = data_reshaped.astype('float32')  # Ensure the data is of type float32\n",
    "\n",
    "    # Make a prediction and store it\n",
    "    prediction = modelLSTM.predict(data_reshaped)\n",
    "    predictions.append(prediction[0][0])  # Adjust based on your model's output shape\n",
    "\n",
    "# Create a DataFrame to store predictions\n",
    "predictions_df = pd.DataFrame(predictions, columns=['Prediction'])\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predictions_df['Prediction'], label='Predictions', color = 'black')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('Predictions across flight trajectory')\n",
    "#plt.legend()\n",
    "\n",
    "#file_name_without_extension = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# # Define the output path for the figure\n",
    "# output_path = os.path.join(output_dir, f'prediction_plot_{file_name_without_extension}_L.png')\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig(output_path)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf530c-0343-4a1d-b3b3-ce441998b5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642526a-3eac-4f3b-88c9-c40faaedfb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df and predictions_df are already defined as per your previous steps\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10, 18))  # Now 3 Rows, 1 Column\n",
    "\n",
    "# Plot for velocity with black line color\n",
    "axs[0].plot(df['time'], df['velocity'], label='Velocity', color='black')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Velocity (m/s)')\n",
    "#axs[0].set_title('Velocity across Time')\n",
    "#axs[0].legend()\n",
    "\n",
    "# Plot for vertical rate with black line color\n",
    "axs[1].plot(df['time'], df['vertrate'], label='Vertical Rate', color='black')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Vertical Rate (m/s)')\n",
    "#axs[1].set_title('Vertical Rate across Time')\n",
    "#axs[1].legend()\n",
    "\n",
    "axs[2].plot(df['time'], df['heightAGL'], label='AGL', color='black')\n",
    "axs[2].set_xlabel('Time')\n",
    "axs[2].set_ylabel('Height AGL (m/s)')\n",
    "\n",
    "# Plot for predictions with black line color\n",
    "axs[3].plot(predictions_df['Prediction'], label='Predictions', color='black')\n",
    "axs[3].set_xlabel('Time Step')\n",
    "axs[3].set_ylabel('Probability Score')\n",
    "#axs[2].set_title('Predictions across Time Steps')\n",
    "#axs[2].legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to not overlap\n",
    "\n",
    "#Save the figure\n",
    "# plt.savefig('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\plot_pred_acrosstime_ex7_FORPPT.png')  # Change the file path and name as needed\n",
    "# file_name_without_extension = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# # Define the output path for the figure\n",
    "# output_path = os.path.join(output_dir, f'velocity_vertical_prediction_plot_{file_name_without_extension}_FORPPT.png')\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig(output_path)\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a85b64-8334-4818-a1cb-08ff50f0466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1596153717\t+190\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f94c9e-612c-4c27-95f1-f5cc3958bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "380/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bac69-1c68-4d84-a57c-f6d55f51519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df and predictions_df are already defined as per your previous steps\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 18))  # Now 3 Rows, 1 Column\n",
    "\n",
    "# # Plot for velocity with black line color\n",
    "# axs[0].plot(df['time'], df['velocity'], label='Velocity', color='black')\n",
    "# axs[0].set_xlabel('Time')\n",
    "# axs[0].set_ylabel('Velocity (m/s)')\n",
    "# #axs[0].set_title('Velocity across Time')\n",
    "# #axs[0].legend()\n",
    "\n",
    "# Plot for vertical rate with black line color\n",
    "axs[0].plot(df['time'], df['vertrate'], label='Vertical Rate', color='black')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Vertical Rate (m/s)')\n",
    "#axs[1].set_title('Vertical Rate across Time')\n",
    "#axs[1].legend()\n",
    "axs[0].axvline(x=1596153907, color='red', linestyle='--', label='Time Marker')\n",
    "\n",
    "axs[1].plot(df['time'], df['heightAGL'], label='AGL', color='black')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Height AGL (m)')\n",
    "axs[1].axvline(x=1596153907, color='red', linestyle='--', label='Time Marker')\n",
    "\n",
    "# Plot for predictions with black line color\n",
    "axs[2].plot(predictions_df['Prediction'], label='Predictions', color='black')\n",
    "axs[2].set_xlabel('Time Step')\n",
    "axs[2].set_ylabel('Predicted Drop Likelihood')\n",
    "axs[2].set_ylim(0, 1)  # Set y-axis range from 0 to 1\n",
    "\n",
    "#axs[2].set_title('Predictions across Time Steps')\n",
    "#axs[2].legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to not overlap\n",
    "\n",
    "#Save the figure\n",
    "# plt.savefig('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\plot_pred_acrosstime_ex7_FORPPT.png')  # Change the file path and name as needed\n",
    "# file_name_without_extension = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# # Define the output path for the figure\n",
    "# output_path = os.path.join(output_dir, f'velocity_vertical_prediction_plot_{file_name_without_extension}_FORPPT.png')\n",
    "output_path = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\ModelOutputVisual.png\"\n",
    "# # Save the figure\n",
    "plt.savefig(output_path)\n",
    "\n",
    "\n",
    "# Display the plotfor axs2\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a7ea2-00d4-47bc-8702-f285a24fdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIFFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d81d8b-c599-4837-afa9-dfaaec702d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "#file_path = r\"C:\\Users\\admin-magstadt\\Desktop\\ADSBData\\NondropSamplesADSB\\N130CG_2019_282_17_55_24.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 12))  # 2 Rows, 1 Column\n",
    "\n",
    "# First plot for velocity\n",
    "axs[0].plot(df['time'], df['velocity'], label='Velocity', color='black')\n",
    "axs[0].set_xlabel(' ')\n",
    "axs[0].set_ylabel('Velocity')\n",
    "#axs[0].set_title('Velocity across Time')\n",
    "# Add a gray translucent box for the first 30 seconds\n",
    "start_time = df['time'].iloc[0]  # Assuming the first row has the start time\n",
    "axs[0].axvspan(start_time, start_time + 30, color='gray', alpha=0.5)\n",
    "#axs[0].legend()\n",
    "\n",
    "# Second plot for vertical rate\n",
    "axs[1].plot(df['time'], df['vertrate'], label='Vertical Rate', color='black')\n",
    "axs[1].set_xlabel(' ')\n",
    "axs[1].set_ylabel('Vertical Rate (m/s)')\n",
    "#axs[1].set_title('Vertical Rate across Time')\n",
    "# Add a gray translucent box for the first 30 seconds\n",
    "axs[1].axvspan(start_time, start_time + 30, color='gray', alpha=0.5)\n",
    "#axs[1].legend()\n",
    "\n",
    "axs[2].plot(df['time'], df['heightAGL'], label='Height AGL (m)', color='black')\n",
    "axs[2].set_xlabel('time')\n",
    "axs[2].set_ylabel('Height AGL (m)')\n",
    "#axs[2].set_title('Vertical Rate across Time')\n",
    "# Add a gray translucent box for the first 30 seconds\n",
    "axs[2].axvspan(start_time, start_time + 30, color='gray', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Define the output path for the figure\n",
    "output_path = os.path.join(output_dir, f'velocity_vertical_and30secondwindow_plot_{file_name_without_extension}_FORPPT_CNN.png')\n",
    "\n",
    "# # Save the figure\n",
    "plt.savefig(output_path)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed7dc5-b9ec-4dfe-8a25-6db04be49056",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b70f95-8289-4d98-b878-0261eb779f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "#file_path = 'C:/Users/admin-magstadt/Desktop/ADSBData/NondropSamplesADSB/N130CG_2019_282_17_55_24.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the 'time' column is in seconds and starts from 0 or any specific value\n",
    "start_time = df['time'].iloc[0]\n",
    "end_time = df['time'].iloc[-1]\n",
    "\n",
    "# Define the duration of the box in seconds and the step size\n",
    "box_duration = 30  # Duration of the box\n",
    "step_size = 1  # Step size in seconds\n",
    "\n",
    "# Create a directory to save the plots\n",
    "import os\n",
    "plot_dir = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\scratch\\\\\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "# Generate and save plots\n",
    "filenames = []\n",
    "for current_start in np.arange(start_time, end_time - box_duration, step_size):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot for velocity\n",
    "    axs[0].plot(df['time'], df['velocity'], label='Velocity (m/s)', color='black')\n",
    "    axs[0].set_xlabel('Unix Time')\n",
    "    axs[0].set_ylabel('Velocity (m/s)')\n",
    "    axs[0].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Plot for vertical rate\n",
    "    axs[1].plot(df['time'], df['vertrate'], label='Vertical Rate (m/s)', color='black')\n",
    "    axs[1].set_xlabel('Unix Time')\n",
    "    axs[1].set_ylabel('Vertical Rate (m/s)')\n",
    "    axs[1].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f'{plot_dir}/plot_{current_start}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    filenames.append(filename)\n",
    "\n",
    "output_path = os.path.join(output_dir, f'velocity_vertical_GIF_{file_name_without_extension}_CNN.gif')\n",
    "    \n",
    "# Create a GIF\n",
    "with imageio.get_writer(output_path, mode='I', duration=0.2) as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Optionally, remove the images after creating the GIF\n",
    "for filename in filenames:\n",
    "    os.remove(filename)\n",
    "\n",
    "writer.close()\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e95541-3c6a-479b-b8de-fe82e584029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "#file_path = 'C:/Users/admin-magstadt/Desktop/ADSBData/NondropSamplesADSB/N130CG_2019_282_17_55_24.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Number of zeros to add at the start and end\n",
    "n_zeros_start = 15\n",
    "n_zeros_end = 15\n",
    "\n",
    "# Create zeros series for padding at the start and end\n",
    "zeros_series_start = pd.Series([0] * n_zeros_start)\n",
    "zeros_series_end = pd.Series([0] * n_zeros_end)\n",
    "\n",
    "# Concatenate the zeros with the predictions\n",
    "# Ensure predictions_df['Prediction'] is a Series; if not, adjust accordingly\n",
    "padded_predictions = pd.concat([zeros_series_start, predictions_df['Prediction'], zeros_series_end], ignore_index=True)\n",
    "\n",
    "# Now padded_predictions has the original predictions with 15 zeros at the start and end\n",
    "# Ensure the DataFrame index or plotting logic matches this new length if using df['time'] for plotting\n",
    "\n",
    "# If you need to adjust this Series back into a DataFrame for consistency:\n",
    "predictions_df = pd.DataFrame(padded_predictions, columns=['Prediction'])\n",
    "\n",
    "predictions_df['time'] = df['time']\n",
    "\n",
    "# Assuming predictions_df is already defined or loaded similarly\n",
    "# predictions_df = pd.read_csv('your_predictions_file_path.csv')  # Replace with your actual predictions file path\n",
    "\n",
    "# Define the start and end times\n",
    "start_time = df['time'].iloc[0]\n",
    "end_time = df['time'].iloc[-1]\n",
    "\n",
    "# Define the duration of the box in seconds and the step size\n",
    "box_duration = 30  # Duration of the box\n",
    "step_size = 1  # Step size in seconds\n",
    "\n",
    "# Create a directory to save the plots\n",
    "plot_dir = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\scratch\\\\\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "# Generate and save plots\n",
    "filenames = []\n",
    "for current_start in np.arange(start_time, end_time - box_duration, step_size):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 18))  # Adjusted for an additional subplot\n",
    "    \n",
    "    # Plot for velocity\n",
    "    axs[0].plot(df['time'], df['velocity'], label='Velocity', color='black')\n",
    "    axs[0].set_xlabel('Time')\n",
    "    axs[0].set_ylabel('Velocity')\n",
    "    axs[0].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Plot for vertical rate\n",
    "    axs[1].plot(df['time'], df['vertrate'], label='Vertical Rate', color='black')\n",
    "    axs[1].set_xlabel('Time')\n",
    "    axs[1].set_ylabel('Vertical Rate')\n",
    "    axs[1].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Third panel for predictions, with line being revealed as the box scans\n",
    "    axs[2].set_xlim([df['time'].min()-15, df['time'].max()+15])  # Set x-axis to full range of df['time']\n",
    "    axs[2].set_ylim([0, 1])  # Set y-axis from 0 to 1\n",
    "    \n",
    "    # Plot the predictions up to the current point in the scan\n",
    "    current_end = current_start + box_duration\n",
    "    mask = (predictions_df['time'] <= current_end)\n",
    "    axs[2].plot(predictions_df['time'][mask], predictions_df['Prediction'][mask], label='Predictions', color='red')\n",
    "    \n",
    "    axs[2].set_xlabel('Time')\n",
    "    axs[2].set_ylabel('Predictions')\n",
    "    axs[2].axvspan(current_start, current_end, color='gray', alpha=0)\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f'{plot_dir}/plot_{current_start}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    filenames.append(filename)\n",
    "\n",
    "output_path = os.path.join(output_dir, f'velocity_vertical_and_prediction_GIF_{file_name_without_extension}_CNN.gif')\n",
    "  \n",
    "with imageio.get_writer(output_path, mode='I', duration=0.2) as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Optionally, remove the images after creating the GIF\n",
    "for filename in filenames:\n",
    "    os.remove(filename)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1adbe-b3f2-4a2a-b1fe-ec52ef04ec18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03fb98-2697-4db0-b082-747f563c6501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07f3ca-20d4-4adb-9c66-280c5197cb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a463216-1ca2-4c1f-9bde-e1011c345c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfdcc4c-71a0-49bb-9bab-da79130a4db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c823f-1095-4336-a9cd-e3eb22cf3bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10014aaa-266f-48c0-b0bb-f23b46887dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ac3f7-75b7-4de6-8d72-306645df658c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # old CoDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58b0b0-763e-4276-9f84-5bbbe3f2f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of zeros to add at the start and end\n",
    "# n_zeros_start = 15\n",
    "# n_zeros_end = 15\n",
    "\n",
    "# # Create zeros series for padding at the start and end\n",
    "# zeros_series_start = pd.Series([0] * n_zeros_start)\n",
    "# zeros_series_end = pd.Series([0] * n_zeros_end)\n",
    "\n",
    "# # Concatenate the zeros with the predictions\n",
    "# # Ensure predictions_df['Prediction'] is a Series; if not, adjust accordingly\n",
    "# padded_predictions = pd.concat([zeros_series_start, predictions_df['Prediction'], zeros_series_end], ignore_index=True)\n",
    "\n",
    "# # Now padded_predictions has the original predictions with 15 zeros at the start and end\n",
    "# # Ensure the DataFrame index or plotting logic matches this new length if using df['time'] for plotting\n",
    "\n",
    "# # If you need to adjust this Series back into a DataFrame for consistency:\n",
    "# predictions_df = pd.DataFrame(padded_predictions, columns=['Prediction'])\n",
    "\n",
    "# predictions_df['time'] = df['time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0769db-500c-4558-b465-264a40bc7ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9de1f5-83d1-4954-a816-34facc12167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import imageio\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # Load the data\n",
    "# file_path = 'C:/Users/admin-magstadt/Desktop/ADSBData/NondropSamplesADSB/N130CG_2019_282_17_55_24.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Assuming predictions_df is already defined or loaded similarly\n",
    "# # predictions_df = pd.read_csv('path_to_predictions.csv')  # example if you need to load it\n",
    "\n",
    "# # Define the start and end times\n",
    "# start_time = df['time'].iloc[0]\n",
    "# end_time = df['time'].iloc[-1]\n",
    "\n",
    "# # Define the duration of the box in seconds and the step size\n",
    "# box_duration = 30  # Duration of the box\n",
    "# step_size = 1  # Step size in seconds\n",
    "\n",
    "# # Create a directory to save the plots\n",
    "# plot_dir = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\scratch\\\\\"\n",
    "# if not os.path.exists(plot_dir):\n",
    "#     os.makedirs(plot_dir)\n",
    "\n",
    "# # Generate and save plots\n",
    "# filenames = []\n",
    "# for current_start in np.arange(start_time, end_time - box_duration, step_size):\n",
    "#     fig, axs = plt.subplots(3, 1, figsize=(10, 18))  # Adjusted for an additional subplot\n",
    "    \n",
    "#     # Plot for velocity\n",
    "#     axs[0].plot(df['time'], df['velocity'], label='Velocity', color='black')\n",
    "#     axs[0].set_xlabel('Time')\n",
    "#     axs[0].set_ylabel('Velocity')\n",
    "#     axs[0].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "#     # Plot for vertical rate\n",
    "#     axs[1].plot(df['time'], df['vertrate'], label='Vertical Rate', color='black')\n",
    "#     axs[1].set_xlabel('Time')\n",
    "#     axs[1].set_ylabel('Vertical Rate')\n",
    "#     axs[1].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "#     # Third panel for predictions\n",
    "#     # Filter predictions_df to include only data up to the current point in the box duration\n",
    "#     current_predictions = predictions_df[predictions_df['time'] <= current_start + box_duration]\n",
    "#     axs[2].plot(current_predictions['time'], current_predictions['Prediction'], label='Predictions', color='red')\n",
    "#     axs[2].set_xlabel('Time')\n",
    "#     axs[2].set_ylabel('Predictions')\n",
    "#     axs[2].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "#     # Save the plot\n",
    "#     filename = f'{plot_dir}/plot_{current_start}.png'\n",
    "#     plt.savefig(filename)\n",
    "#     plt.close()\n",
    "#     filenames.append(filename)\n",
    "\n",
    "# # Create a GIF\n",
    "# with imageio.get_writer('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\plot_scan_4.gif', mode='I', duration=0.2) as writer:\n",
    "#     for filename in filenames:\n",
    "#         image = imageio.imread(filename)\n",
    "#         writer.append_data(image)\n",
    "\n",
    "# # Optionally, remove the images after creating the GIF\n",
    "# for filename in filenames:\n",
    "#     os.remove(filename)\n",
    "\n",
    "# print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ba471-3b1b-49f8-938b-e2d634b8cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import imageio\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # Load the data\n",
    "# file_path = 'C:/Users/admin-magstadt/Desktop/ADSBData/NondropSamplesADSB/N130CG_2019_282_17_55_24.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Assuming predictions_df is already defined or loaded similarly\n",
    "# # Make sure 'predictions_df' is in the same scale as desired for the plot.\n",
    "# # If the predictions are not in the scale of 0 to 1, you would need to normalize them.\n",
    "# # predictions_df['Prediction'] = predictions_df['Prediction'] / predictions_df['Prediction'].max()\n",
    "\n",
    "# # Define the start and end times\n",
    "# start_time = df['time'].iloc[0]\n",
    "# end_time = df['time'].iloc[-1]\n",
    "\n",
    "# # Define the duration of the box in seconds and the step size\n",
    "# box_duration = 30  # Duration of the box\n",
    "# step_size = 1  # Step size in seconds\n",
    "\n",
    "# # Create a directory to save the plots\n",
    "# plot_dir = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\scratch\\\\\"\n",
    "# if not os.path.exists(plot_dir):\n",
    "#     os.makedirs(plot_dir)\n",
    "\n",
    "    \n",
    "# min_time = (predictions_df['time'].min()-15)\n",
    "# max_time = (predictions_df['time'].max()+15)\n",
    "    \n",
    "# # Generate and save plots\n",
    "# filenames = []\n",
    "# for current_start in np.arange(start_time, end_time - box_duration, step_size):\n",
    "#     fig, axs = plt.subplots(3, 1, figsize=(10, 18))  # Adjusted for an additional subplot\n",
    "    \n",
    "#     # Plot for velocity\n",
    "#     axs[0].plot(df['time'], df['velocity'], label='Velocity', color='black')\n",
    "#     axs[0].set_xlabel('Time')\n",
    "#     axs[0].set_ylabel('Velocity')\n",
    "#     axs[0].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "#     # Plot for vertical rate\n",
    "#     axs[1].plot(df['time'], df['vertrate'], label='Vertical Rate', color='black')\n",
    "#     axs[1].set_xlabel('Time')\n",
    "#     axs[1].set_ylabel('Vertical Rate')\n",
    "#     axs[1].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "#     # Third panel for predictions, scaled from 0 to 1\n",
    "#     current_predictions = predictions_df[(predictions_df['time'] >= current_start) & (predictions_df['time'] <= current_start + box_duration)]\n",
    "#     axs[2].plot(current_predictions['time'], current_predictions['Prediction'], label='Predictions', color='red')\n",
    "#     axs[2].set_ylim([0, 1])  # Set y-axis from 0 to 1\n",
    "#     axs[2].set_xlim([min_time, max_time])  # Set y-axis from 0 to 1\n",
    "\n",
    "#     axs[2].set_xlabel('Time')\n",
    "#     axs[2].set_ylabel('Predictions')\n",
    "#     axs[2].axvspan(current_start, current_start + box_duration, color='gray', alpha=0.5)\n",
    "    \n",
    "#     # Save the plot\n",
    "#     filename = f'{plot_dir}/plot_{current_start}.png'\n",
    "#     plt.savefig(filename)\n",
    "#     plt.close()\n",
    "#     filenames.append(filename)\n",
    "\n",
    "# # Create a GIF\n",
    "# with imageio.get_writer('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\figures\\\\plot_scan_7.gif', mode='I', duration=0.2) as writer:\n",
    "#     for filename in filenames:\n",
    "#         image = imageio.imread(filename)\n",
    "#         writer.append_data(image)\n",
    "\n",
    "# # Optionally, remove the images after creating the GIF\n",
    "# for filename in filenames:\n",
    "#     os.remove(filename)\n",
    "\n",
    "# print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50987159-2256-4336-8916-01f235d367b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d890668-2525-4078-989a-55b3e7c469f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2bf8b7-e4dc-41b9-b31a-2924159f36ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef9d4d-0b10-4fc7-b464-b105fa65904e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4a060-1c62-4e8a-abe9-a6b5a7523755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices where predicted label does not match true label\n",
    "incorrect_indices = np.where(predicted_labels != test_labels)[0]\n",
    "#incorrect_indices.shape\n",
    "# # Plot the corresponding test data for incorrect predictions\n",
    "# for index in incorrect_indices:\n",
    "#     plt.plot(test_data[index], label=f\"Index {index} (Predicted {predicted_labels[index]}, Actual {test_labels[index]})\")\n",
    "\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Test Data with Incorrect Predictions')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5460441-2b26-4311-811f-ec45b93f7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[10]\n",
    "#print(predicted_labels[133:950])\n",
    "#print(test_labels[10])\n",
    "\n",
    "incorrect = predicted_labels-test_labels\n",
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283d8cc-f510-4326-8b12-e09a648bc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify incorrect predictions\n",
    "incorrect_indices = np.where(predicted_labels != test_labels)[0]\n",
    "\n",
    "# Extract incorrect predictions data\n",
    "incorrect_data = test_data[incorrect_indices]\n",
    "incorrect_data.shape\n",
    "# # Plotting\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# # Titles for each feature plot\n",
    "# feature_titles = ['Feature 1', 'Feature 2', 'Feature 3']\n",
    "\n",
    "# for i in range(3):\n",
    "#     # Assuming each feature is in a separate column (adjust indexing if necessary)\n",
    "#     for incorrect_sample in incorrect_data:\n",
    "#         axs[i].plot(incorrect_sample[:, i], label=f'Sample {i}')\n",
    "#     axs[i].set_title(feature_titles[i])\n",
    "#     axs[i].set_xlabel('Time Step')\n",
    "#     axs[i].set_ylabel('Value')\n",
    "\n",
    "# # Show only a few legends to avoid clutter\n",
    "# handles, labels = axs[0].get_legend_handles_labels()\n",
    "# fig.legend(handles[:min(len(handles), 10)], labels[:min(len(labels), 10)], loc='upper right')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dec54-4cb0-41db-ade8-4eadfbf8675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import geemap\n",
    "# import ee\n",
    "# import numpy as np\n",
    "# from pyopensky import OpenskyImpalaWrapper\n",
    "\n",
    "# # Initialize Earth Engine and OpenSky\n",
    "# ee.Initialize(project='ee-magstadt')\n",
    "# opensky = OpenskyImpalaWrapper()\n",
    "\n",
    "# # Initial timestamps\n",
    "# start_timestamp = 1557599125\n",
    "# end_timestamp = start_timestamp + 30\n",
    "\n",
    "# # Loop configuration\n",
    "# num_iterations = 4  # Define how many iterations you want\n",
    "# time_increment = 1  # Seconds to increase each time\n",
    "\n",
    "# # Loop\n",
    "# for i in range(num_iterations):\n",
    "#     converted_time1 = datetime.datetime.utcfromtimestamp(start_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     converted_time2 = datetime.datetime.utcfromtimestamp(end_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#     # Record start time of iteration\n",
    "#     iteration_start_time = datetime.datetime.now()\n",
    "\n",
    "#     # Query data\n",
    "#     df = opensky.query(\n",
    "#         type=\"adsb\",\n",
    "#         start=str(converted_time1),\n",
    "#         end=str(converted_time2),\n",
    "#         icao24=['a07b6a']\n",
    "#     )\n",
    "\n",
    "#     if df is not None and not df.empty:\n",
    "#         ee_df = geemap.pandas_to_ee(df, latitude='lat', longitude='lon')\n",
    "\n",
    "#         # Buffer the geometry bounds before clipping the DEM\n",
    "#         distance = 100  # meters\n",
    "#         buffered_bounds = ee_df.geometry().bounds().buffer(distance)\n",
    "#         dem = ee.Image(\"USGS/3DEP/10m\").clip(buffered_bounds)\n",
    "\n",
    "#         # Extract data for each image at appropriate scale\n",
    "#         terrain_fc = ee.Terrain.products(dem).sampleRegions(collection=ee_df, scale=10.2)\n",
    "#         terrain_df = geemap.ee_to_pandas(terrain_fc)\n",
    "\n",
    "#         # Drop rows with missing values\n",
    "#         df_sample = terrain_df.dropna()\n",
    "#         # Calculate height AGL\n",
    "#         baro_altitude = df_sample['geoaltitude']\n",
    "#         elevation = df_sample['elevation']\n",
    "#         height_AGL = baro_altitude - elevation\n",
    "#         df_sample['heightAGL'] = height_AGL\n",
    "\n",
    "#         # Check if there are enough samples for prediction\n",
    "#         if df_sample.shape[0] >= 25:\n",
    "#             # Prepare data for prediction\n",
    "#             subset_array = df_sample[['velocity', 'vertrate']].values[:25]#, 'heightAGL'\n",
    "#             arrayPRED = np.reshape(subset_array, (1, 25, 2))\n",
    "\n",
    "#             # Make prediction\n",
    "#             prediction = modelCNN.predict(arrayPRED)\n",
    "\n",
    "#             # Record end time of iteration and calculate time difference\n",
    "#             iteration_end_time = datetime.datetime.now()\n",
    "#             time_difference = iteration_end_time - iteration_start_time\n",
    "\n",
    "#             # Print results\n",
    "#             print(f\"Iteration {i+1}: Time difference: {time_difference}, Prediction: {prediction}\")\n",
    "#         else:\n",
    "#             print(f\"Iteration {i+1}: Not enough data points for prediction.\")\n",
    "#     else:\n",
    "#         print(f\"Iteration {i+1}: No data returned from query.\")\n",
    "\n",
    "#     # Increment the timestamps for the next iteration\n",
    "#     start_timestamp += time_increment\n",
    "#     end_timestamp += time_increment\n",
    "# print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d0786-8437-403e-bcda-ec03a1683d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "# import geopandas as gpd\n",
    "\n",
    "# # Read in the Shapefile\n",
    "# shp_file = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\VLATfromJIM\\\\Output\\\\OutputNonDropSamples\\\\T910_2021_240_221715.shp\"\n",
    "# gdf = gpd.read_file(shp_file)\n",
    "\n",
    "# # Calculate the number of chunks\n",
    "# num_chunks = len(gdf) // 30\n",
    "\n",
    "# # Iterate through chunks and export as Shapefiles\n",
    "# for i in range(10):# num_chunks\n",
    "#     chunk = gdf.iloc[i*30:(i+1)*30]\n",
    "#     chunk.to_file(f\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\CLEANATUNONDROPSAMPELS\\\\T910_2021_240_221715_{i+1}.shp\")\n",
    "# print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
