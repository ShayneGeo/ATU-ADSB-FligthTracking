{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b5b1c-9a61-4715-b3d4-0e8a03591d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\FireOccurence_3_27_2023\\\\National_USFS_Fire_Occurrence_Point_R123456_noNull_post1981.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d3a4f-5fa3-4c5b-b853-3f3fcbf1eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from herbie import Herbie\n",
    "#from toolbox import EasyMap, pc\n",
    "#from toolbox.mapping import EasyMap\n",
    "#from paint.standard2 import cm_tmp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import traceback  # Add this line to import the traceback module\n",
    "\n",
    "# https://mesowest.utah.edu/html/hrrr/zarr_documentation/html/ex_python_plot_zarr.html\n",
    "\n",
    "import s3fs\n",
    "import numcodecs as ncd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Make sure to import numpy if you haven't already\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import csv\n",
    "from io import StringIO\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62616aac-6754-42f1-b99e-6617a37d3539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959c4e1-f07a-41ac-a8f1-d031f84e4fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check the first few rows to ensure it's loaded properly\n",
    "#data.head()\n",
    "year = 109\n",
    "\n",
    "for i in range(38):\n",
    "    \n",
    "    yearfull = year+1900\n",
    "    fp = \"C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\FireOccurence_3_27_2023\\\\National_USFS_Fire_Occurrence_Point_R123456_noNull_post1981.shp\"\n",
    "    data = gpd.read_file(fp)\n",
    "\n",
    "\n",
    "    dataYear = data[data['FIREYEAR'].astype(int) == yearfull]\n",
    "\n",
    "    point_lat = dataYear['LATDD83']\n",
    "    point_lon = dataYear['LONGDD83']\n",
    "    farea_change_shifted = dataYear['SIZECLASS']\n",
    "    OBJECTID = dataYear['OBJECTID']\n",
    "\n",
    "    # #DATE = gdf_1200_pt['t']\n",
    "    # # Assuming 't' is your existing datetime column in gdf_1200_pt\n",
    "    dataYear['t'] = pd.to_datetime(dataYear['DISCOVERYD'])  # Convert to datetime format\n",
    "\n",
    "    # # Calculate two days before and format it as YYYYMMDD\n",
    "    # data['tminus2days'] = (data['t'] - pd.Timedelta(days=2)).dt.strftime('%Y%m%d')\n",
    "    date = dataYear['t']\n",
    "\n",
    "#     # #start_index = 20\n",
    "#     # #end_index = 32\n",
    "#     # point_lat = point_lat#[start_index:end_index]\n",
    "#     # point_lon = point_lon#[start_index:end_index]\n",
    "#     # gdf_DATE = date#[start_index:end_index]\n",
    "#     # #farea_change_shifted_Label = farea_change_shifted#[start_index:end_index]\n",
    "#     # #farea_change_shifted_Label = np.array(farea_change_shifted_Label)\n",
    "#     # #farea_change_shifted_Label = farea_change_shifted_Label[:500]\n",
    "\n",
    "#     subset_point_lat = point_lat[:len(date)]\n",
    "#     subset_point_lon = point_lon[:len(date)]\n",
    "#     subset_point_date = date[:len(date)]\n",
    "#     subset_point_farea_change_shifted = farea_change_shifted[:len(date)]\n",
    "#     subset_OBJECTID = OBJECTID[:len(date)]\n",
    "\n",
    "#     def fetch_daymet_data(lat, lon, vars=None, years=None, start_date=None, end_date=None):\n",
    "#         base_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n",
    "\n",
    "#         # Prepare the parameters for the request\n",
    "#         params = {\n",
    "#             \"lat\": lat,\n",
    "#             \"lon\": lon\n",
    "#         }\n",
    "\n",
    "#         if vars:\n",
    "#             params[\"vars\"] = \",\".join(vars)\n",
    "#         if years:\n",
    "#             params[\"years\"] = \",\".join(map(str, years))\n",
    "#         if start_date:\n",
    "#             params[\"start\"] = start_date\n",
    "#         if end_date:\n",
    "#             params[\"end\"] = end_date\n",
    "\n",
    "#         # Make the request\n",
    "#         response = requests.get(base_url, params=params)\n",
    "\n",
    "#         if response.status_code == 200:\n",
    "#             return response.text\n",
    "#         else:\n",
    "#             response.raise_for_status()\n",
    "\n",
    "#     def compute_dates(date_str):\n",
    "#         formatted_date = date_str.strftime('%Y%m%d')\n",
    "#         end_date_obj = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "#         start_date_obj = end_date_obj - timedelta(days=30)  # subtract a month (approximately 30 days)\n",
    "\n",
    "#         start_date = start_date_obj.strftime('%Y-%m-%d')\n",
    "#         end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "#         return start_date, end_date\n",
    "\n",
    "#     import numpy as np  # Make sure to import numpy if you haven't already\n",
    "#     from datetime import datetime, timedelta\n",
    "#     import requests\n",
    "#     import csv\n",
    "#     from io import StringIO\n",
    "\n",
    "#     filtered_arrays = []\n",
    "#     filtered_labels = []\n",
    "#     filtered_OBJECTID = []\n",
    "\n",
    "#     for lat, lon, date_str, label, object_id in zip(subset_point_lat, subset_point_lon, subset_point_date, subset_point_farea_change_shifted, subset_OBJECTID):\n",
    "#         try:\n",
    "#             start_date, end_date = compute_dates(date_str)\n",
    "\n",
    "#             all_data = []\n",
    "\n",
    "#             vars_list = [\"tmax\", \"tmin\", \"srad\", \"vp\", \"swe\", \"prcp\", \"dayl\"]\n",
    "#             for var in vars_list:\n",
    "#                 data = fetch_daymet_data(lat, lon, [var], start_date=start_date, end_date=end_date)\n",
    "\n",
    "#                 # Convert the CSV data to a list\n",
    "#                 csv_file = StringIO(data)\n",
    "#                 csv_reader = csv.reader(csv_file)\n",
    "#                 headers = next(csv_reader)  # retrieve the headers (column names)\n",
    "\n",
    "#                 # Identify the start of the actual data\n",
    "#                 for row in csv_reader:\n",
    "#                     if row and row[0] == 'year':\n",
    "#                         headers = row\n",
    "#                         break\n",
    "\n",
    "#                 # Process the rows after the headers\n",
    "#                 var_data = []\n",
    "#                 for row in csv_reader:\n",
    "#                     try:\n",
    "#                         temp_value = float(row[2])  # extracting the third column\n",
    "#                         var_data.append(temp_value)\n",
    "#                     except ValueError:\n",
    "#                         print(f\"Skipped row due to non-numeric value: {row}\")\n",
    "\n",
    "#                 all_data.append(var_data)\n",
    "\n",
    "#             # Transpose the data to get it in the shape (11, 7)\n",
    "#             transposed_data = np.transpose(all_data)\n",
    "\n",
    "#             # Shape it as (1, 11, 7)\n",
    "#             data_array = np.expand_dims(transposed_data, axis=0)\n",
    "\n",
    "#             # Check if the shape matches (1, 31, 7)\n",
    "#             if data_array.shape == (1, 31, 7):\n",
    "#                 filtered_arrays.append(data_array)\n",
    "#                 filtered_labels.append(label)\n",
    "#                 filtered_OBJECTID.append(object_id)\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "\n",
    "#     # Stack the arrays to get the final shape\n",
    "#     final_data = np.vstack(filtered_arrays)\n",
    "#     print(final_data.shape)\n",
    "\n",
    "\n",
    "#     print(\"Final Data Shape:\", final_data.shape)\n",
    "#     print(\"Filtered Labels Length:\", len(filtered_labels))\n",
    "#     print(\"Filtered OBJECTID Length:\", len(filtered_OBJECTID))\n",
    "\n",
    "#     # Save the final_data to a binary file\n",
    "#     np.save(f'C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_{year}.npy', final_data)\n",
    "#     np.save(f'C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_{year}_labels.npy', filtered_labels)\n",
    "#     np.save(f'C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_{year}_oid.npy', filtered_OBJECTID)\n",
    "    \n",
    "#     year=year+1\n",
    "#     print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460125e-5293-4915-8ec0-d7a6c7de7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0684bbc-33bb-498f-b2ea-7d9a08e5f031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9339476-99bb-4299-bbf4-b244fa2e172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the final_data to a binary file\n",
    "# np.save('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_84.npy', final_data)\n",
    "# np.save('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_84_labels.npy', filtered_labels)\n",
    "# np.save('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_84_oid.npy', filtered_OBJECTID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf822f8-7438-4d30-b5a3-a4c81471be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the saved file\n",
    "# loaded_data_81 = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_81.npy')\n",
    "# loaded_data_82 = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_82.npy')\n",
    "# loaded_data_83 = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_83.npy')\n",
    "# loaded_data_84 = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_84.npy')\n",
    "\n",
    "# final_data = np.vstack((loaded_data_81,loaded_data_82,loaded_data_83,loaded_data_84))\n",
    "# final_data.shape\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store the loaded data\n",
    "loaded_data = []\n",
    "\n",
    "# Loop through the file numbers from 81 to 121 (inclusive)\n",
    "for i in range(81, 122):  # Change the range to 81 to 84 for the specific files you mentioned\n",
    "    file_path = f'C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_{i}.npy'\n",
    "    data = np.load(file_path)\n",
    "    loaded_data.append(data)\n",
    "\n",
    "# Stack the loaded arrays vertically\n",
    "final_data = np.vstack(loaded_data)\n",
    "\n",
    "# Check the shape of the final_data array\n",
    "print(final_data.shape)\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "# loaded_data_81_labels = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_81_labels.npy')\n",
    "# loaded_data_82_labels = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_82_labels.npy')\n",
    "# loaded_data_83_labels = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_83_labels.npy')\n",
    "# loaded_data_84_labels = np.load('C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_84_labels.npy')\n",
    "\n",
    "# final_data_Labels = np.concatenate((loaded_data_81_labels, loaded_data_82_labels,loaded_data_83_labels,loaded_data_84_labels))\n",
    "# final_data_Labels.shape\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store the loaded data\n",
    "loaded_data = []\n",
    "\n",
    "# Loop through the file numbers from 81 to 121 (inclusive)\n",
    "for i in range(81, 122):  # Change the range to 81 to 84 for the specific files you mentioned\n",
    "    file_path = f'C:\\\\Users\\\\admin-magstadt\\\\Desktop\\\\daymet_analysis\\\\OUTPUTDATA\\\\final_data_{i}_Labels.npy'\n",
    "    data = np.load(file_path)\n",
    "    loaded_data.append(data)\n",
    "\n",
    "# Stack the loaded arrays vertically\n",
    "final_data_labels = np.concatenate(loaded_data)\n",
    "\n",
    "# Check the shape of the final_data array\n",
    "print(final_data_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a12fdf-405d-4072-8afc-d2fab5029a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the first sample (first element) from final_data\n",
    "first_sample = final_data[379611]\n",
    "\n",
    "# Create a figure with subplots for each column\n",
    "fig, axs = plt.subplots(7, 1, figsize=(8, 12))\n",
    "\n",
    "# Plot each column of the first sample\n",
    "for i in range(7):\n",
    "    axs[i].plot(first_sample[:, i])\n",
    "    axs[i].set_xlabel('X-Axis Label')\n",
    "    axs[i].set_ylabel(f'Y-Axis Label {i+1}')\n",
    "    axs[i].set_title(f'Plot of Column {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f51d30-98ef-4185-a340-fee190c2f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices where final_data_labels are \"A\"\n",
    "indices_A = np.where(final_data_labels == \"E\")[0]\n",
    "\n",
    "# Extract the corresponding rows from final_data\n",
    "final_data_A = final_data[indices_A]\n",
    "\n",
    "# Check the shape of final_data_A\n",
    "print(final_data_A.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8196d-f7a1-4a2c-b811-778742bf111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract the first 7 columns of final_data_A for all rows\n",
    "# first_7_columns = final_data_A[:, :, :7]\n",
    "\n",
    "# # Reshape the data for plotting (assuming you want to flatten the 3D array)\n",
    "# reshaped_data = first_7_columns.reshape(-1, 7)\n",
    "\n",
    "# # Create a figure with subplots for each column\n",
    "# fig, axs = plt.subplots(7, 1, figsize=(8, 12))\n",
    "\n",
    "# # Plot each column\n",
    "# for i in range(7):\n",
    "#     axs[i].plot(reshaped_data[:, i])\n",
    "#     axs[i].set_xlabel('X-Axis Label')\n",
    "#     axs[i].set_ylabel(f'Y-Axis Label {i+1}')\n",
    "#     axs[i].set_title(f'Plot of Column {i+1}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1642ff9-7885-4f1f-be06-ea2c551aac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first part of the final_data_A array\n",
    "plt.plot(final_data_A[0, :])  \n",
    "\n",
    "plt.ylim(0, 5)  # Replace min_value and max_value with your desired range\n",
    "\n",
    "plt.xlabel('X-Axis Label')\n",
    "plt.ylabel('Y-Axis Label')\n",
    "plt.title('Plot of final_data_A')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e7552-e6f9-4e11-9fdc-616a4eac6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram\n",
    "plt.hist(final_data_labels, bins=num_classes)  # 'num_classes' should be the number of unique classes\n",
    "plt.title(\"Label Histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148cdbe-dec4-446f-afb7-91d4ef95fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the string labels to numerical labels\n",
    "final_data_labels = label_encoder.fit_transform(final_data_labels)\n",
    "\n",
    "# Parameters\n",
    "final_data = final_data.astype(np.float32)\n",
    "#final_data_labels = np.array(final_data_labels, dtype=np.float32)\n",
    "\n",
    "input_shape = final_data[0].shape\n",
    "num_classes = len(set(final_data_labels))\n",
    "print(input_shape)\n",
    "print(num_classes)\n",
    "\n",
    "# Parameters\n",
    "input_shape = final_data[0].shape\n",
    "num_classes = len(set(final_data_labels))\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(final_data, np.array(final_data_labels), epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07eeab-575b-4713-b2e8-bffda7a0416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the SizeClass column\n",
    "# data.plot(column='SIZECLASS', legend=True, figsize=(10, 6))\n",
    "\n",
    "# plt.title(\"SizeClass Distribution\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fa15f-8c7c-4977-8da4-72420e9073ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming data is already loaded\n",
    "# # Group by FIREYEAR and get counts of SIZECLASS for each year\n",
    "# grouped = data.groupby('FIREYEAR')['SIZECLASS'].value_counts().unstack()\n",
    "\n",
    "# # Plot the data\n",
    "# grouped.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
    "# plt.title(\"Distribution of SizeClass by FireYear\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.xlabel(\"Year\")\n",
    "# plt.legend(title='SIZECLASS')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b8de90-498a-4483-8412-01b53e62f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming data is already loaded as before\n",
    "# # Group by FIREYEAR and sum TOTALACRES for each year\n",
    "# grouped = data.groupby('FIREYEAR')['TOTALACRES'].sum()\n",
    "\n",
    "# # Plot the data\n",
    "# grouped.plot(kind='line', figsize=(15, 7))\n",
    "# plt.title(\"Sum of Total Acres by Fire Year\")\n",
    "# plt.ylabel(\"Total Acres\")\n",
    "# plt.xlabel(\"Year\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50796d-c64b-4851-9412-55aaabf33548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# point_lat = data['LATDD83']\n",
    "# point_lon = data['LONGDD83']\n",
    "# farea_change_shifted = data['SIZECLASS']\n",
    "\n",
    "# # #DATE = gdf_1200_pt['t']\n",
    "# # # Assuming 't' is your existing datetime column in gdf_1200_pt\n",
    "# data['t'] = pd.to_datetime(data['DISCOVERYD'])  # Convert to datetime format\n",
    "\n",
    "# # # Calculate two days before and format it as YYYYMMDD\n",
    "# # data['tminus2days'] = (data['t'] - pd.Timedelta(days=2)).dt.strftime('%Y%m%d')\n",
    "# date = data['t']\n",
    "\n",
    "# # #start_index = 20\n",
    "# # #end_index = 32\n",
    "# # point_lat = point_lat#[start_index:end_index]\n",
    "# # point_lon = point_lon#[start_index:end_index]\n",
    "# # gdf_DATE = date#[start_index:end_index]\n",
    "# # #farea_change_shifted_Label = farea_change_shifted#[start_index:end_index]\n",
    "# # #farea_change_shifted_Label = np.array(farea_change_shifted_Label)\n",
    "# # #farea_change_shifted_Label = farea_change_shifted_Label[:500]\n",
    "\n",
    "\n",
    "# subset_point_lat = point_lat[:len(date)]\n",
    "# subset_point_lon = point_lon[:len(date)]\n",
    "# subset_point_date = date[:len(date)]\n",
    "# subset_point_farea_change_shifted = farea_change_shifted[:len(date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7174877-fed3-45b3-b550-0e673bb282e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "# import requests\n",
    "# import csv\n",
    "# from io import StringIO\n",
    "\n",
    "# def fetch_daymet_data(lat, lon, vars=None, years=None, start_date=None, end_date=None):\n",
    "#     base_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n",
    "    \n",
    "#     # Prepare the parameters for the request\n",
    "#     params = {\n",
    "#         \"lat\": lat,\n",
    "#         \"lon\": lon\n",
    "#     }\n",
    "\n",
    "#     if vars:\n",
    "#         params[\"vars\"] = \",\".join(vars)\n",
    "#     if years:\n",
    "#         params[\"years\"] = \",\".join(map(str, years))\n",
    "#     if start_date:\n",
    "#         params[\"start\"] = start_date\n",
    "#     if end_date:\n",
    "#         params[\"end\"] = end_date\n",
    "\n",
    "#     # Make the request\n",
    "#     response = requests.get(base_url, params=params)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         return response.text\n",
    "#     else:\n",
    "#         response.raise_for_status()\n",
    "\n",
    "# def compute_dates(date_str):\n",
    "#     formatted_date = date_str.strftime('%Y%m%d')\n",
    "#     end_date_obj = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "#     start_date_obj = end_date_obj - timedelta(days=30)  # subtract a month (approximately 30 days)\n",
    "    \n",
    "#     start_date = start_date_obj.strftime('%Y-%m-%d')\n",
    "#     end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     return start_date, end_date\n",
    "\n",
    "\n",
    "\n",
    "# all_arrays = []\n",
    "# all_labels = []\n",
    "\n",
    "\n",
    "# for lat, lon, date_str, label in zip(subset_point_lat, subset_point_lon, subset_point_date, subset_point_farea_change_shifted):\n",
    "#     try:\n",
    "        \n",
    "#         start_date, end_date = compute_dates(date_str)\n",
    "\n",
    "#         all_data = []\n",
    "\n",
    "#         vars_list = [\"tmax\", \"tmin\", \"srad\", \"vp\", \"swe\", \"prcp\", \"dayl\"]\n",
    "#         for var in vars_list:\n",
    "#             data = fetch_daymet_data(lat, lon, [var], start_date=start_date, end_date=end_date)\n",
    "\n",
    "#              # Convert the CSV data to a list\n",
    "#             csv_file = StringIO(data)\n",
    "#             csv_reader = csv.reader(csv_file)\n",
    "#             headers = next(csv_reader)  # retrieve the headers (column names)\n",
    "\n",
    "#             # Identify the start of the actual data\n",
    "#             for row in csv_reader:\n",
    "#                 if row and row[0] == 'year':\n",
    "#                     headers = row\n",
    "#                     break\n",
    "\n",
    "#             # Process the rows after the headers\n",
    "#             var_data = []\n",
    "#             for row in csv_reader:\n",
    "#                 try:\n",
    "#                     temp_value = float(row[2])  # extracting the third column\n",
    "#                     var_data.append(temp_value)\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Skipped row due to non-numeric value: {row}\")\n",
    "\n",
    "#             all_data.append(var_data)\n",
    "\n",
    "#         # Transpose the data to get it in the shape (11,7)\n",
    "#         transposed_data = np.transpose(all_data)\n",
    "\n",
    "#         # Shape it as (1,11,7)\n",
    "#         data_array = np.expand_dims(transposed_data, axis=0)\n",
    "#         all_arrays.append(data_array)\n",
    "#         all_labels.append(label)\n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "# Stack the arrays to get the final shape\n",
    "#final_data = np.vstack(all_arrays)\n",
    "#print(final_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3c0ea-0e24-497c-bc98-e5ca6bef230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "# import requests\n",
    "# import csv\n",
    "# from io import StringIO\n",
    "\n",
    "# def fetch_daymet_data(lat, lon, vars=None, years=None, start_date=None, end_date=None):\n",
    "#     base_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n",
    "    \n",
    "#     # Prepare the parameters for the request\n",
    "#     params = {\n",
    "#         \"lat\": lat,\n",
    "#         \"lon\": lon\n",
    "#     }\n",
    "\n",
    "#     if vars:\n",
    "#         params[\"vars\"] = \",\".join(vars)\n",
    "#     if years:\n",
    "#         params[\"years\"] = \",\".join(map(str, years))\n",
    "#     if start_date:\n",
    "#         params[\"start\"] = start_date\n",
    "#     if end_date:\n",
    "#         params[\"end\"] = end_date\n",
    "\n",
    "#     # Make the request\n",
    "#     response = requests.get(base_url, params=params)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         return response.text\n",
    "#     else:\n",
    "#         response.raise_for_status()\n",
    "\n",
    "# def compute_dates(date_str):\n",
    "#     formatted_date = date_str.strftime('%Y%m%d')\n",
    "#     end_date_obj = datetime.strptime(formatted_date, '%Y%m%d')\n",
    "#     start_date_obj = end_date_obj - timedelta(days=30)  # subtract a month (approximately 30 days)\n",
    "    \n",
    "#     start_date = start_date_obj.strftime('%Y-%m-%d')\n",
    "#     end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     return start_date, end_date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize an empty list to store all data arrays\n",
    "# all_arrays = []\n",
    "# all_labels = []\n",
    "\n",
    "# # Define the common number of days for all time periods\n",
    "# common_num_days = 30\n",
    "\n",
    "# for lat, lon, date_str, label in zip(subset_point_lat, subset_point_lon, subset_point_date, subset_point_farea_change_shifted):\n",
    "#     try:\n",
    "#         start_date, end_date = compute_dates(date_str)\n",
    "\n",
    "#         all_data = []\n",
    "\n",
    "#         vars_list = [\"tmax\", \"tmin\", \"srad\", \"vp\", \"swe\", \"prcp\", \"dayl\"]\n",
    "#         for var in vars_list:\n",
    "#             data = fetch_daymet_data(lat, lon, [var], start_date=start_date, end_date=end_date)\n",
    "\n",
    "#             # Convert the CSV data to a list\n",
    "#             csv_file = StringIO(data)\n",
    "#             csv_reader = csv.reader(csv_file)\n",
    "#             headers = next(csv_reader)  # retrieve the headers (column names)\n",
    "\n",
    "#             # Identify the start of the actual data\n",
    "#             for row in csv_reader:\n",
    "#                 if row and row[0] == 'year':\n",
    "#                     headers = row\n",
    "#                     break\n",
    "\n",
    "#             # Process the rows after the headers\n",
    "#             var_data = []\n",
    "#             for row in csv_reader:\n",
    "#                 try:\n",
    "#                     temp_value = float(row[2])  # extracting the third column\n",
    "#                     var_data.append(temp_value)\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Skipped row due to non-numeric value: {row}\")\n",
    "\n",
    "#             # Check if var_data has the same length as common_num_days\n",
    "#             if len(var_data) != common_num_days:\n",
    "#                 raise ValueError(\"Data doesn't have a common number of days.\")\n",
    "\n",
    "#             all_data.append(var_data)\n",
    "\n",
    "#         # Transpose the data to get it in the shape (11, common_num_days)\n",
    "#         transposed_data = np.transpose(all_data)\n",
    "\n",
    "#         # Shape it as (1, 11, common_num_days)\n",
    "#         data_array = np.expand_dims(transposed_data, axis=0)\n",
    "#         all_arrays.append(data_array)\n",
    "#         all_labels.append(label)\n",
    "#     except ValueError as ve:\n",
    "#         print(f\"Error for lat={lat}, lon={lon}: {ve}\")\n",
    "#         continue\n",
    "\n",
    "# # Stack the arrays to get the final shape\n",
    "# final_data = np.vstack(all_arrays)\n",
    "# print(final_data.shape)\n",
    "# all_labels\n",
    "\n",
    "# final_data = final_data.astype(np.float32)\n",
    "# all_labels = np.array(all_labels, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a8c12-fba1-45d3-b8c9-e8ecc3c9ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# # Parameters\n",
    "# input_shape = final_data[0].shape\n",
    "# num_classes = len(set(all_labels))\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "# model.add(MaxPooling1D(2))\n",
    "# model.add(Conv1D(64, 3, activation='relu'))\n",
    "# model.add(MaxPooling1D(2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(final_data, np.array(all_labels), epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43282bda-4f28-4cc3-876b-291f5ae3bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # Define the GRIDMET API endpoint and parameters\n",
    "# gridmet_api_url = \"https://api.opensciencedatacloud.org/gridmet/data\"\n",
    "# latitude = YOUR_LATITUDE  # Replace with the latitude of your location\n",
    "# longitude = YOUR_LONGITUDE  # Replace with the longitude of your location\n",
    "# start_date = \"YYYY-MM-DD\"  # Replace with your desired start date\n",
    "# end_date = \"YYYY-MM-DD\"  # Replace with your desired end date\n",
    "\n",
    "# # Define the parameters for the API request\n",
    "# params = {\n",
    "#     \"latitude\": latitude,\n",
    "#     \"longitude\": longitude,\n",
    "#     \"start_date\": start_date,\n",
    "#     \"end_date\": end_date,\n",
    "# }\n",
    "\n",
    "# # Make the API request to download GRIDMET data\n",
    "# response = requests.get(gridmet_api_url, params=params)\n",
    "\n",
    "# # Check if the request was successful (status code 200)\n",
    "# if response.status_code == 200:\n",
    "#     # Save the data to a file or process it as needed\n",
    "#     with open(\"gridmet_data.csv\", \"wb\") as file:\n",
    "#         file.write(response.content)\n",
    "#     print(\"GRIDMET data downloaded successfully.\")\n",
    "# else:\n",
    "#     print(\"Error downloading GRIDMET data. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3235a-2788-41b5-ad2a-51402668049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import csv\n",
    "# from io import StringIO\n",
    "\n",
    "# def fetch_daymet_data(lat, lon, vars=None, years=None, start_date=None, end_date=None):\n",
    "#     base_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n",
    "    \n",
    "#     # Prepare the parameters for the request\n",
    "#     params = {\n",
    "#         \"lat\": lat,\n",
    "#         \"lon\": lon\n",
    "#     }\n",
    "\n",
    "#     if vars:\n",
    "#         params[\"vars\"] = \",\".join(vars)\n",
    "#     if years:\n",
    "#         params[\"years\"] = \",\".join(map(str, years))\n",
    "#     if start_date:\n",
    "#         params[\"start\"] = start_date\n",
    "#     if end_date:\n",
    "#         params[\"end\"] = end_date\n",
    "\n",
    "#     # Make the request\n",
    "#     response = requests.get(base_url, params=params)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         return response.text\n",
    "#     else:\n",
    "#         response.raise_for_status()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     lat, lon = 35.0, -85.0\n",
    "#     #vars = [\"tmax\", \"tmin\", \"srad\", \"vp\", \"swe\", \"prcp\", \"dayl\"]\n",
    "#     vars = [\"tmax\"]\n",
    "\n",
    "#     start_date = \"2020-01-01\"\n",
    "#     end_date = \"2020-01-11\"\n",
    "\n",
    "#     data = fetch_daymet_data(lat, lon, vars, start_date=start_date, end_date=end_date)\n",
    "    \n",
    "#     # Convert the CSV data to a list\n",
    "#     csv_file = StringIO(data)\n",
    "#     csv_reader = csv.reader(csv_file)\n",
    "#     headers = next(csv_reader)  # retrieve the headers (column names)\n",
    "\n",
    "#     # Identify the start of the actual data\n",
    "#     for row in csv_reader:\n",
    "#         if row and row[0] == 'year':\n",
    "#             headers = row\n",
    "#             break\n",
    "\n",
    "#     # Process the rows after the headers\n",
    "#     processed_data = []\n",
    "#     for row in csv_reader:\n",
    "#         try:\n",
    "#             temp_value = float(row[2])  # extracting the third column\n",
    "#             processed_data.append(temp_value)\n",
    "#         except ValueError:\n",
    "#             print(f\"Skipped row due to non-numeric value: {row}\")\n",
    "\n",
    "#     print(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db623c-337d-4828-8130-0720ac0ccb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import csv\n",
    "# from io import StringIO\n",
    "\n",
    "# def fetch_daymet_data(lat, lon, vars=None, years=None, start_date=None, end_date=None):\n",
    "#     base_url = \"https://daymet.ornl.gov/single-pixel/api/data\"\n",
    "    \n",
    "#     # Prepare the parameters for the request\n",
    "#     params = {\n",
    "#         \"lat\": lat,\n",
    "#         \"lon\": lon\n",
    "#     }\n",
    "\n",
    "#     if vars:\n",
    "#         params[\"vars\"] = \",\".join(vars)\n",
    "#     if years:\n",
    "#         params[\"years\"] = \",\".join(map(str, years))\n",
    "#     if start_date:\n",
    "#         params[\"start\"] = start_date\n",
    "#     if end_date:\n",
    "#         params[\"end\"] = end_date\n",
    "\n",
    "#     # Make the request\n",
    "#     response = requests.get(base_url, params=params)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         return response.text\n",
    "#     else:\n",
    "#         response.raise_for_status()\n",
    "        \n",
    "                \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     lat = 35.0\n",
    "#     lon = -85.0\n",
    "\n",
    "#     start_date = \"2020-01-01\"\n",
    "#     end_date = \"2020-01-11\"\n",
    "    \n",
    "#     vars_list = [\"tmax\", \"tmin\", \"srad\", \"vp\", \"swe\", \"prcp\", \"dayl\"]\n",
    "\n",
    "#     all_data = []\n",
    "#     for var in vars_list:\n",
    "#         data = fetch_daymet_data(lat, lon, [var], start_date=start_date, end_date=end_date)\n",
    "        \n",
    "#         # Convert the CSV data to a list\n",
    "#         csv_file = StringIO(data)\n",
    "#         csv_reader = csv.reader(csv_file)\n",
    "#         headers = next(csv_reader)  # retrieve the headers (column names)\n",
    "\n",
    "#         # Identify the start of the actual data\n",
    "#         for row in csv_reader:\n",
    "#             if row and row[0] == 'year':\n",
    "#                 headers = row\n",
    "#                 break\n",
    "\n",
    "#         # Process the rows after the headers\n",
    "#         var_data = []\n",
    "#         for row in csv_reader:\n",
    "#             try:\n",
    "#                 temp_value = float(row[2])  # extracting the third column\n",
    "#                 var_data.append(temp_value)\n",
    "#             except ValueError:\n",
    "#                 print(f\"Skipped row due to non-numeric value: {row}\")\n",
    "        \n",
    "#         all_data.append(var_data)\n",
    "\n",
    "#     # Transpose the data to get it in the shape (11,7)\n",
    "#     transposed_data = np.transpose(all_data)\n",
    "    \n",
    "#     # Shape it as (1,11,7)\n",
    "#     final_data = np.expand_dims(transposed_data, axis=0)\n",
    "\n",
    "#     print(final_data.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c0aa2-b14e-4be6-82ad-292997aad32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# def compute_dates(date_str):\n",
    "#     end_date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "#     start_date_obj = end_date_obj - timedelta(days=30)  # subtract a month (approximately 30 days)\n",
    "    \n",
    "#     start_date = start_date_obj.strftime('%Y-%m-%d')\n",
    "#     end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     return start_date, end_date\n",
    "\n",
    "\n",
    "\n",
    "# # # ... [Your other imports and fetch_daymet_data function go here]\n",
    "\n",
    "# # point_lat = [...]  # Your list of latitudes\n",
    "# # point_lon = [...]  # Your list of longitudes\n",
    "# # dates = [...]      # Your list of dates\n",
    "\n",
    "# all_arrays = []\n",
    "\n",
    "# for lat, lon, date_str in zip(subset_point_lat, subset_point_lon, subset_point_date):\n",
    "#     start_date, end_date = compute_dates(date_str)\n",
    "\n",
    "#     all_data = []\n",
    "#     for var in vars_list:\n",
    "#         data = fetch_daymet_data(lat, lon, [var], start_date=start_date, end_date=end_date)\n",
    "\n",
    "#          # Convert the CSV data to a list\n",
    "#         csv_file = StringIO(data)\n",
    "#         csv_reader = csv.reader(csv_file)\n",
    "#         headers = next(csv_reader)  # retrieve the headers (column names)\n",
    "\n",
    "#         # Identify the start of the actual data\n",
    "#         for row in csv_reader:\n",
    "#             if row and row[0] == 'year':\n",
    "#                 headers = row\n",
    "#                 break\n",
    "\n",
    "#         # Process the rows after the headers\n",
    "#         var_data = []\n",
    "#         for row in csv_reader:\n",
    "#             try:\n",
    "#                 temp_value = float(row[2])  # extracting the third column\n",
    "#                 var_data.append(temp_value)\n",
    "#             except ValueError:\n",
    "#                 print(f\"Skipped row due to non-numeric value: {row}\")\n",
    "                \n",
    "#         all_data.append(var_data)\n",
    "\n",
    "#     # Transpose the data to get it in the shape (11,7)\n",
    "#     transposed_data = np.transpose(all_data)\n",
    "\n",
    "#     # Shape it as (1,11,7)\n",
    "#     data_array = np.expand_dims(transposed_data, axis=0)\n",
    "#     all_arrays.append(data_array)\n",
    "\n",
    "# # Stack the arrays to get the final shape\n",
    "# final_data = np.vstack(all_arrays)\n",
    "# print(final_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c167b-c0c3-462d-aa2b-cbffe604d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Assuming your final_data has the shape (1, 11, 7)\n",
    "# # and vars_list = [\"tmax\", \"tmin\", \"srad\", \"vp\", \"swe\", \"prcp\", \"dayl\"]\n",
    "\n",
    "# data_to_plot = final_data.squeeze()  # This reduces the dimension from (1,11,7) to (11,7)\n",
    "\n",
    "# # 1. Line Plot for Each Variable Over Time\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# for i, var in enumerate(vars_list):\n",
    "#     plt.plot(data_to_plot[:, i], label=var)\n",
    "# plt.legend()\n",
    "# plt.title(\"Daymet Data Over Time\")\n",
    "# plt.xlabel(\"Day\")\n",
    "# plt.ylabel(\"Value\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # 2. Heatmap\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(data_to_plot, annot=True, cmap='viridis', yticklabels=vars_list)\n",
    "# plt.title(\"Heatmap of Daymet Data\")\n",
    "# plt.xlabel(\"Day\")\n",
    "# plt.ylabel(\"Variable\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db0ad2-ec63-40fa-8b96-68455ea80d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import s3fs\n",
    "# import numpy as np\n",
    "# import xarray as xr\n",
    "# import cartopy.crs as ccrs\n",
    "# import zarr\n",
    "\n",
    "# all_forecasts_TMP = []\n",
    "# all_forecasts_GUST = []\n",
    "# labels = []\n",
    "\n",
    "# start_index=0\n",
    "\n",
    "# def retrieve_data(s3_url):\n",
    "#     with fs.open(s3_url, 'rb') as compressed_data: \n",
    "#         buffer = ncd.blosc.decompress(compressed_data.read())\n",
    "\n",
    "#         dtype = \"<f2\"\n",
    "#         if \"surface/PRES\" in s3_url: \n",
    "#             dtype = \"<f4\"\n",
    "\n",
    "#         chunk = np.frombuffer(buffer, dtype=dtype)\n",
    "\n",
    "#         entry_size = 150 * 150\n",
    "#         num_entries = len(chunk) // entry_size\n",
    "\n",
    "#         if num_entries == 1: \n",
    "#             data_array = np.reshape(chunk, (150, 150))\n",
    "#         else:\n",
    "#             data_array = np.reshape(chunk, (num_entries, 150, 150))\n",
    "\n",
    "#     return data_array\n",
    "\n",
    "# for i in range(100):#len(point_lat)-1):\n",
    "#     pt_lat = point_lat[start_index]\n",
    "#     pt_lon = point_lon[start_index]\n",
    "#     date = gdf_DATE[start_index]\n",
    "#     hr = '00'\n",
    "#     level = 'surface'\n",
    "#     fs = s3fs.S3FileSystem(anon=True)\n",
    "#     chunk_index = xr.open_zarr(s3fs.S3Map(\"s3://hrrrzarr/grid/HRRR_chunk_index.zarr\", s3=fs))\n",
    "#     projection = ccrs.LambertConformal(central_longitude=262.5, central_latitude=38.5, standard_parallels=(38.5, 38.5),\n",
    "#                                        globe=ccrs.Globe(semimajor_axis=6371229, semiminor_axis=6371229))\n",
    "#     x, y = projection.transform_point(pt_lon, pt_lat, ccrs.PlateCarree())\n",
    "#     nearest_point = chunk_index.sel(x=x, y=y, method=\"nearest\")\n",
    "#     fcst_chunk_id = f\"0.{nearest_point.chunk_id.values}\"\n",
    "    \n",
    "#     for var in ['GUST', 'TMP']:\n",
    "#         data_url = f'hrrrzarr/sfc/{date}/{date}_{hr}z_fcst.zarr/{level}/{var}/{level}/{var}/'\n",
    "        \n",
    "#         try:\n",
    "#             data = retrieve_data(data_url + fcst_chunk_id)\n",
    "#             gridpoint_forecast = data[:, nearest_point.in_chunk_y, nearest_point.in_chunk_x]\n",
    "            \n",
    "#             if var == 'GUST':\n",
    "#                 all_forecasts_GUST.append(gridpoint_forecast)\n",
    "#             else:\n",
    "#                 all_forecasts_TMP.append(gridpoint_forecast)\n",
    "                \n",
    "#             if var == 'TMP':\n",
    "#                 labels.append(farea_change_shifted[start_index])\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error retrieving data for {var} at index {i}. Error: {e}\")\n",
    "\n",
    "#     start_index += 1\n",
    "#     print(i)\n",
    "\n",
    "# all_forecasts_TMP = np.array(all_forecasts_TMP)\n",
    "# all_forecasts_GUST = np.array(all_forecasts_GUST)\n",
    "\n",
    "# combined_forecasts = np.stack([all_forecasts_TMP, all_forecasts_GUST], axis=2)\n",
    "\n",
    "# print(combined_forecasts.shape)\n",
    "# print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584555e2-4e56-4534-ac9b-84bfe690de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import s3fs\n",
    "# import numpy as np\n",
    "# import xarray as xr\n",
    "# import cartopy.crs as ccrs\n",
    "# import zarr\n",
    "\n",
    "# # Initialize lists to store data\n",
    "# all_temperatures = []\n",
    "# all_precipitation = []\n",
    "# labels = []\n",
    "\n",
    "# start_index = 0\n",
    "\n",
    "# def retrieve_data(s3_url):\n",
    "#     with fs.open(s3_url, 'rb') as compressed_data:\n",
    "#         # You may need to use a different decompression method depending on the format of Daymet data\n",
    "#         # For example, if it's NetCDF, use xarray to open it directly.\n",
    "#         # buffer = ncd.blosc.decompress(compressed_data.read())\n",
    "#         buffer = compressed_data.read()\n",
    "\n",
    "#         # Assuming the data format is NetCDF (modify accordingly if different)\n",
    "#         data_array = xr.open_dataset(buffer)\n",
    "\n",
    "#     return data_array\n",
    "\n",
    "# for i in range(100):  # Adjust the number of iterations as needed\n",
    "#     # Replace with your Daymet-specific parameters\n",
    "#     latitude = point_lat[1]\n",
    "#     longitude = point_lon[1]\n",
    "#     year = 1981\n",
    "#     variable = 'tmax'  # Replace with the Daymet variable you need (e.g., 'tmax', 'prcp')\n",
    "\n",
    "#     # Construct the S3 URL based on Daymet parameters\n",
    "#     data_url = f's3://daymet-data/year/{year}/{variable}/{latitude}_{longitude}.nc4'\n",
    "    \n",
    "#     try:\n",
    "#         data = retrieve_data(data_url)\n",
    "\n",
    "#         # Extract the variable data for the specific location\n",
    "#         gridpoint_data = data[variable]\n",
    "\n",
    "#         # Append to the appropriate list\n",
    "#         if variable == 'prcp':\n",
    "#             all_precipitation.append(gridpoint_data)\n",
    "#         else:\n",
    "#             all_temperatures.append(gridpoint_data)\n",
    "\n",
    "#         # You may need to adjust the label extraction depending on your use case\n",
    "#         # labels.append(YOUR_LABEL_DATA)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error retrieving data for {variable} at index {i}. Error: {e}\")\n",
    "\n",
    "#     start_index += 1\n",
    "#     print(i)\n",
    "\n",
    "# # Convert lists to numpy arrays if needed\n",
    "# all_temperatures = np.array(all_temperatures)\n",
    "# all_precipitation = np.array(all_precipitation)\n",
    "\n",
    "# # You can further process the data as needed\n",
    "# print(all_temperatures.shape)\n",
    "# print(all_precipitation.shape)\n",
    "# print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
